
* centos 7 安装过程
** root 密码丢失
   grub 启动项目 的ro 改成
#+BEGIN_EXAMPLE
   rw init=/sysroot/bin/
#+END_EXAMPLE

进入shell后
#+BEGIN_EXAMPLE


   chroot /sysroot
   passwd root
   touch /.autorelabel
   exit
   reboot
#+END_EXAMPLE

** 网络设置脚本
   修改/etc/sysconfig/network-scripts/ 相应网口的文件

** firewall && selinux
/etc/selinux/config 
   
SELINUX=disabled

** 安装源从光盘

打开/etc/yum.repos.d/CentOS-Base.repo
在所有的源上都加入
#+BEGIN_EXAMPLE
enabled=0
#+END_EXAMPLE

添加新的源
#+BEGIN_EXAMPLE

[media]

name=CentOS-$releasever - media
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=#####
baseurl=file:///mnt/
gpgcheck=1
enabled=1
gpgkey=file:///media/cdrom/RPM-GPG-KEY-CentOS-7     
#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

#+END_EXAMPLE

在/etc/fstab上把centos 光盘加入

#+BEGIN_EXAMPLE

/usr/local/src/CentOS-7.0-1406-x86_64-DVD.iso /media/cdrom/ iso9660 defaults,ro,loop 0 0

#+END_EXAMPLE
** 安装的软件
   - xulrunner.i686  (32位程序支持 |CentOS7 64位 安装32位运行库)
   - gcc automake gdb wirshark-gnome
   - 

* 221 安装 
** freeswitch 

   zlib zlib-devel libjpeg-devel sqlite-devel curl-devel pcre-devel speex-devel ldns-devel libedit-devel openssl-devel

** mysql
   mariadb.x86_64 mariadb-server mysql-connector-odbc

   mysqladmin -uroot password root

** fusionpbx
*** php
    php php-common php-pdo php-soap php-xml php-xmlrpc php-mysql php-fpm
*** nginx
    nginx-release-centos-7-0.el7.ngx.noarch.rpm
    yum install -y nginx

mkdir /etc/nginx/sites-available
mkdir /etc/nginx/sites-enabled

cd /etc/nginx
rm nginx.conf
wget http://www.fusionpbx.com/downloads/centos/nginx/nginx.conf
cd /etc/nginx/sites-available
wget http://www.fusionpbx.com/downloads/centos/nginx/fusionpbx.conf
ln /etc/nginx/sites-available/fusionpbx.conf /etc/nginx/sites-enabled/fusionpbx.conf


*** openssl

*** Permissions
    - selinux
    /etc/selinux/config  ==> SELINUX=disabled
    - firewall
      firewalld.service
*** odbc 
    create databases freeswitch;
    /etc/odbc.ini
    
#+BEGIN_SRC 
[freeswitch]
Driver = MySQL
SERVER = localhost
PORT = 3306
DATABASE = freeswitch
OPTION = 67108864
USER = root
PASSWORD = root
#+END_SRC



**** freeswitch odbc 

     find . | xargs grep dsn
     
     添加一个变量 dsn
     value : odbc://freeswitch

*** /etc/nginx/sites-available/fusionpbx.conf
   listen 80 ==> listen 8080

 如果 是80则出现的不是fusionpbx页面,而是nginx的默认页面, 不知道原因 

添加变量local_ip_v4为bind ip

<param name="register-transport" value="tcp"/>

变量domain一定要使能
** quickbuild
   - conf/hibernate.properties
     打开postgresql设置
*** postgresql

    - postgresql-server postgresql
#+BEGIN_SRC 
postgresql-setup initdb
ps aux | grep postgre
#+END_SRC
    看到生成的默认数据库在/var/lib/pgsql/data

    - 更改listen端口
    /var/lib/pgsql/data/postgresql.conf
#+BEGIN_SRC 
    listen_addresses = 'localhost' 
#+END_SRC
    默认是localhost, 根据需要更改
    
    - 更改认证方式
      /var/lib/pgsql/data/pg_hba.conf
      ident ===> trust

    - create user
      su - postgres
      createuser quickbuild -W
      createdb quickbuild

#+BEGIN_SRC 
#psql
键入 \l, 查看所有 database
postgres-# \l

#+END_SRC      
* ghc on centos 7
** 
https://ghc.haskell.org/trac/ghc/wiki/Building

 glibc-devel ncurses-devel gmp-devel autoconf automake libtool gcc make perl python ghc happy alex git

** halcyon
eval "$( curl -sL https://github.com/mietek/halcyon/raw/master/setup.sh )"

which halcyon

eval "$( /app/halcyon/halcyon paths )"

halcyon install

halcyon install --ghc-version=7.10.1 --cabal-version=1.22.6.0

** cabal config
 1.3 The cabal-install configuration file

You can edit the cabal configuration file to set defaults, for *nix based systems this is:

 ~/.cabal/config

The config file on a Windows system is

 %appdata%\cabal\config


1.3.1 Things to put in the config file

To turn on --global by default:

 user-install: False

The root-cmd configuration parameter can be used to automatically run cabal-install with root privileges on *nix based systems, when needed:

 root-cmd: sudo



=============================================================



The cabal configuration is stored in $HOME/.cabal/config and contains various options including credential information for Hackage upload. One addition to configuration is to completely disallow the installation of packages outside of sandboxes to prevent accidental collisions.

-- Don't allow global install of packages.
require-sandbox: True

A library can also be compiled with runtime profiling information enabled. More on this is discussed in the section on Concurrency and profiling.

library-profiling: True

Another common flag to enable is the documentation which forces the local build of Haddock documentation, which can be useful for offline reference. On a Linux filesystem these are built to the /usr/share/doc/ghc/html/libraries/ directory.

documentation: True

If GHC is currently installed the documentation for the Prelude and Base libraries should be available at this local link:

* quickbuild
Re: Lost administrator password
Postby tardis4500 » Wed Oct 09, 2013 7:35 pm

Found a previous post with this answer and it worked:

If you've forgot the admin password, please edit the file "<QuickBuild server install dir>/conf/wrapper.conf" to uncomment the line "#wrapper.java.additional.4=-Dreset", and the admin password will be reset as 12345.

I didn't find it when I searched the forums because the thread title has "Passwor" and not "Password" in it. You might want to update the thread title if you can.


* taiga back
http://taigaio.github.io/taiga-doc/dist/#_installation_guide

** dir
  taiga/back
** packets
   - binutils autoconf flex bison libjpeg-turbo-devel bzip2-devel libzip-devel
   - freetype-devel zlib-devel ncurses-devel gdbm-devel 
   - automake libtool libffi-devel curl git tmux
   - postgresql postgresql-contrib postgresql-doc  postgresql-devel postgresql-server
   - libxml2-devel libxslt-devel
   - pytho3 python-pip (手动安装)


**** Install pip

To install pip, securely download get-pip.py. [2]

Then run the following (which may require administrator access):

python get-pip.py


** cmd

#+BEGIN_SRC 
sudo -u postgres createuser zcsong
sudo -u postgres createdb taiga -O zcsong



#+END_SRC

     #+BEGIN_SRC 
wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - |python3
wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py -O - |python3
     #+END_SRC

#+BEGIN_SRC 

     pip3 install virtualenvwrapper

export VIRTUALENVWRAPPER_PYTHON=python3.4
source /usr/bin/virtualenvwrapper.sh
mkvirtualenv -p /usr/bin/python3.4 taiga

     pip3 install virtualenvwrapper (消除错误 Error while finding spec for 'virtualenvwrapper.hook_loader' (<class 'ImportError'>: No module name)


     pip install -r requirements.txt

     python manage.py migrate --noinput
     python manage.py loaddata initial_user
     python manage.py loaddata initial_project_templates
     python manage.py loaddata initial_role
    python manage.py compilemessages
   python manage.py collectstatic --noinput

python manage.py migrate --noinput
python manage.py loaddata initial_user
python manage.py loaddata initial_project_templates
python manage.py compilemessages
python manage.py collectstatic --noinput
python manage.py sample_data



#+END_SRC
 *This creates a new user admin with password 123123.*


#+BEGIN_EXAMPLE
/usr/bin/virtualenvwrapper.sh

#  1. Create a directory to hold the virtual environments.
#     (mkdir $HOME/.virtualenvs).
#  2. Add a line like "export WORKON_HOME=$HOME/.virtualenvs"
#     to your .bashrc.
#  3. Add a line like "source /path/to/this/file/virtualenvwrapper.sh"
#     to your .bashrc.
#  4. Run: source ~/.bashrc
#  5. Run: workon
#  6. A list of environments, empty, is printed.
#  7. Run: mkvirtualenv temp
#  8. Run: workon
#  9. This time, the "temp" environment is included.
# 10. Run: workon temp
# 11. The virtual environment is activated.

#+END_EXAMPLE



settings/local.py

#+BEGIN_SRC 
from .common import *

MEDIA_URL = "http://example.com/media/"
STATIC_URL = "http://example.com/static/"
ADMIN_MEDIA_PREFIX = "http://example.com/static/admin/"
SITES["front"]["scheme"] = "http"
SITES["front"]["domain"] = "example.com"

SECRET_KEY = "theveryultratopsecretkey"

DEBUG = False
TEMPLATE_DEBUG = False
PUBLIC_REGISTER_ENABLED = True

DEFAULT_FROM_EMAIL = "no-reply@example.com"
SERVER_EMAIL = DEFAULT_FROM_EMAIL

# Uncomment and populate with proper connection parameters
# for enable email sending.
#EMAIL_BACKEND = "django.core.mail.backends.smtp.EmailBackend"
#EMAIL_USE_TLS = False
#EMAIL_HOST = "localhost"
#EMAIL_HOST_USER = ""
#EMAIL_HOST_PASSWORD = ""
#EMAIL_PORT = 25

# Uncomment and populate with proper connection parameters
# for enable github login/singin.
#GITHUB_API_CLIENT_ID = "yourgithubclientid"
#GITHUB_API_CLIENT_SECRET = "yourgithubclientsecret"

#+END_SRC
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.
workon taiga
python3 manage.py runserver

add /etc/nginx/sites-available/taiga

sudo ln -s /etc/nginx/sites-available/taiga /etc/nginx/sites-enabled/taiga


#+BEGIN_SRC 
server {
    listen 8820 default_server;
    server_name _;

    large_client_header_buffers 4 32k;
    client_max_body_size 50M;
    charset utf-8;

    access_log /home/zcsong/taiga/logs/nginx.access.log;
    error_log /home/zcsong/taiga/logs/nginx.error.log;

    # Frontend
    location / {
        root /home/zcsong/taiga/front/dist/;
        try_files $uri $uri/ /index.html;
    }

    # Backend
    location /api {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000/api;
        proxy_redirect off;
    }

    # Django admin access (/admin/)
    location /admin {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000$request_uri;
        proxy_redirect off;
    }

    # Static files
    location /static {
        alias /home/zcsong/taiga/back/static;
    }

    # Media files
    location /media {
        alias /home/zcsong/taiga/back/media;
    }
}


#+END_SRC



Copy and edit initial configuration on ~/taiga-front-dist/dist/js/conf.json
#+BEGIN_SRC 
{
    "api": "http://example.com/api/v1/",
    "eventsUrl": "ws://example.com/events",
    "debug": "true",
    "publicRegisterEnabled": true,
    "feedbackEnabled": true,
    "privacyPolicyUrl": null,
    "termsOfServiceUrl": null,
    "maxUploadFileSize": null,
    "contribPlugins": []
}

#+END_SRC
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.

如nginx运行时的用户和taiga的放置目录不同, 则要加入taiga所属用户的组, 否则会出现文件访问无权限, 参考下面:

*** stat() 13 permission denied nginx 
http://stackoverflow.com/questions/25774999/nginx-stat-failed-13-permission-denied
Nginx operates within the direcotry, so if you can't cd to that directory from the nginx user then it will fail (as does the stat command in your log). Make sure the www-user can cd all the way to the /username/test/static. You can confirm that the stat will fail or succeed by running

sudo -u www-data stat /username/test/static
In your case probably the /username directory is the issue here. Usually www-data does not have permissions to cd to other users home directories.

The best solution in that case would be to add www-data to username group:
#+BEGIN_SRC 
gpasswd -a www-data username
#+END_SRC

and make sure that username group can enter all directories along the path:

#+BEGIN_SRC 
chmod g+x /username && chmod g+x /username/test && chmod g+x /username/test/static
#+END_SRC



***  python的沙盒环境--virtualenv
http://blog.csdn.net/jianhong1990/article/details/7840139

使用 VirtualEnv 的理由：

隔离项目之间的第三方包依赖，如A项目依赖django1.2.5，B项目依赖django1.3。
为部署应用提供方便，把开发环境的虚拟环境打包到生产环境即可,不需要在服务器上再折腾一翻。
使用说明：

安装： sudo easy_install virtualenv

建立新的运行环境：virtualenv <env-name>

进入相应的独立环境：source <env-path>/bin/activate



*** 最后成功

是因为在我自己的机器上开了back, 221的为什么会连接到我的机器
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.

~/taiga-front-dist/dist/js/conf.json
#+BEGIN_SRC 
{
    "api": "http://example.com/api/v1/",
    "eventsUrl": "ws://example.com/events",
    "debug": "true",
    "publicRegisterEnabled": true,
    "feedbackEnabled": true,
    "privacyPolicyUrl": null,
    "termsOfServiceUrl": null,
    "maxUploadFileSize": null,
    "contribPlugins": []
}
#+END_SRC

这里api设成是和url主地址相同的形式, 如网站是http://192.168.51.221:8820, 这里设成http://192.168.51.221:8820/api/v1

api的请求通过nginx转成内部请求, 通过 http://127.0.0.1:8000/api 去调用接口

#+BEGIN_SRC 
location /api {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000/api;
        proxy_redirect off;
    }

    # Django admin access (/admin/)
    location /admin {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000$request_uri;
        proxy_redirect off;
    }
#+END_SRC


** Circus and gunicorn
   pip2 install circus

~/circus.ini
#+BEGIN_SRC 
[circus]
check_delay = 5
endpoint = tcp://127.0.0.1:5555
pubsub_endpoint = tcp://127.0.0.1:5556
statsd = true

[watcher:taiga]
working_dir = /home/zcsong/taiga/back
cmd = gunicorn
args = -w 3 -t 60 --pythonpath=. -b 127.0.0.1:8001 taiga.wsgi
uid = zcsong
numprocesses = 1
autostart = true
send_hup = true
stdout_stream.class = FileStream
stdout_stream.filename = /home/zcsong/taiga/logs/gunicorn.stdout.log
stdout_stream.max_bytes = 10485760
stdout_stream.backup_count = 4
stderr_stream.class = FileStream
stderr_stream.filename = /home/zcsong/taiga/logs/gunicorn.stderr.log
stderr_stream.max_bytes = 10485760
stderr_stream.backup_count = 4

[env:taiga]
PATH = /home/zcsong/.virtualenvs/taiga/bin:$PATH
TERM=xterm
SHELL=/bin/bash
USER=zcsong
LANG=en_US.UTF-8
HOME=/home/zcsong
PYTHONPATH=/home/zcsong/.virtualenvs/taiga/lib/python3.4/site-packages

#+END_SRC
** nohup
   nohup python manage.py runserver &
   nohup command > myout.file 2>&1 


** systemd
#+BEGIN_SRC 
[Unit]
Description=taiga back
After=network.target

[Service]
WorkingDirectory=/home/zcsong/taiga/back
User=zcsong
Group=www-data
Environment=PATH=/home/zcsong/.virtualenvs/taiga/bin:$PATH 
Environment=PYTHONPATH=/home/zcsong/.virtualenvs/taiga/lib/python3.4/site-packages

#Restart=on-failure

#ExecStartPre=/usr/sbin/nginx -t -c /etc/nginx/nginx.conf
ExecStart=/home/zcsong/.virtualenvs/taiga/bin/python3.4 manage.py runserver
#ExecReload=/bin/kill -s HUP $MAINPID
#ExecStop=/bin/kill -s QUIT $MAINPID
#PrivateTmp=true
 
[Install]
WantedBy=multi-user.target

#+END_SRC

* taiga on centos 7
https://github.com/taigaio/taiga-doc/blob/master/setup-production.adoc
* gocd
** systemd service file
#+BEGIN_SRC 

GO_SERVER_PORT=8810
export GO_SERVER_PORT
GO_SERVER_SSL_PORT=8811
export GO_SERVER_SSL_PORT
SERVER_WORK_DIR=/home/gocd/go-server
export SERVER_WORK_DIR
DAEMON=Y

#+END_SRC


#+BEGIN_SRC 
[Unit]
Description=go cd
After=network.target

[Service]
Type=forking
PIDFile=/home/gocd/go-server/go-server.pid
Environment=JAVA_HOME=/usr/lib/jvm/jre-1.7.0-openjdk/
ExecStart=/home/gocd/go-server/server.sh
ExecStop=/home/gocd/go-server/stop-server.sh
WorkingDirectory=/home/gocd/go-server
User=go
Group=gitit
#Restart=on-failure

[Install]
WantedBy=multi-user.target

#+END_SRC

* gitlab
** centos 7
https://about.gitlab.com/downloads/#centos7

#+BEGIN_EXAMPLE
sudo yum install curl policycoreutils openssh-server openssh-clients
sudo systemctl enable sshd
sudo systemctl start sshd
sudo yum install postfix
sudo systemctl enable postfix
sudo systemctl start postfix
sudo firewall-cmd --permanent --add-service=http
sudo systemctl reload firewalld

curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
sudo yum install gitlab-ce

sudo gitlab-ctl reconfigure

#+END_EXAMPLE

gitlab-ctl start
gitlab-ctl stop

** nginx 配置

gitlab 使用自带的nginx，默认配置在

#+BEGIN_EXAMPLE
/var/opt/gitlab/nginx
#+END_EXAMPLE

默认使用80端口，和系统的nginx冲突，改为8850
** subgit 

#+BEGIN_EXAMPLE
/opt/subgit-3.2.4/bin/subgit configure svn://192.168.51.239/iplc/esc /var/opt/gitlab/git-data/repositories/iplc/esc.git

#+END_EXAMPLE

Make sure you see the line

#+BEGIN_EXAMPLE
Git repository is served by GitLab, hooks will be installed into 'custom_hooks' directory.
#+END_EXAMPLE



#+BEGIN_EXAMPLE

Using subgit to migrate non-standard layout subversion repository with no branches, tags and trunk

Tag: git,svn,subgit

I have been using subgit to convert my subversion repository to git. Unfortunately, I have a sub-project in one of my branches which is not in standard layout. The non-standard sub-project is not included in the conversion.

The content of the sub-project is

/my-subproject
   file1
   dir1
     subdir1-file1
   file2

Is there a way to specify it in the mapping in subgit.conf? Below is the mapping in subgit.conf.

Eg.

[git "my-subproject"]
translationRoot = my-subprojcet
repository = /var/git/my-subproject.git
pathEncoding=UTF-8

trunk = trunk:refs/heads/master
branches = branches/*:refs/heads/*
shelves = shelves/*:refs/shelves/*
tags = tags/*:refs/tags/*

Best How To :

As found in subgit project mapping the configuration should be

[git "my-subproject"]
repository = /var/git/my-subproject.git

translationRoot = /

trunk = /my-subproject:refs/heads/master
branches = branches/project/*:refs/heads/*
shelves = shelves/project/*:refs/shelves/*
tags = tags/project/*:refs/tags/*

I hope this would be useful.

#+END_EXAMPLE



** sync2git
https://github.com/dpocock/sync2git

** gitlab 安装以及管理
https://www.jianshu.com/p/37abb00e4b67

Gitlab 默认安装文件位置

1 项目文件位置gitlab代码放在/var/opt/gitlab/git-data/下）

2 配置文件目录  /etc/gitlab/gitlab.rb修改完配置后执行gitlab-ctl reconfigure生效

3安装文件目录   /var/opt/gitlab/

4备份文件目录   /var/opt/gitlab/backups

5 日志位置：/var/log/gitlab

6 部分配置文件地址/opt/gitlab/embedded/service/gitlab-rails/config

vim gitlab.yml 修改服务器的ip地址 ，邮件发送名称

7 文件上传位置/var/opt/gitlab/gitlab-rails/uploads/

gitlab安装以后有两个目录：

           一个在/opt/gitlab，这里都是程序文件，不包含数据。

            另一个在/var/opt/gitlab，这里都输数据文件。

安装后的配置文件目录：

主文件：/etc/gitlab/  

主配置文件：/var/opt/gitlab/

 /opt/gitlab/

日志目录：/var/log/gitlab/


gitlab-ctl

tail #查看所有日志gitlab-ctl tail nginx/gitlab_access.log #查看nginx访问日志cd op

迁移需要/etc/gitlab/gitlab.rb

6 查看版本的信息

cat /opt/gitlab/embedded/service/gitlab-rails/VERSION

* npm国内镜像设置
** 使用淘宝镜像
https://npm.taobao.org/


你可以使用我们定制的 cnpm (gzip 压缩支持) 命令行工具代替默认的 npm:

$ npm install -g cnpm --registry=https://registry.npm.taobao.org


安装模块

从 registry.npm.taobao.org 安装所有模块. 当安装的时候发现安装的模块还没有同步过来, 淘宝 NPM 会自动在后台进行同步, 并且会让你从官方 NPM registry.npmjs.org 进行安装. 下次你再安装这个模块的时候, 就会直接从 淘宝 NPM 安装了.

$ cnpm install [name]

同步模块

直接通过 sync 命令马上同步一个模块, 只有 cnpm 命令行才有此功能:

$ cnpm sync connect

当然, 你可以直接通过 web 方式来同步: /sync/connect

$ open https://npm.taobao.org/sync/connect

其它命令

支持 npm 除了 publish 之外的所有命令, 如:

$ cnpm info connect



** 
方法一

国内镜像源，http://cnpmjs.org

1、通过config命令，

npm config set registry http://registry.cnpmjs.org
npm info underscore （如果上面配置正确这个命令会有字符串response）

或者

npm install -g cnpm --registry=http://r.cnpmjs.org

registry 参数的作用就是指向需要 download 的仓库。 cnpm 跟国外的 npm 是同步的，只要 npm 有更新，cnpm 就会跟着一起更新。

2、也可以安装 cnpm，安装好了之后使用 cnpm 来下载文件，原理跟上面是一样的，命令如下：

cnpm install -g package_name

3、编辑 ~/.npmrc 加入下面内容

registry = https://registry.npm.taobao.org

方法二

使用代理


# 设置代理地址和端口
npm config set proxy=http://127.0.0.1:8086

# 设置 https 的代理
npm config set https_proxy=http://127.0.0.1:8086

# 修改registry为npm默认镜像
npm config set registry=http://registry.npmjs.org

开启本地代理，npm 走你~

方法三

直接下载到本地。

直接把文件 download 下来，然后放到 node_module 之中就行了。如果是全局模块，找到全局 node_module 的位置，然后解压放进去就行了。

** 


npm

npm --registry=https://registry.npm.taobao.org install

electron

## Electron Mirror of China
ELECTRON_MIRROR="https://npm.taobao.org/mirrors/electron/"

phantomjs

PHANTOMJS_CDNURL=http://cnpmjs.org/downloads npm install phantomjs

chromedirver

CHROMEDRIVER_CDNURL=http://npm.taobao.org/mirrors/chromedriver

http://npm.taobao.org/mirrors

    3月26日发布 

* npm tftp 
https://github.com/gagle/node-tftp

npm install tftp -g





* mingw on linux
CRT - c-runtime
The first goal of this library is to provide an alternative C-runtime for x86/x64 Windows operating systems. It shall be compatible to the msvcrt variants. The second goal is that this library provides a build-variant for kernel-mode, too. Additionally it shall provide some optional features well known from the POSIX world. This library is at the moment under construction. We still search for an final name for it. Current suggestions are "ironCrate", and "wormcrt". If you have better suggestions, feel free to contact our developer team.
The version of this library is at the moment 0.0, as this library is in pre-alpha phase. We are at the moment in planning and drafting phase. Before we will change version to 0.1 at least some basics have to be present (build-environment, startup-code, partial function set for threading). 
** compile on centos7
*** need to download the following packages
    - binutils
    - GCC
    - Mingw-w64

*** archlinux 
community/mingw-w64-binutils 2.25.1-1 (mingw-w64-toolchain mingw-w64) [installed]
    Cross binutils for the MinGW-w64 cross-compiler
community/mingw-w64-crt 4.0.4-1 (mingw-w64-toolchain mingw-w64) [installed]
    MinGW-w64 CRT for Windows
community/mingw-w64-gcc 5.3.0-1 (mingw-w64-toolchain mingw-w64) [installed: 5.2.0-2]
    Cross GCC for the MinGW-w64 cross-compiler
community/mingw-w64-headers 4.0.4-1 (mingw-w64-toolchain mingw-w64) [installed]
    MinGW-w64 headers for Windows



** linux 下构建 MingGW-w64 交叉编译工具



有不少人在windows下做ffmpeg开发时，喜欢使用mingw或mingw-w64，且不说这种ffmpeg构建方式比较复杂，还有一个缺点，就是gcc在windows下执行速度实在令人着急。笔者推荐大家在linux下使用mingw-w64交叉编译的方式，比较省时，而且相对来说比较简单。

本文所要讲述的就是如何在linux下构建MingGW-w64交叉编译工具。

别紧张，没那么复杂。网络上已经有很多类似的工具链构建脚本，本文使用的就是zeranoe的脚本。（最新版脚本下载地址）

交叉编译工具构建步骤：
1. 下载脚本

wget http://zeranoe.com/scripts/mingw_w64_build/mingw-w64-build-3.5.8

2. 修改脚本权限，使其可执行

chmod 755 mingw-w64-build-3.5.8

3. 执行以下命令，查看脚本可用选项

./mingw-w64-build-3.5.8 --help

4. 开始构建交叉编译工具

./mingw-w64-build-3.5.8 --build-type=win32 --default-configure --pthreads-w32-ver=2-9-1 --gcc-langs=all --clean-build --enable-gendef

好了，就这么简单。接下来要做的就是等着脚本执行完成即可（当然了，脚本执行需要依赖一些运行环境，如果你的系统尚未安装这些工具，脚本会提示你，根据提示进行安装就ok了）。

** 
As amazing as it may first seem, the MinGW-w64 project allows users to compile native Windows binaries on Linux.
http://www.blogcompiler.com/2010/07/11/compile-for-windows-on-linux/


Compile for Windows on Linux
Introduction

In an earlier post, we saw how to obtain gcc on windows, using the MinGW-w64 suite. However, users familiar to gcc are often using one of the operating systems of the Unix family, such as Linux. As amazing as it may first seem, the MinGW-w64 project allows users to compile native Windows binaries on Linux. This concept of targeting a different platform than the compiler is running on is however not new, and is known as cross-compilation.

Cross-compiling Windows binaries on Linux may have many benefits to it.

    Increased compilation speed. Linux is generally faster than Windows with the mingw toolchain.
    Reduced operating system complexity. On cross-platform projects that are also built on Linux, we can get one less operating system to maintain.
    Access to Unix build tools. Build tools such as make, autoconf, automake and Unix utilities as grep, sed, and cat, to mention a few, become available for use in Windows builds as well. Even though projects such as MSYS port a few of these utilities to Windows, the performance is generally lower, and the versions are older and less supported than the native Unix counterparts. Also, if you already have a build environment set up under Linux, you don’t have to set it up again on Windows, but just use the existing one.
    Lower license costs. As we know, Windows costs in terms of license fees. Building on Linux, developers do not need to have  a Windows installation on their machines, but maybe just a central Windows installation for testing purposes.

How It Works

On a Linux build environment, a gcc that compiles native binaries is usually installed in “/usr/bin”. Native headers and libraries are in turn found in “/usr/include” and “/usr/lib”, respectively. We can see that all these directories are rooted in “/usr”.

Any number of cross-compiler environments can be installed on the same system, as long as they are rooted in different directories. In our example, we will use “/opt/mingw32” and “/opt/mingw64” as root directories for the new build environments. Now, we would perhaps expect to find “/opt/mingw32/bin/gcc” and “/opt/mingw64/bin/gcc”, but we instead see “/opt/mingw32/bin/i686-w64-mingw32-gcc” and “/opt/mingw64/bin/x86_64-w64-mingw32-gcc”. The reason for this is that we  (and configure scripts) should be able to pick the “right” gcc, even if  we have multiple compilers in the PATH environment variable. If they were all named gcc, cross-compiling would easily become messy.
Cross-World Hello

    Go to the MinGW-w64 download page. We need two toolchains – one for targeting win32 and another for targeting win64.Open “Toolchains targetting Win32” , followed by “Automated Builds”, “mingw-builds” and a recent version (e.g. mingw-w64-bin_x86_64-linux_20131228.tar.bz2).

    Now do the same for “Toolchains targetting Win64” (e.g. mingw-w64-bin_x86_64-linux_20131228.tar.bz2).

    There are some notes on the package naming convention below to help you pick the right one. Also note that the direct links above might be to older versions when you read this — so please check the directory structure for updates.
    Unpack the first archive to /opt/mingw32 and the second to /opt/mingw64.
    In a text editor (e.g. gedit or nano), paste in the little greeting-code and save it to /tmp/hello.c
    #include <stdio.h>

    int main()

    {

    printf("Hello World!\n");

    return 0;

    }
    Compile it for both 32- and 64-bit Windows with the following commands.

        /opt/mingw32/bin/i686-w64-mingw32-gcc /tmp/hello.c -o /tmp/hello-w32.exe
        /opt/mingw64/bin/x86_64-w64-mingw32-gcc /tmp/hello.c -o /tmp/hello-w64.exe

    Run “hello-w32.exe” on 32-bit Windows, and “hello-w64.exe” on 64-bit Windows.

 

In order to build useful applications, it is convenient to use existing libraries such as the OpenSSL library on Windows.

Package Naming Conventions

As we saw on the MinGW-w64 download page, there are a lot of available packages with only subtle and perhaps confusing name differences. The automatically built packages have the following generic naming pattern.

    mingw-TARGET-bin_HOST_DATE.PKG

    TARGET states which platform we want the compiled binaries to run, and can be either “w32” (32-bit Windows) or “w64” (64-bit Windows).
    HOST gives the host system, that is, the system on which the compiler binaries themselves are run. Thus, we are cross-compiling if HOST is different from TARGET. If we have a Intel 32-bit Linux distribution, we can pick a HOST value of “i686-linux”, from a 64-bit Linux host we would choose “x86_64-linux”, and from 32-bit Windows we can choose “i686-mingw”.
    DATE is the date, in the form YYYYMMDD, when the automatic build was created.
    PKG is the compressed archive format, such as “zip”, “tar.bz2” and such. Generally, zip archives contain binaries that run on Windows, all other archives contains binaries that run on Linux.

Running the Binaries

Using Wine, we can even test the binaries directly from Linux. However, this only works on 32-bit Windows binaries and is not perfect due to bugs and missing features in Wine itself. After downloading and installing Wine for our distribution, we can test our program above by running “wine hello-w32.exe”.
Note that 64-bit Windows can run 32-bit binaries due to an emulation layer called Windows 32-bit On Windows 64-bit, but native binaries are more efficient.




** 
https://wiki.wxwidgets.org/Cross-Compiling_Under_Linux

Flags

You might need these flags when compiling:

-Wl,--subsystem,windows -mwindows \
-DWINVER=0x0400 -D__WIN95__ -D__GNUWIN32__ \
-DSTRICT -DHAVE_W32API_H -D__WXMSW__ -D__WINDOWS__

And these while linking:

-lregex -lpng -ljpeg -lzlib -ltiff -lstdc++ -lgcc -lodbc32 -lwsock32 -lwinspool -lwinmm -lshell32 \
-lcomctl32 -lctl3d32 -lodbc32 -ladvapi32 -lodbc32 -lwsock32 -lopengl32 -lglu32 -lole32 -loleaut32 \
-luuid

** Build_System_Management

 https://wiki.wxwidgets.org/Build_System_Management


*** common 编译

**** Link error: undefined reference to `htonl@4' with MinGW 

 http://mingw.5.n7.nabble.com/Link-error-undefined-reference-to-htonl-4-with-MinGW-td502.html



 Markus Selve wrote:

 > selvem@B5561X1D ~/test
 > $ gcc -Wall -lws2_32  -o conv conv.o
 > conv.o(.text+0x47):conv.c: undefined reference to `htonl@4'

 Congratulations, you just made probably the two most common mingw
 errors.  If it makes you feel any better the archives of this list are
 bulging at the seams with people making these mistakes over and over.

 First, the order you specify things on the gcc command line matters.  If
 A depends on B then B must come after A.  So put -lws2_32 after conv.o.

 Second, you did not call WSAStartup() which is required before using ANY
 socket function.  For something like htonl() it probably doesn't matter,
 but for any program that actually does anything with sockets it will not
 work.  <http://www.mingw.org/MinGWiki/index.php/sockets> 
**** libwsock32.a位置
 在centos上是在这里
 /usr/i686-w64-mingw32/sys-root/mingw/lib/
 在archlinux是在这里

 /usr/i686-w64-mingw32/lib/


 所以在archlinux 作个软连接, 保持和centos上一致

* compile 编译问题
** Compiling problems: cannot find crt1.o

 http://stackoverflow.com/questions/91576/crti-o-file-missing

 gcc -B/usr/lib/x86_64-linux-gnu hello.c

 So, you can just add -B/usr/lib/x86_64-linux-gnu to the CFLAGS variable in your Makefile.


*** LIBRARY_PATH

 Run this to see where these files are located

 $ find /usr/ -name crti*
 /usr/lib/x86_64-linux-gnu/crti.o

 then add this path to LIBRARY_PATH variable

 $ export LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LIBRARY_PATH



*** gcc -B
 gcc -B/usr/lib/x86_64-linux-gnu hello.c

 So, you can just add -B/usr/lib/x86_64-linux-gnu to the CFLAGS variable in your Makefile.

*** sysroot


 Even I got the same compilation error when I was cross compiling i686-cm-linux-gcc.

 The below compilation option solved my problem

 $ i686-cm-linux-gcc a.c --sysroot=/opt/toolchain/i686-cm-linux-gcc

 Note: The sysroot should point to compiler directory where usr/include available

 In my case the toolchain is installed at /opt/toolchain/i686-cm-linux-gcc directory and usr/include is also available in the same directory

 =========================================

 his solved for me (cross compiling pjsip for ARM):

 export LDFLAGS='--sysroot=/home/me/<path-to-my-sysroot-parent>/sysroot'

** flatcc 在centos 7 上编译
   用 ninja 方式不行, 改用make 方式
   - 进入scripts目录
     cp build.cfg.make build.cfg
     然后再上层目录执行 ./scritps/build.sh
** cpb编译
*** centos 
  yum whatprovides libstdc++.so.6

 然后会提示哪个安装包有这个库文件如下：


  yum install libstdc++-4.8.2-16.el7.i686
 s
   
*** 
 　(02:37:00 PM) zcsong: rootfs.img最大多少,　
 (02:37:20 PM) zcsong: 我现在是１２ｍ 烧不成功
 (02:37:57 PM) 王磊: 我的是11点几M
 (02:38:25 PM) 王磊: 总共的空间只有11.5M
 (02:38:42 PM) 王磊: 你把你的打包的脚本发过来看下
 (02:39:14 PM) zcsong: 好的,　我再删除一些文件吧,
 (02:39:33 PM) zcsong: 我放到里边的东西比较多
  (04:00:40 PM) zcsong: app升级的ubi-header.img是怎么生成的??
 (04:00:55 PM) 王磊: 现在不用了 
 (04:01:01 PM) zcsong: ??
 (04:01:14 PM) 王磊: 那个是之前的nand使用的
 (04:01:20 PM) zcsong: 那升级怎么是什么文件
 (04:01:45 PM) zcsong: 直接tar.bz2
 (04:01:46 PM) zcsong: ??
 (04:02:18 PM) 王磊: 直接升级到nor里面
 (04:02:24 PM) 王磊: 使用rootfs.img
 (04:02:41 PM) 王磊: 如果要是用nand，可以使用ubi打包 
 (04:03:07 PM) 王磊: ubi的打包工具我也有 
 (04:03:55 PM) 王磊: 你现在要用ubi吗
 (04:05:59 PM) zcsong: 先给我吧,　
 (04:06:39 PM) zcsong: 以后要用这种模式吗?
 (04:07:04 PM) 王磊: 不用 
 (04:07:16 PM) 王磊: 后面都用nor的了
 (04:07:26 PM) 王磊: 后面用两块nor了
 (04:12:43 PM) zcsong: 哦,　那就不用了
 (04:13:13 PM) 王磊: 恩 

*** cpb需要创建的软连接
 编译不过,　且表现为头文件或者类型没有定义,　定位为搜索路径不对, 原来的有些软连接都是在另外一台机器上的绝对路径.要根据自己的环境作相应的配置


 find . -type l | grep -v bin | grep -v etc |grep -v '\.so.*'

 find . -type l | grep -v bin | grep -v etc |grep -v '\.so.*' | xargs ls -l | grep disk230


 ./linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm -> /disk230/cpb_devkit/kernel/arch/arm/include/asm
 ./linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux -> /disk230/cpb_devkit/kernel/include/linux/
 ./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include
 ./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/lib
 ./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/include
 ./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/lib
 ./linux-devkit/sysroots/i686-arago-linux/usr/include/linux -> /disk230/cpb_devkit/kernel/include/linux/
 ./usr -> /disk230/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/

 #+BEGIN_SRC sh

 BASE_DIR=/opt/cpb_devkit

 rm -f ${BASE_DIR}/usr
 ln -s ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/ ${BASE_DIR}/usr

 rm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm
 ln -s  ${BASE_DIR}/kernel/arch/arm/include/asm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm 

 rm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux
 ln -s ${BASE_DIR}/kernel/include/linux/ ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux

 rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include
 ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include

 rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf
 ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/lib ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf 

 rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include
 ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/include ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include

 rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf
 ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/lib ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf

 rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/include/linux
 ln -s ${BASE_DIR}/kernel/include/linux/ ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/include/linux
 #+END_SRC
** gdb cross compile
 https://sourceware.org/gdb/wiki/BuildingCrossGDBandGDBserver

 #+BEGIN_EXAMPLE

  When using autoconf, there are three system definitions (or machine definitions)
  that are used to identify the “actors” in the build process; (...) These three definitions are:

 host

     The system that is going to run the software once it is built. Once the software
     has been built, it will execute on this particular system.

 build

     The system where the build process is being executed. For most uses this
     would be the same as the host system, but in case of cross-compilation
     the two obviously differ.

 target

     The system against which the software being built will run on. This only exists, or rather
     has a meaning, when the software being built may interact specifically with a
     system that differs from the one it's being executed on (our host). This is the case
     for compilers, debuggers, profilers and analyzers and other tools in general.

 #+END_EXAMPLE

*** gdb 在ppc上编译　
 config_ppc.mk
 #+BEGIN_SRC 
 GDB_VERSION=7.12
 CLEAN_DIRS+=gdb

 gdb:$(PACKET_DIR)/gdb-$(GDB_VERSION).tar.gz 
	 $(EXTRACT_GZ)
	 cd $@ &&  ./configure --target=ppc-linux

 .gdb: gdb
	 cd $< && make

 #+END_SRC

*** gdb 
 #+BEGIN_SRC 
 /path/to/gdb-src/configure --target=arm-linux-gnueabi

 #+END_SRC
*** gdb server

 #+BEGIN_SRC 

 /path/to/gdb-src/gdb/gdbserver/configure --host=arm-linux-gnueabi
 #+END_SRC

** gcc 搜索目录
*** gcc __STDC_HOSTED__

 __STDC_HOSTED__ 如果编译器的目标系统环境中包含完整的标准C库，那么这个宏就定义为1，否则宏的值为0

 This macro is defined, with value 1, if the compiler's target is a hosted environment. A hosted environment has the complete facilities of the standard C library available. 

**** C standard library
 https://en.wikipedia.org/wiki/C_standard_library


 According to the C standard the macro __STDC_HOSTED__ shall be defined to 1 if the implementation is hosted. A hosted implementation has all the headers specified by the C standard. An implementation can also be freestanding which means that these headers will not be present. If an implementation is freestanding, it shall define __STDC_HOSTED__ to 0.

 http://gcc.gnu.org/onlinedocs/gcc/Standards.html
 GCC aims towards being usable as a conforming freestanding implementation, or as the compiler for a conforming hosted implementation. By default, it will act as the compiler for a hosted implementation, defining __STDC_HOSTED__ as 1 and presuming that when the names of ISO C functions are used, they have the semantics defined in the standard. To make it act as a conforming freestanding implementation for a freestanding environment, use the option -ffreestanding; it will then define __STDC_HOSTED__ to 0 and not make assumptions about the meanings of function names from the standard library, with exceptions noted below. To build an OS kernel, you may well still need to make your own arrangements for linking and startup. See Options Controlling C Dialect. 
**** include_next
 首先，我将会说明一下这条指令的功能，然后说明一下为什么要引人这条指令，希望能说个明白。

 #include_next和#include指令一样，也是包含一个头文件，它们的不同地方是包含的路径不一样。

 #include_next的意思就是“包含指定的这个文件所在的路径的后面路径的那个文件”，听起来是不是很坳口，我自己也觉得是这样，但下面举个例子说明就清楚了。

 例如有个搜索路径链，在#include中，它们的搜索顺序依次是A，B，C，D和E。在B目录中有个头文件叫a.h，在D目录中也有个头文件叫a.h，如果在我们的源代码中这样写#include <a.h>，那么我们就会包含的是B目录中的a.h头文件，如果我们这样写#include_next <a.h>那么我们就会包含的是D目录中的a.h头文件。#include_next <a.h>的意思按我们上面的引号包含中的解释来说就是“在B目录中的a.h头文件后面的目录路径（即C，D和E）中搜索a.h头文件并包含进来）。#include_next <a.h>的操作会是这样的，它将在A，B，C，D和E目录中依次搜索a.h头文件，那么首先它会在B目录中搜索到a.h头文件，那它就会以B目录作为分割点，搜索B目录后面的目录（C，D和E），然后在这后面的目录中搜索a.h头文件，并把在这之后搜索到的a.h头文件包含进来。这样说的话大家应该清楚了吧。

 

 还有一点是#include_next是不区分<>和""的包含形式的。

 

 现在来说说为什么要引人这条指令！

 假如，你要创建一个新的头文件，而这个新的头文件和现在已有的头文件有相同的名字，而且你想用你的这个新的头文件，那么你要做的就是把这个新的头文件放在#include指令的搜索路径的前面，即是在旧的头文件的前面新的头文件首先被搜索到，这样你就可以使用你这个新的头文件。但是你在另一个源代码文件中想使用旧的头文件了，那怎么办！有个办法就是使用绝对路径来搜索，那么就不存在这样的问题了。问题出在，如果我们把头文件的位置移动了，移到了其它的目录里了，那我们就得在相应的源码文件中修改这个包含的绝对路径，如果一个源码文件还好，但如果是大型工程的话，修改的地方多了就容易出问题。

 又进一步说，如果你这个新的头文件引用了旧的头文件，而这个新的头文件如果没有使用只编译一次的预处理语句包含（即#ifndef，#endif等），那么就会陷入一个无限的递归包含中，这个新的头文件就会无限的包含自己，就会出现一个致命的错误。如果我们使用#include_next就会避免这样的问题。

 在标准的C中，这没有一个办法来解决上面的问题的，因此GNU就引人了这个指令#include_next。

 

 下面再举一个#include_next的例子。

 假设你用-I选项指定了一个编译包含的路径 '-I /usr/local/include'，这个路径下面有个signal.h的头文件，在系统的'/usr/include'下也有个signal.h头文件，我们知道-I选项的路径首先搜索。如果我们这样 #include <signal.h> 包含，就会包含进/usr/local/include下的signal.h头文件；如果是 #include_next <signal.h>，就会包含 '/usr/include'下的signal.h头文件。

 GNU建议一般没有其它可取代的办法的情况下才使用#include_next的。

 

 又一个例子，如在系统头文件stdio.h中，里面有个函数（应该说是一个宏）getc，它从标准输入中读取一个字符。你想重新定义一个getc，并放到自己新建的stdio.h文件中，那么你可以这样使用你自定义的getc。

 #include_next "stdio.h"
 #undef getc
 #define getc(fp) ((int)'x')

 

 更多的说明请参考GNU的官方文档和GCC文档。
 http://www.delorie.com/gnu/docs/gcc/cpp_11.html

*** gcc 信息查看
 ./arm-linux-gnueabihf-gcc --help

   -dumpspecs               Display all of the built in spec strings
   -dumpversion             Display the version of the compiler
   -dumpmachine             Display the compiler's target processor
   -print-search-dirs       Display the directories in the compiler's search path
   -print-libgcc-file-name  Display the name of the compiler's companion library
   -print-file-name=<lib>   Display the full path to library <lib>
   -print-prog-name=<prog>  Display the full path to compiler component <prog>
   -print-multiarch         Display the target's normalized GNU triplet, used as
                            a component in the library path
   -print-multi-directory   Display the root directory for versions of libgcc
   -print-multi-lib         Display the mapping between command line options and
                            multiple library search directories
   -print-multi-os-directory Display the relative path to OS libraries
   -print-sysroot           Display the target libraries directory
   -print-sysroot-headers-suffix Display the sysroot suffix used to find headers


 ./arm-linux-gnueabihf-gcc -v

 Using built-in specs.
 COLLECT_GCC=./arm-linux-gnueabihf-gcc
 COLLECT_LTO_WRAPPER=/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/4.7.3/lto-wrapper
 Target: arm-linux-gnueabihf
 Configured with: /cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/src/gcc-linaro-4.7-2013.03/configure --build=i686-build_pc-linux-gnu --host=i686-build_pc-linux-gnu --target=arm-linux-gnueabihf --prefix=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install --with-sysroot=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install/arm-linux-gnueabihf/libc --enable-languages=c,c++,fortran --enable-multilib --with-arch=armv7-a --with-tune=cortex-a9 --with-fpu=vfpv3-d16 --with-float=hard --with-pkgversion='crosstool-NG linaro-1.13.1-4.7-2013.03-20130313 - Linaro GCC 2013.03' --with-bugurl=https://bugs.launchpad.net/gcc-linaro --enable-__cxa_atexit --enable-libmudflap --enable-libgomp --enable-libssp --with-gmp=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-mpfr=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-mpc=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-ppl=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-cloog=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-libelf=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-host-libstdcxx='-L/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static/lib -lpwl' --enable-threads=posix --disable-libstdcxx-pch --enable-linker-build-id --enable-gold --with-local-prefix=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install/arm-linux-gnueabihf/libc --enable-c99 --enable-long-long --with-mode=thumb
 Thread model: posix
 gcc version 4.7.3 20130226 (prerelease) (crosstool-NG linaro-1.13.1-4.7-2013.03-20130313 - Linaro GCC 2013.03) 

***  gcc查找头文件的顺序

 刚刚翻了翻gcc查找include的头文件的优先级，首先会在当前目录下找，假如没有找到，会在以下几个地方找：
 1.编译的时候指定
 2.gcc的specs里
 3.使用-I参数指定的路径
 4.gcc环境变量设置（C_INCLUDE_PATH）

 而在这四个当中，-I参数指定的路径优先级最高。在gcc的手册里是这么说的：

        -I dir
            Add the directory dir to the list of directories to be searched for header files.  Directories named
            by -I are searched before the standard system include directories.  If the directory dir is a stan-
            dard system include directory, the option is ignored to ensure that the default search order for sys-
            tem directories and the special treatment of system headers are not defeated .

 使用-I参数指定的路径会在标准的系统include路径之前被搜索。

 简单写一行shell，就能看到include的搜索的顺序了。
 echo 'main(){}' | gcc -E -v  -

 #include "..." 搜索从这里开始：
 #include <...> 搜索从这里开始：
 /usr/local/include
 /usr/lib/gcc/i386-redhat-linux/4.1.2/include
 /usr/include

 加上-I参数 
 echo 'main(){}' | gcc -E -v  -I /home/chengcheng/mmsapp/include -,结果则是
 /home/chengcheng/mmsapp/include
 /usr/local/include
 /usr/lib/gcc/i386-redhat-linux/4.1.2/include
 /usr/include



 =================================


   -print-search-dirs       Display the directories in the compiler's search path
   -print-libgcc-file-name  Display the name of the compiler's companion library
   -print-file-name=<lib>   Display the full path to library <lib>
   -print-prog-name=<prog>  Display the full path to compiler component <prog>
   -print-multiarch         Display the target's normalized GNU triplet, used as
                            a component in the library path
   -print-multi-directory   Display the root directory for versions of libgcc
   -print-multi-lib         Display the mapping between command line options and
                            multiple library search directories
   -print-multi-os-directory Display the relative path to OS libraries
   -print-sysroot           Display the target libraries directory
   -print-sysroot-headers-suffix Display the sysroot suffix used to find headers

*** ./arm-linux-gnueabihf-gcc -print-search-dirs

 install: /home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/
 programs: =/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/arm-linux-gnueabihf/
 libraries: =/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf/

*** ./arm-linux-gnueabihf-gcc -print-multi-directory

* lighttpd设置(调试本地fastcgi)

** 编辑/etc/lighttpd/lighttpd.conf
修改

#+BEGIN_SRC 
server.username         = "songzc"
server.groupname        = "songzc"
server.document-root = "/home/songzc/codbase/exam/Vue_Full_Project/www"

#+END_SRC
  

** 添加 /etc/lighttpd/conf.d/fastcgi.conf

#+BEGIN_SRC 

server.modules += ( "mod_fastcgi" )
fastcgi.debug = 1
#server.indexfiles += ( "dispatch.fcgi" ) #this is deprecated
#index-file.names += ( "dispatch.fcgi" ) #dispatch.fcgi if rails specified
#server.error-handler-404   = "/dispatch.fcgi" #too
fastcgi.server = (
    ".cgi" => (
    "localhost" => ( 
       "socket" => "/run/lighttpd/rails-fastcgi.sock",
        "bin-path" => "/home/songzc/codbase/330/acum/dist/i386/agioe_fcgi.cgi",
 #   "bin-path" => "/tmp/test.fastcgi",
        "max-procs" => 1,
"check-local" => "disable"
                )
    )
    
)

include "conf.d/fastcgi.conf"

#+END_SRC


** 手动执行ligttpd
#+BEGIN_SRC 
chmod 777  /run/lighttpd/

export LD_LIBRARY_PATH=/home/songzc/codbase/rail_robot/dist/i386:$LD_LIBRARY_PATH 
lighttpd -D -f /etc/lighttpd/lighttpd.conf
#+END_SRC

* cmake cross compile

http://www.vtk.org/Wiki/CMake_Cross_Compiling


#+BEGIN_EXAMPLE

Setting up the system and toolchain

When cross compiling, CMake cannot guess the target system, so you have to preset some CMake variables, e.g. using a toolchain file. The following variables have to be preset:

CMAKE_SYSTEM_NAME 
    this one is mandatory, it is the name of the target system, i.e. the same as CMAKE_SYSTEM_NAME would have if CMake would run on the target system. Typical examples are "Linux" and "Windows". This variable is used for constructing the file names of the platform files like Linux.cmake or Windows-gcc.cmake. If your target is an embedded system without OS set CMAKE_SYSTEM_NAME to "Generic". If CMAKE_SYSTEM_NAME is preset, the CMake variable CMAKE_CROSSCOMPILING is automatically set to TRUE, so this can be used for testing in the CMake files. 
CMAKE_SYSTEM_VERSION 
    optional, version of your target system, not used very much. 
CMAKE_SYSTEM_PROCESSOR 
    optional, processor (or hardware) of the target system. This variable is not used very much except for one purpose, it is used to load a CMAKE_SYSTEM_NAME-compiler-CMAKE_SYSTEM_PROCESSOR.cmake file, which can be used to modify settings like compiler flags etc. for the target. You probably only have to set this one if you are using a cross compiler where every target hardware needs special build settings. 

Since CMake cannot guess the target system, it also cannot guess which compiler it should use, so you have to preset this too:

CMAKE_C_COMPILER 
    the C compiler executable, may be the full path or just the filename. If it is specified with full path, then this path will be prefered when searching the C++ compiler and the other tools (binutils, linker, etc.). If this compiler is a gcc-cross compiler with a prefixed name (e.g. "arm-elf-gcc") CMake will detect this and automatically find the corresponding C++ compiler (i.e. "arm-elf-c++"). The compiler can also be preset via the CC environment variables. 
CMAKE_CXX_COMPILER 
    the C++ compiler executable, may be the full path or just the filename. It is handled the same way as CMAKE_C_COMPILER. If the toolchain is a GNU toolchain, you only need to set one of both. 

Once the system and the compiler are determined by CMake, it loads the corresponding files in the following order:

    Platform/${CMAKE_SYSTEM_NAME}.cmake (optional, but issues a stern warning)
    Platform/${CMAKE_SYSTEM_NAME}-<compiler>.cmake (optional)
    Platform/${CMAKE_SYSTEM_NAME}-<compiler>-${CMAKE_SYSTEM_PROCESSOR}.cmake (optional)

<compiler> is either the basename of the compiler executable, e.g. "gcc" (this is also used if gcc has a different name) or "cl", or by a compiler id, which is detected by compiling a test source file.

For testing the host system, there is a corresponding set of variables, which is set automatically by CMake:

    CMAKE_HOST_SYSTEM_NAME
    CMAKE_HOST_SYSTEM_VERSION
    CMAKE_HOST_SYSTEM_PROCESSOR
    CMAKE_HOST_SYSTEM

Without cross compiling the variables for the host system and the target system are identical. In most cases you will want to test for the target system, then the same way as without cross compiling use the CMAKE_SYSTEM_xxx variables, this will work both for cross compiling and for native building.

With these variables correctly set, CMake will now use the cross compiling toolchain for building and in the CMakeLists.txt you can still use the CMAKE_SYSTEM_XXX variables for testing for which system you are building. This is already enough to use CMake for cross compiling simple (buildsystem-wise) projects.
Searching and finding external software

Most non-trivial projects will depend on external libraries or tools. CMake offers the FIND_PROGRAM(), FIND_LIBRARY(), FIND_FILE(), FIND_PATH() and FIND_PACKAGE() commands for this purpose. They search the file system in common places for files and return the results. FIND_PACKAGE() is a bit different in that it actually doesn't search itself, but "only" executes FindXXX.cmake modules, which usually call the FIND_PROGRAM(), FIND_LIBRARY(), FIND_FILE() and FIND_PATH() commands then.

When cross compiling e.g. for a target with an ARM processor getting /usr/lib/libjpeg.so as the result of a FIND_PACKAGE(JPEG) wouldn't be much of a help, since this would be the JPEG library for the host system, e.g. an x86 Linux box. So you need to tell CMake to search in other locations. This can be done by setting the following variables:

CMAKE_FIND_ROOT_PATH 
    this is a list of directories, each of the directories listed there will be prepended to each of the search directories of every FIND_XXX() command. So e.g. if your target environment is installed under /opt/eldk/ppc_74xx, set CMAKE_FIND_ROOT_PATH to this directory. Then e.g. FIND_LIBRARY(BZ2_LIB bz2) will search in /opt/eldk/ppc_74xx/lib, /opt/eldk/ppc_74xx/usr/lib, /lib, /usr/lib and so give /opt/eldk/ppc_74xx/usr/lib/libbz2.so as result. By default CMAKE_FIND_ROOT_PATH is empty. If set, at first the directories prefixed with the directories given in CMAKE_FIND_ROOT_PATH will be searched and after that the unprefixed versions of the search directories will be searched. This behaviour can be modified individually for every FIND_XXX() call with the NO_CMAKE_FIND_ROOT_PATH, ONLY_CMAKE_FIND_ROOT_PATH and CMAKE_FIND_ROOT_PATH_BOTH options or the default for all FIND_XXX() commands can be adjusted with the CMAKE_FIND_ROOT_PATH_MODE_PROGRAM, CMAKE_FIND_ROOT_PATH_MODE_LIBRARY and CMAKE_FIND_ROOT_PATH_MODE_INCLUDE variables. If you don't want to use only libraries that come with the toolchain but also build and use additional libraries for your target platform, you should create an install directory for these packages, e.g. $HOME/eldk-ppc_74xx-inst/ and add this to CMAKE_FIND_ROOT_PATH, so the FIND_XXX() commands will search there too. If you then build packages for your target platform, they should be installed into this directory. 
CMAKE_FIND_ROOT_PATH_MODE_PROGRAM
    This sets the default behaviour for the FIND_PROGRAM() command. It can be set to NEVER, ONLY or BOTH (default). If set to NEVER, CMAKE_FIND_ROOT_PATH will not be used for FIND_PROGRAM() calls (except where it is enabled explicitely). If set to ONLY, only the search directories with the prefixes coming from CMAKE_FIND_ROOT_PATH will be used in FIND_PROGRAM(). The default is BOTH, which means that at first the prefixed directories and after that the unprefixed directories will be searched. In most cases FIND_PROGRAM() is used to search for an executable which will then be executed e.g. using EXECUTE_PROCESS() or ADD_CUSTOM_COMMAND(). So in most cases an executable from the build host is required, so usually set CMAKE_FIND_ROOT_PATH_MODE_PROGRAM to NEVER. 
CMAKE_FIND_ROOT_PATH_MODE_LIBRARY
    This is the same as above, but for the FIND_LIBRARY() command. In most cases this is used to find a library which will then be used for linking, so a library for the target is required. So in the common case, set it to ONLY. 
CMAKE_FIND_ROOT_PATH_MODE_INCLUDE
    This is the same as above and used for both FIND_PATH() and FIND_FILE(). In many cases this is used for finding include directories, so the target environment should be searched. So in the common case, set it to ONLY. You may have to adjust this behaviour for some of the FIND_PATH() or FIND_FILE() calls using the NO_CMAKE_FIND_ROOT_PATH, ONLY_CMAKE_FIND_ROOT_PATH and CMAKE_FIND_ROOT_PATH_BOTH options. 

The toolchain file

Defining all the variables mentioned above using -DCMAKE_SYSTEM_NAME etc. would be quite tedious and error prone. To make things easier, there is another cmake variable you can set:

CMAKE_TOOLCHAIN_FILE 
    absolute or relative path to a cmake script which sets up all the toolchain related variables mentioned above 

For instance for crosscompiling from Linux to Embedded Linux on PowerPC this file could look like this:

# this one is important
SET(CMAKE_SYSTEM_NAME Linux)
#this one not so much
SET(CMAKE_SYSTEM_VERSION 1)

# specify the cross compiler
SET(CMAKE_C_COMPILER   /opt/eldk-2007-01-19/usr/bin/ppc_74xx-gcc)
SET(CMAKE_CXX_COMPILER /opt/eldk-2007-01-19/usr/bin/ppc_74xx-g++)

# where is the target environment 
SET(CMAKE_FIND_ROOT_PATH  /opt/eldk-2007-01-19/ppc_74xx /home/alex/eldk-ppc74xx-inst)

# search for programs in the build host directories
SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
# for libraries and headers in the target directories
SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

If this file is named Toolchain-eldk-ppc74xx.cmake and is located in your home directory and you are building in the subdirectory build then you can do:

~/src$ cd build
~/src/build$ cmake -DCMAKE_TOOLCHAIN_FILE=~/Toolchain-eldk-ppc74xx.cmake ..
...

You don't have to write a toolchain file for every piece of software you want to build, the toolchain files are per target platform, i.e. if you are building several software packages all for the same target platform, you have to write only one toolchain file and you can use this for all packages.

If your compiler is not able to build a simple program by default without special flags or files (e.g. linker scripts or memory layout files), the toolchain file as shown above doesn't work. Then you have to force the compiler:

INCLUDE(CMakeForceCompiler)

# this one is important
SET(CMAKE_SYSTEM_NAME eCos)

# specify the cross compiler
CMAKE_FORCE_C_COMPILER(arm-elf-gcc GNU)
CMAKE_FORCE_CXX_COMPILER(arm-elf-g++ GNU)

# where is the target environment 
SET(CMAKE_FIND_ROOT_PATH  /home/alex/src/ecos/install )

# search for programs in the build host directories
SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
# for libraries and headers in the target directories
SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

This is done using the CMAKE_FORCE_XXX_COMPILER() macros. The second argument is the compiler id, which is used by CMake to recognize the compiler.


A toolchain for crosscompiling for Win32 using mingw32 might look like this:

# the name of the target operating system
SET(CMAKE_SYSTEM_NAME Windows)

# which compilers to use for C and C++
SET(CMAKE_C_COMPILER i486-mingw32-gcc)
SET(CMAKE_CXX_COMPILER i486-mingw32-g++)
SET(CMAKE_RC_COMPILER i486-mingw32-windres)

# here is the target environment located
SET(CMAKE_FIND_ROOT_PATH /usr/i486-mingw32)

# adjust the default behaviour of the FIND_XXX() commands:
# search headers and libraries in the target environment, search 
# programs in the host environment
set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

System introspection

Many non-trivial software projects have a set of system introspection tests for finding out properties of the (target) system. In CMake there are macros provided for this purpose like e.g. CHECK_INCLUDE_FILES() or CHECK_C_SOURCE_RUNS(). Most of these tests will internally use either the TRY_COMPILE() or the TRY_RUN() CMake commands. The TRY_COMPILE() commands work as expected also when cross compiling, they will try to compile the piece of code with the cross compiling toolchain, which will give the expected result. All tests using TRY_RUN() internally cannot work, since the created executables cannot run on the build host system. At first TRY_RUN() tries to compile the software, which will work the same way when cross compiling. If this succeeded, it will check the variable CMAKE_CROSSCOMPILING whether the resulting executable is runnable or not. If not, it will create two cache variables, which then have to be set by the user or via the CMake cache. Let's say the command looks like this:

TRY_RUN(SHARED_LIBRARY_PATH_TYPE SHARED_LIBRARY_PATH_INFO_COMPILED
        ${PROJECT_BINARY_DIR}/CMakeTmp
        ${PROJECT_SOURCE_DIR}/CMake/SharedLibraryPathInfo.cxx
        OUTPUT_VARIABLE OUTPUT
        ARGS "LDPATH")

The variable SHARED_LIBRARY_PATH_INFO_COMPILED will be set to the result of the build (i.e. TRUE or FALSE). CMake will create a cache variable SHARED_LIBRARY_PATH_TYPE and preset it to PLEASE_FILL_OUT-FAILED_TO_RUN. This one has to be set to the exit code of the executable if it would have been executed on the target. It will also create a cache variable SHARED_LIBRARY_PATH_TYPE__TRYRUN_OUTPUT and preset it to PLEASE_FILL_OUT-NOTFOUND. This one has to be set to the output the executable prints to stdout and stderr if it is executed on the target. This variable is only created if the TRY_RUN() command was used with the RUN_OUTPUT_VARIABLE or the OUTPUT_VARIABLE argument. You have to fill in appropriate values for these variables. To help you with this CMake tries its best to give you useful information.

To do so CMake creates a file ${CMAKE_BINARY_DIR}/TryRunResults.cmake. There you will find all variables which CMake could not determine, from which CMake file they were called, the source file, the arguments for the executable and the path to the executable. CMake will also copy the executables in the build directory, they have the names cmTryCompileExec-<name of the variable>, e.g. cmTryCompileExec-SHARED_LIBRARY_PATH_TYPE. You can then try to run this executable manually on the actual target platform and check the results.

Once you have these results, they have to get in the CMake cache. You can either use ccmake/CMakeSetup/"make edit_cache" and edit the variables directly in the cache. Then you won't be able to reuse your changes in another build directory or if you remove CMakeCache.txt. The second option is to use the TryRunResults.cmake file. Copy it to a safe location (i.e. where it is not deleted if you delete the build dir) and give it a useful name, e.g. MyProjectTryRunResults-eldk-ppc.cmake. Then edit it so that the SET() commands set the required values. You can the use this file to preload the CMake cache by using the -C option of cmake:

src/build/ $ cmake -C ~/MyProjectTryRunResults-eldk-ppc.cmake .

You don't have to use the other CMake options again, they are now already in the cache. This way you can use MyProjectTryRunResults-eldk-ppc.cmake in multiple build trees and it could also be distributed with your project so it gets easier for other users who want to compile it.

This script may be helpful to automatically populate the TRY_RUN results with those placed in a CMakeCache.txt that were created on the target.
Using executables in the build created during the build

In some cases during a build executables are created which are then used in ADD_CUSTOM_COMMAND() or ADD_CUSTOM_TARGET() during the same build process.

When cross compiling this won't work without modifications because the executables cannot run on the build host. Starting with CMake 2.6 it is possible to "import" executable targets into a CMake project. When cross compiling this has to be used to import executables built in a native build into the cross-build. This can be done like this:

# when crosscompiling import the executable targets from a file
IF(CMAKE_CROSSCOMPILING)
  SET(IMPORT_EXECUTABLES "IMPORTFILE-NOTFOUND" CACHE FILEPATH "Point it to the export file from a native build")
  INCLUDE(${IMPORT_EXECUTABLES})
ENDIF(CMAKE_CROSSCOMPILING)

...


# only build the generator if not crosscompiling
IF(NOT CMAKE_CROSSCOMPILING)
   ADD_EXECUTABLE(mygenerator mygen.cpp)
   TARGET_LINK_LIBRARIES(mygenerator ${SOME_LIBS})
ENDIF(NOT CMAKE_CROSSCOMPILING)

# then use the target name as COMMAND, CMake >= 2.6 knows how to handle this
ADD_CUSTOM_COMMAND(OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   COMMAND mygenerator foo.dat -o ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   DEPENDS foo.dat )


...
# export the generator target to a file, so it can be imported (see above) by another build
# the IF() is not necessary, but makes the intention clearer
IF(NOT CMAKE_CROSSCOMPILING) 
  EXPORT(TARGETS mygenerator FILE ${CMAKE_BINARY_DIR}/ImportExecutables.cmake )
ENDIF(NOT CMAKE_CROSSCOMPILING) 

So during the native build the target "mygenerator" will be built and used in ADD_CUSTOM_COMMAND(). As command only the target name is used. CMake >= 2.6.0 recognizes this and creates the dependencies and will use the path to the created executable when executing the command. At the end the EXPORT() function (since CMake 2.6.0) is called, which "exports" the listed targets to the file ${CMAKE_BINARY_DIR}/ImportExecutables.cmake, which will look like this:

ADD_EXECUTABLE(mygenerator IMPORT)
SET_TARGET_PROPERTIES(mygenerator PROPERTIES 
                      LOCATION /home/alex/build-native/bin/mygenerator )

This file is then included when cross compiling, it either has to be specified using -D or via the cmake GUI. Then later on the command for actually building mygenerator is excluded. In ADD_CUSTOM_COMMAND() mygenerator will be recognized as an imported target and it will be used when executing the command.


If the executable mygenerator also has to be built when cross compiling, then some more logic needs to be added, e.g. like this:

# when crosscompiling import the executable targets from a file
IF(CMAKE_CROSSCOMPILING)
  SET(IMPORT_EXECUTABLES "IMPORTFILE-NOTFOUND" CACHE FILEPATH "Point it to the export file from a native build")
  INCLUDE(${IMPORT_EXECUTABLES})
ENDIF(CMAKE_CROSSCOMPILING)

...

# always build the executable
ADD_EXECUTABLE(mygenerator mygen.cpp)
TARGET_LINK_LIBRARIES(mygenerator ${SOME_LIBS})

# but use different names for the command
IF(CMAKE_CROSSCOMPILING)
   SET(mygenerator_EXE native-mygenerator)
ELSE(CMAKE_CROSSCOMPILING)
   SET(mygenerator_EXE mygenerator)
ENDIF(CMAKE_CROSSCOMPILING)

# then use the target name as COMMAND, CMake >= 2.6 knows how to handle this
ADD_CUSTOM_COMMAND(OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   COMMAND ${mygenerator_EXE} foo.dat -o ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   DEPENDS foo.dat )


...
# export the generator target to a file, so it can be imported (see above) by another build
# the IF() is not necessary, but makes the intention clearer
# use the NAMESPACE option of EXPORT() to get a different target name for mygenerator when exporting
IF(NOT CMAKE_CROSSCOMPILING) 
  EXPORT(TARGETS mygenerator FILE ${CMAKE_BINARY_DIR}/ImportExecutables.cmake NAMESPACE native- )
ENDIF(NOT CMAKE_CROSSCOMPILING) 

Cross compilation for Windows CE

Building for Windows CE requires Visual Studio 2005 or 2008 (No Express Edition!) with at least one installed SDK. If you don't have a specific installation file for your target device it is possible to use the Windows CE 5.0 Standard SDK from http://www.microsoft.com/downloads/details.aspx?familyid=fa1a3d66-3f61-4ddc-9510-ae450e2318c3.

CMake supports Windows CE out of the box since version 2.8.10 when used with the NMake Makefiles generator. To use it you first need the corresponding environment variables set, for which the CMake command env_vs8_wince has been added in the following version. Using 2.8.10 is possible too, if the environment is set manually. To get there start a command prompt and type the following commands:

"%VS80COMNTOOLS%vsvars32.bat"
cmake -E env_vs8_wince "STANDARDSDK_500 (ARMV4I)" > env.bat
env.bat

Then the Makefiles can be generated and built with the following commands:

cmake -G "NMake Makefiles" -DCMAKE_SYSTEM_NAME=WindowsCE -DCMAKE_SYSTEM_PROCESSOR=ARMV4I \path\to\source
cmake --build .

Starting with CMake 2.8.11 it is also possible to create Visual Studio solution for Windows CE targets. Depending on the installed SDKs CMake will accept additional generators. The following command will create Visual Studio 2005 files for the WinCE standard SDK:

cmake -G "Visual Studio 8 2005 STANDARDSDK_500 (ARMV4I)" \path\to\source

To use VS2008 instead of VS2005 simple replace "VS80COMNTOOLS" with "VS90COMNTOOLS", "vs8" with "vs9" and "8 2005" with "9 2008".
Information how to set up various cross compiling toolchains

    [1], detailed instructions for using CMake for the iPhone (external, thirdparty website).
    eldk, embedded Linux cross compiling toolchain from Denx
    mingw - gcc for cross compiling from Linux to Windows
    SDCC - the small devices C compiler
    eCos - the embedded Configurable operating system
    ADSP - the Analog Devices toolchain for their DSPs
    IBM BlueGene/L
    Cray XT3 / Catamount
    Crosstool NG - may be used to easily build various cross compiler toolchain. The produced toolchain seems to work well with CMake cross-compiling.
    MXE - Builds compiler and libraries for cross compiling from Linux to Windows. Comes with CMake toolchain file!

How to cross compile specific projects

    ITK
    ParaView3
    Python

FAQ/Potential Problems

    On mixed 32/64 bit Linux installations cross compilation cannot be used to build for 32/64 bit only.

    FindXXX.cmake modules, which rely on executing a binary tool like pkg-config may have problems, since the pkg-config of the target platform cannot be executed on the host. Tools like pkg-config should be used only optional in FindXXX.cmake files.

    What about Scratchbox ? CMake should work without problems in scratchbox, then it will just work in native mode.

    Can it build software for PS3 ? If you build software for PS3, you build software for two architectures, for the PowerPC and for the cells. This is done using two different toolchains. Currently CMake doesn't support using multiple toolchains in one buildtree or building for multiple target architectures in one build tree. So building for PS3 doesn't work out-of-the-box. It should work using ADD_CUSTOM_COMMAND() or by using two buildtrees.


#+END_EXAMPLE


https://cmake.org/cmake/help/v3.0/manual/cmake-toolchains.7.html

Cross Compiling

If cmake(1) is invoked with the command line parameter -DCMAKE_TOOLCHAIN_FILE=path/to/file, the file will be loaded early to set values for the compilers. A typical cross-compiling toolchain has content such as:

set(CMAKE_SYSTEM_NAME Linux)

set(CMAKE_SYSROOT /home/devel/rasp-pi-rootfs)
set(CMAKE_STAGING_PREFIX /home/devel/stage)

set(CMAKE_C_COMPILER /home/devel/gcc-4.7-linaro-rpi-gnueabihf/bin/arm-linux-gnueabihf-gcc)
set(CMAKE_CXX_COMPILER /home/devel/gcc-4.7-linaro-rpi-gnueabihf/bin/arm-linux-gnueabihf-g++)

set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)

The CMAKE_SYSTEM_NAME is the CMake-identifier of the target platform to build for.

The CMAKE_SYSROOT is optional, and may be specified if a sysroot is available.

The CMAKE_STAGING_PREFIX is also optional. It may be used to specify a path on the host to install to. The CMAKE_INSTALL_PREFIX is always the runtime installation location, even when cross-compiling.

The CMAKE_<LANG>_COMPILER variables may be set to full paths, or to names of compilers to search for in standard locations. In cases where CMake does not have enough information to extract information from the compiler, the CMakeForceCompiler module can be used to bypass some of the checks.

CMake find_* commands will look in the sysroot, and the CMAKE_FIND_ROOT_PATH entries by default in all cases, as well as looking in the host system root prefix. Although this can be controlled on a case-by-case basis, when cross-compiling, it can be useful to exclude looking in either the host or the target for particular artifacts. Generally, includes, libraries and packages should be found in the target system prefixes, whereas executables which must be run as part of the build should be found only on the host and not on the target. This is the purpose of the CMAKE_FIND_ROOT_PATH_MODE_* variables.

Some compilers are inherently cross compilers, such as Clang and the QNX QCC compiler. The CMAKE_<LANG>_COMPILER_TARGET can be set to pass a value to those supported compilers when compiling:

set(CMAKE_SYSTEM_NAME Linux)

set(triple arm-linux-gnueabihf)

set(CMAKE_C_COMPILER clang)
set(CMAKE_C_COMPILER_TARGET ${triple})
set(CMAKE_CXX_COMPILER clang++)
set(CMAKE_CXX_COMPILER_TARGET ${triple})

Or, for QCC:

set(CMAKE_SYSTEM_NAME QNX)

set(arch gcc_ntoarmv7le)

set(CMAKE_C_COMPILER qcc)
set(CMAKE_C_COMPILER_TARGET ${arch})
set(CMAKE_CXX_COMPILER QCC)
set(CMAKE_CXX_COMPILER_TARGET ${arch})

Similarly, some compilers do not ship their own supplementary utilities such as linkers, but provide a way to specify the location of the external toolchain which will be used by the compiler driver. The CMAKE_<LANG>_COMPILER_EXTERNAL_TOOLCHAIN variable can be set in a toolchain file to pass the path to the compiler driver.

The CMAKE_CROSSCOMPILING variable is set to true when CMake is cross-compiling.

* postgresql
** PostgreSQL数据库远程连接功能的开启
 
需要修改连个配置文件，默认位于 安装目录的data子文件夹下。
 
1.postgresql.conf
修改成监听所有ip地址的连接请求，如下：
listen_addresses = '*'  
 
2.pg_hda.conf
在末尾的地方添加一行，如下：
host    all         all         0.0.0.0/0      md5



  
** How to change PostgreSQL user password?
https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password



Then type:

    sudo -u postgres psql

Then:

    \password postgres

Then to quit:

    \q

If that does not work, reconfigure authentication.

Edit /etc/postgresql/9.1/main/pg_hba.conf (path will differ) and change:

    local   all             all                                     peer

to:

    local   all             all                                     md5

Then restart the server:

sudo service postgresql restart

** PostgreSQL Requirements

As of GitLab 9.3, PostgreSQL 9.2 or newer is required, and earlier versions are not supported. We highly recommend users to use at least PostgreSQL 9.6 as this is the PostgreSQL version used for development and testing.

Users using PostgreSQL must ensure the pg_trgm extension is loaded into every GitLab database. This extension can be enabled (using a PostgreSQL super user) by running the following query for every database:

CREATE EXTENSION pg_trgm;

On some systems you may need to install an additional package (e.g. postgresql-contrib) for this extension to become available.
** postgresal使用错误解决

 https://blog.csdn.net/wangyezi19930928/article/details/20358369


 错误：psql: FATAL: Peer authentication failed for user "postgres"

 解决办法如下:

 1）. 运行下面的命令编辑pg_hba.conf文件 sudo vim /etc/postgresql/9.1/main/pg_hba.conf

 2）. 将

  # Database administrative login by Unix domain socket

 local     all      postgres        peer

 改为

 # Database administrative login by Unix domain socket

 local     all     postgres         trust

 3）. 保存后执行下面的命令重新加载配置文件: sudo /etc/init.d/postgresql reload


* python(windows)
** python on windows
*** windows　python 测试脚本环境搭建 (mysys2)
  用纯正的windows环境运行python脚本测试ＡＣＵ,　暂时没有调试成功. 现在用MSYS2代替
  - 下载地址
  　http://msys2.github.io/
  　按照这个地址上的网页步骤安装
  - 安装好后再安装python
    pacman -S python
  - 设置acu协议库的路径(放置libproto.so的目录)
    export LD_LIBRARY_PATH=协议库路径:$LD_LIBRARY_PATH
    这个语句也可以写在.bashrc里,　不用每次都敲一次
  - 执行测试
    现在有２个python文件
    1. msg.py -- 对协议python封装
    2. test.py -- 使用msg.py的例子
  - windown下编译.
    1. 安装mingw编译器
       pacman -S　mingw-w64-i686-gcc
    2. ./config_for_win.sh
    3. make
    4. 生成的libproto放在 hosts/win/lib和src/protocol下



*** 安装时
   - 尽量安装目录自己设置,　且不要太长,
     如设成 D:/python
   - 选择设置path
   - 择安装pip
   - 
*** nose
   - pip install nose
   - 
** pyinstaller

http://www.pyinstaller.org/

** python structure
*** 
http://stackoverflow.com/questions/5548387/python-structures

ypedef struct LibraryInfo
    {
        uint32_t    size;                                // Size of the structure
        char        libName[MAX_LIBRARY_NAME+1];                        // Library name
        char        provider[MAX_LIBRARY_PROVIDER_NAME+1];                  // Provider
        uint32_t    version;                                                    // Library version, i.e: 0x01030005 --> v.01.03.0005  
    } LibraryInfo;  

The equivalent Python Code is:

class LibraryInfo(Structure):  
    _fields_=[("size",c_uint),  
              ("libName",c_char * MAX_LIBRARY_NAME ),  
              ("provider",c_char * MAX_LIBRARY_PROVIDER_NAME),  
              ("version",c_uint)]  


libraryInfo = LibraryInfo()
resCode = QueryLibraryInfo(byref(libraryInfo))

*** 
http://stackoverflow.com/questions/18536182/parsing-binary-data-into-ctypes-structure-object-via-readinto
*** 

#pragma pack(1)
typedef union { 
 unsigned char Mpi;  
 unsigned char Ip[4];  
 unsigned char Mac[6]; 
 } CON_ADR_TYPE; 
 
typedef struct { 
 CON_ADR_TYPE Adr;  
 unsigned char AdrType;  
 unsigned char SlotNr;  
 unsigned char RackNr;  
 } CON_TABLE_TYPE; 
#pragma pack(1)  


from ctypes import *
class CON_ADR_TYPE(Union):
 _pack_=1
 _fields_=[("Mpi", c_byte),
  ("Ip", c_byte*4),
  ("Mac", c_byte*6)]

class CON_TABLE_TYPE(Structure): 
 _fields_=[("Adr", CON_ADR_TYPE),
  ("AdrType", c_byte),
  ("SlotNr", c_byte),
  ("RackNr", c_byte)]
 _pack_=1
*** 


 调用C编写的动态链接库
代码示例

from ctypes import * 
dll = CDLL("add.dll")#加载cdecl的dll。另外加载stdcall的dll的方式是WinDLL("dllpath") 
sum=dll.Add(1, 102) 

若参数为指针

p=1 
sum=dll.sub(2, byref(p))#通过库中的byref关键字来实现 

若参数为结构体
C代码如下：

typedef struct 
{ 
    char words[10]; 
}keywords; 
 
typedef struct 
{ 
    keywords *kws; 
    unsigned int len; 
}outStruct; 
 
extern "C"int __declspec(dllexport) test(outStruct *o); 
 
int test(outStruct *o) 
{ 
    unsigned int i = 4; 
    o->kws = (keywords *)malloc(sizeof(unsigned char) * 10 * i); 
    strcpy(o->kws[0].words, "The First Data"); 
    strcpy(o->kws[1].words, "The Second Data"); 
    o->len = i; 
    return 1; 
} 


Python代码如下：

class keywords(Structure): 
    _fields_ = [('words', c_char *10),] 
 
class outStruct(Structure): 
    _fields_ = [('kws', POINTER(keywords)),('len', c_int),] 
 
o = outStruct() 
dll.test(byref(o)) 
 
print (o.kws[0].words) 
print (o.kws[1].words) 
print (o.len) 

 

 

调用Windows API

#导入ctypes模块 
from ctypes import * 
windll.user32.MessageBoxW(0, '内容！', '标题', 0) 
 
#也可以用以下方式为API函数建立个别名后再进行调用 
MessageBox = windll.user32.MessageBoxW 
MessageBox(0, '内容！', '标题', 0) 
*** 

#+BEGIN_SRC c
typedef struct {
    char words[10];
}keywords;

typedef struct {
    keywords *kws;
    unsigned int len;
}outStruct;

extern "C"int __declspec(dllexport) test(outStruct *o);

int test(outStruct *o)
{
    unsigned int i = 4;

    o->kws = (keywords *)malloc(sizeof(unsigned char) * 10 * i);

    strcpy(o->kws[0].words, "The First Data");

    strcpy(o->kws[1].words, "The Second Data");

    o->len = i;

    return 1;
}

#+END_SRC

#+BEGIN_SRC python
class keywords(Structure):

        _fields_ = [('words', c_char *10),]

 

class outStruct(Structure):

        _fields_ = [('kws', POINTER(keywords)),

                    ('len', c_int),]

o = outStruct()

dll.test(byref(o))

 

print o.kws[0].words;

print o.kws[1].words;

print o.len

#+END_SRC

** python os.path
sys.path is only searched for Python modules. For dynamic linked libraries, the paths searched must be in LD_LIBRARY_PATH. Check if your LD_LIBRARY_PATH includes /usr/local/lib, and if it doesn't, add it and try again.

Some more information (source):

    In Linux, the environment variable LD_LIBRARY_PATH is a colon-separated set of directories where libraries should be searched for first, before the standard set of directories; this is useful when debugging a new library or using a nonstandard library for special purposes. The environment variable LD_PRELOAD lists shared libraries with functions that override the standard set, just as /etc/ld.so.preload does. These are implemented by the loader /lib/ld-linux.so. I should note that, while LD_LIBRARY_PATH works on many Unix-like systems, it doesn't work on all; for example, this functionality is available on HP-UX but as the environment variable SHLIB_PATH, and on AIX this functionality is through the variable LIBPATH (with the same syntax, a colon-separated list).


RUN_LD_PATH

** python load dll error (os error winerror 126)

*** Search Path Used by Windows to Locate a DLL
https://msdn.microsoft.com/en-us/library/7d83bc18.aspx

With both implicit and explicit linking, Windows first searches for "known DLLs", such as Kernel32.dll and User32.dll. Windows then searches for the DLLs in the following sequence:

    The directory where the executable module for the current process is located.

    The current directory.

    The Windows system directory. The GetSystemDirectory function retrieves the path of this directory.

    The Windows directory. The GetWindowsDirectory function retrieves the path of this directory.

    The directories listed in the PATH environment variable.
    System_CAPS_noteNote

    The LIBPATH environment variable is not used.

*** 使用cmd模式，python *.py，会弹出缺少的依赖库名。
libgcc_s_dw2-1.dll

http://blog.csdn.net/aha121/article/details/17054487

最近想把一组api做成一个界面，来控制流程。

问题1：使用IDLE中，直接执行程序报错

Traceback (most recent call last):
  File "E:\study\python\client.py", line 143, in <module>
    gtpdll = CDLL("test.dll")
  File "D:\Python33\lib\ctypes\__init__.py", line 353, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] 找不到指定的模块。

后来捣鼓了好久，才知道是缺少依赖的库。

在一次无意中发现，执行的py文件的时候，使用cmd模式，python *.py，会弹出缺少的依赖库名。

 

问题2：链表结构，就是struct里面有类型为struct的类型。在python中，可以按以下方式初始化

class TEST_LIST(Structure):
        pass
TEST_LIST._fields_= [('prev',POINTER(TEST_LIST)),('next',POINTER(TEST_LIST)),('srcfilename',c_char*256),('destfilename',c_char*256)]

 

问题3：指向指针的指针

在实现的过程中，发现若函数参数为指向指针指针，若使用双层pointer表示，就会有问题，后来使用的是

list = TEST_LIST()
list_p = pointer(list)

然后传参时使用pointer(list_p)

 

问题4：数组初始化

destfile = (c_char*256)()
destfile.value = b'result.txt'

需要注意的是，若C代码中涉及strcmp，那么c_char*256，就需要改成len(destfile.value)

* Linking problems due to symbols with abi::cxx11?
http://stackoverflow.com/questions/36159238/linking-problems-due-to-symbols-with-abicxx11




Disclaimer, the following is not tested in production, use at your own risk.

You can yourself release your library under dual ABI. This is more or less analogous to OSX "fat binary", but built entirely with C++.

The easiest way to do so would be to compile the library twice: with -D_GLIBCXX_USE_CXX11_ABI=0 and with -D_GLIBCXX_USE_CXX11_ABI=1. Place the entire library under two different namespaces depending on the value of the macro:

#if _GLIBCXX_USE_CXX11_ABI
#  define DUAL_ABI cxx11 __attribute__((abi_tag("cxx11")))
#else
#  define DUAL_ABI cxx03
#endif

namespace CryptoPP {
  inline namespace DUAL_ABI {
    // library goes here
  }
}

Now your users can use CryptoPP::whatever as usual, this maps to either CryptoPP::cxx11::whatever or CryptoPP::cxx03::whatever depending on the ABI selected.

Note, the GCC manual says that this method will change mangled names of everything defined in the tagged inline namespace. In my experience this doesn't happen.

The other method would be tagging every class, function, and variable with __attribute__((abi_tag("cxx11"))) if _GLIBCXX_USE_CXX11_ABI is nonzero. This attribute nicely adds [cxx11] to the output of the demangler. I think that using a namespace works just as well though, and requires less modification to the existing code.

In theory you don't need to duplicate the entire library, only functions and classes that use std::string and std::list, and functions and classes that use these functions and classes, and so on recursively. But in practice it's probably not worth the effort, especially if the library is not very big.


#ifdef ABI_CHANGE 
inline namespace abi2 __attribute ((abi_tag)) { 
  class MyType { ... }; 
  MyType fn(); 
} 
#endif

* #CFLAGS+= -D_GLIBCXX_USE_CXX11_ABI=0

 使用 这个定义后 acum_uitls的库和acum一起联调时会死机, 
 查了一下, 发现在库里log出的acumconfig和在acum main里打出来的结构大小不一样.


2016-08-02 12:07:25.339307:[DEBUG]:acum_config.cc(1066): acumConfig [88][16]
2016-08-02 12:07:25.339380:[DEBUG]:acum_config.h(52): name[0], id[0], pre_alarm_enable[0], alarm_enable[0], watchdog_enable[0], buzz_enable[0], acu_query_interval[0]
2016-08-02 12:07:25.339393:[DEBUG]:acum_config.h(56): ip[]
2016-08-02 12:07:25.339401:[DEBUG]:acum_config.h(57): netmask[]
2016-08-02 12:07:25.339409:[DEBUG]:acum_config.h(58): gateway[]
2016-08-02 12:07:25.339418:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.339426:[DEBUG]:acum_config.h(60): tftp[]
2016-08-02 12:07:25.339433:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.339441:[DEBUG]:acum_config.h(63): sql[0]
2016-08-02 12:07:25.339449:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.340555:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.340576:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.340584:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.340590:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.340598:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.340606:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.340613:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.340619:[DEBUG]:acum_config.h(63): sql[1d13940]
2016-08-02 12:07:25.340625:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.340876:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.340908:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.340930:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.340969:[INFO]:acum_config.cc(424): loadAcuConfig end
2016-08-02 12:07:25.340981:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.340990:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.340996:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341004:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341013:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341020:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341026:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341032:[DEBUG]:acum_config.h(63): sql[1d13940]
2016-08-02 12:07:25.341038:[DEBUG]:acum_config.h(66): acu size [3]
2016-08-02 12:07:25.341046:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.341053:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.341061:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.341071:[DEBUG]:main.cc(53): tttttttttttttttttttttttttttttttttt
2016-08-02 12:07:25.341083:[DEBUG]:main.cc(34): acum config size[240][24]
2016-08-02 12:07:25.341612:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.341629:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.341637:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341643:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341649:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341656:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341664:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341671:[DEBUG]:acum_config.h(63): sql[1d28a90]
2016-08-02 12:07:25.341677:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.341853:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.341881:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.341902:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.341935:[INFO]:acum_config.cc(424): loadAcuConfig end
2016-08-02 12:07:25.341946:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.341954:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.341961:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341969:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341977:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341984:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341991:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341999:[DEBUG]:acum_config.h(63): sql[1d28a90]
2016-08-02 12:07:25.342006:[DEBUG]:acum_config.h(66): acu size [3]
2016-08-02 12:07:25.342013:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.342021:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.342029:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.342036:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.342045:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.342053:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(57): netmask[192.168.7.20]
2016-08-02 12:07:25.342061:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(58): gateway[Ò]
2016-08-02 12:07:25.342069:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(59): dns[(null)]
2016-08-02 12:07:25.342077:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(60): tftp[(null)]
2016-08-02 12:07:25.342085:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(61): syslog[(null)]
2016-08-02 12:07:25.342093:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(63): sql[0]
2016-08-02 12:07:25.342100:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(66): acu size [0]
Segmentation fault (core dumped)
