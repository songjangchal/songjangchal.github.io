* 
proc/net/udp

Holds a dump of the UDP socket table. Much of the information is not of use apart from debugging. The "sl" value is the kernel hash slot for the socket, the "local_address" is the local address and port number pair. The "rem_address" is the remote address and port number pair (if connected). "St" is the internal status of the socket. The "tx_queue" and "rx_queue" are the outgoing and incoming data queue in terms of kernel memory usage. The "tr", "tm->when", and "rexmits" fields are not used by UDP. The "uid" field holds the effective UID of the creator of the socket.

Also see http://stackoverflow.com/a/18322579/449347

/proc/net/snmp

This file holds the ASCII data needed for the IP, ICMP, TCP, and UDP management information bases for an SNMP agent.

From http://linux.die.net/man/5/proc
* centos 7 安装过程
** root 密码丢失
   grub 启动项目 的ro 改成
#+BEGIN_EXAMPLE
   rw init=/sysroot/bin/
#+END_EXAMPLE

进入shell后
#+BEGIN_EXAMPLE


   chroot /sysroot
   passwd root
   touch /.autorelabel
   exit
   reboot
#+END_EXAMPLE

** 网络设置脚本
   修改/etc/sysconfig/network-scripts/ 相应网口的文件

** firewall && selinux
/etc/selinux/config 
   
SELINUX=disabled

** 安装源从光盘

打开/etc/yum.repos.d/CentOS-Base.repo
在所有的源上都加入
#+BEGIN_EXAMPLE
enabled=0
#+END_EXAMPLE

添加新的源
#+BEGIN_EXAMPLE

[media]

name=CentOS-$releasever - media
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=#####
baseurl=file:///mnt/
gpgcheck=1
enabled=1
gpgkey=file:///media/cdrom/RPM-GPG-KEY-CentOS-7     
#gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7

#+END_EXAMPLE

在/etc/fstab上把centos 光盘加入

#+BEGIN_EXAMPLE

/usr/local/src/CentOS-7.0-1406-x86_64-DVD.iso /media/cdrom/ iso9660 defaults,ro,loop 0 0

#+END_EXAMPLE
** 安装的软件
   - xulrunner.i686  (32位程序支持 |CentOS7 64位 安装32位运行库)
   - gcc automake gdb wirshark-gnome
   - 

* ghc on centos 7
** 
https://ghc.haskell.org/trac/ghc/wiki/Building

 glibc-devel ncurses-devel gmp-devel autoconf automake libtool gcc make perl python ghc happy alex git

** halcyon
eval "$( curl -sL https://github.com/mietek/halcyon/raw/master/setup.sh )"

which halcyon

eval "$( /app/halcyon/halcyon paths )"

halcyon install

halcyon install --ghc-version=7.10.1 --cabal-version=1.22.6.0

** cabal config
 1.3 The cabal-install configuration file

You can edit the cabal configuration file to set defaults, for *nix based systems this is:

 ~/.cabal/config

The config file on a Windows system is

 %appdata%\cabal\config


1.3.1 Things to put in the config file

To turn on --global by default:

 user-install: False

The root-cmd configuration parameter can be used to automatically run cabal-install with root privileges on *nix based systems, when needed:

 root-cmd: sudo



=============================================================



The cabal configuration is stored in $HOME/.cabal/config and contains various options including credential information for Hackage upload. One addition to configuration is to completely disallow the installation of packages outside of sandboxes to prevent accidental collisions.

-- Don't allow global install of packages.
require-sandbox: True

A library can also be compiled with runtime profiling information enabled. More on this is discussed in the section on Concurrency and profiling.

library-profiling: True

Another common flag to enable is the documentation which forces the local build of Haddock documentation, which can be useful for offline reference. On a Linux filesystem these are built to the /usr/share/doc/ghc/html/libraries/ directory.

documentation: True

If GHC is currently installed the documentation for the Prelude and Base libraries should be available at this local link:

* quickbuild
Re: Lost administrator password
Postby tardis4500 » Wed Oct 09, 2013 7:35 pm

Found a previous post with this answer and it worked:

If you've forgot the admin password, please edit the file "<QuickBuild server install dir>/conf/wrapper.conf" to uncomment the line "#wrapper.java.additional.4=-Dreset", and the admin password will be reset as 12345.

I didn't find it when I searched the forums because the thread title has "Passwor" and not "Password" in it. You might want to update the thread title if you can.



* gitit
** install 
cabal --prefix
cabal --global 
  
cabal install pandoc 
cabal install pandoc -fhighlighting --reinstall


cabal --global install --reinstall -fhighlighting pandoc
cabal --global install --reinstall gitit

cabal install gitit

这里发生冲突,　手动下载编译

cabal get gitit
cabal configure
cabal build
cabal install --global
** cabal config
#+BEGIN_EXAMPLE

install-dirs global
  prefix: /app/cabal
  bindir: $prefix/bin
  libdir: $prefix/lib
  libsubdir: $abi/$pkgkey
  libexecdir: $prefix/libexec
  datadir: $prefix/share
  datasubdir: $abi/$pkgid
  docdir: $datadir/doc/$abi/$pkgid
  htmldir: $docdir/html
  haddockdir: $htmldir
  sysconfdir: $prefix/etc


#+END_EXAMPLE

** systemd
We think that there should be two service files, one named gitit.service:

useradd -m -U gitit

#+BEGIN_EXAMPLE
[Unit]
Description=Gitit wiki
After=network.target

[Service]
ExecStart=/app/cabal/bin/gitit -f /home/gitit/config
WorkingDirectory=/home/gitit/default
User=gitit
Group=gitit
#Restart=on-failure

[Install]
WantedBy=multi-user.target

#+END_EXAMPLE

and the other named gitit@.service:

#+BEGIN_EXAMPLE

[Unit]
Description=Gitit wiki
After=network.target

[Service]
ExecStart=/usr/bin/gitit -f /etc/gitit/%I.conf
WorkingDirectory=/srv/gitit/%I
User=gitit
Group=gitit
#Restart=on-failure

[Install]
WantedBy=multi-user.target

#+END_EXAMPLE

with gitit user and group being created upon installation and %I being the unescaped instance name. User can enable mutliple instances by running e.g. systemctl start gitit@public.service. Depending on the configuration it could run on port 80 (the executable /usr/bin/gitit should be granted the capability CAP_NET_BIND_SERVICE), or proxied behind a web server. The use of the directory /srv/gitit as home of gitit data is still being debated.



注意:
　要用运行账户gitit手动在工作目录下,运行一次,　/app/cabal/bin/gitit -f /home/gitit/config.
  否则　用systemctl 第一次会失败.


** 图片
[link label](/Computer.svg)

** 图片大小
   

你可以直接使用html

比如

<img src="drawing.jpg" alt="Drawing" width="200px" />

或者

![drawing](drawing.jpg)

然后编写CSS文件:

img[alt=drawing] { width: 200px; }

链接

回答 10月 21 '13
delight 图像
delight
365

更新于 8月 1 '14
添加评论
1

@delight 的方法比较通用，官方的markdown确实不支持图片大小的调整。然后，有一些markdown的实现，有这个功能，你可以试一下：

在文件URL之后加上“ =WIDTHxHEIGHT”，看清楚了，“=”号之前有个空格。

示例：

![](./pic/pic1_50.png =100x20)

也可以不写高度，如下：

![](./pic/pic1s.png =250x)

注意： 现在，并不是所有markdown支持这个语法，仅供参考。
** plantuml
http://edwtjo.me/posts/2016-11-04-plant-based-uml-wiki/




* taiga on centos 7
https://github.com/taigaio/taiga-doc/blob/master/setup-production.adoc
* mingw on linux
CRT - c-runtime
The first goal of this library is to provide an alternative C-runtime for x86/x64 Windows operating systems. It shall be compatible to the msvcrt variants. The second goal is that this library provides a build-variant for kernel-mode, too. Additionally it shall provide some optional features well known from the POSIX world. This library is at the moment under construction. We still search for an final name for it. Current suggestions are "ironCrate", and "wormcrt". If you have better suggestions, feel free to contact our developer team.
The version of this library is at the moment 0.0, as this library is in pre-alpha phase. We are at the moment in planning and drafting phase. Before we will change version to 0.1 at least some basics have to be present (build-environment, startup-code, partial function set for threading). 
** compile on centos7
*** need to download the following packages
    - binutils
    - GCC
    - Mingw-w64

*** archlinux 
community/mingw-w64-binutils 2.25.1-1 (mingw-w64-toolchain mingw-w64) [installed]
    Cross binutils for the MinGW-w64 cross-compiler
community/mingw-w64-crt 4.0.4-1 (mingw-w64-toolchain mingw-w64) [installed]
    MinGW-w64 CRT for Windows
community/mingw-w64-gcc 5.3.0-1 (mingw-w64-toolchain mingw-w64) [installed: 5.2.0-2]
    Cross GCC for the MinGW-w64 cross-compiler
community/mingw-w64-headers 4.0.4-1 (mingw-w64-toolchain mingw-w64) [installed]
    MinGW-w64 headers for Windows



** linux 下构建 MingGW-w64 交叉编译工具



有不少人在windows下做ffmpeg开发时，喜欢使用mingw或mingw-w64，且不说这种ffmpeg构建方式比较复杂，还有一个缺点，就是gcc在windows下执行速度实在令人着急。笔者推荐大家在linux下使用mingw-w64交叉编译的方式，比较省时，而且相对来说比较简单。

本文所要讲述的就是如何在linux下构建MingGW-w64交叉编译工具。

别紧张，没那么复杂。网络上已经有很多类似的工具链构建脚本，本文使用的就是zeranoe的脚本。（最新版脚本下载地址）

交叉编译工具构建步骤：
1. 下载脚本

wget http://zeranoe.com/scripts/mingw_w64_build/mingw-w64-build-3.5.8

2. 修改脚本权限，使其可执行

chmod 755 mingw-w64-build-3.5.8

3. 执行以下命令，查看脚本可用选项

./mingw-w64-build-3.5.8 --help

4. 开始构建交叉编译工具

./mingw-w64-build-3.5.8 --build-type=win32 --default-configure --pthreads-w32-ver=2-9-1 --gcc-langs=all --clean-build --enable-gendef

好了，就这么简单。接下来要做的就是等着脚本执行完成即可（当然了，脚本执行需要依赖一些运行环境，如果你的系统尚未安装这些工具，脚本会提示你，根据提示进行安装就ok了）。

** 
As amazing as it may first seem, the MinGW-w64 project allows users to compile native Windows binaries on Linux.
http://www.blogcompiler.com/2010/07/11/compile-for-windows-on-linux/


Compile for Windows on Linux
Introduction

In an earlier post, we saw how to obtain gcc on windows, using the MinGW-w64 suite. However, users familiar to gcc are often using one of the operating systems of the Unix family, such as Linux. As amazing as it may first seem, the MinGW-w64 project allows users to compile native Windows binaries on Linux. This concept of targeting a different platform than the compiler is running on is however not new, and is known as cross-compilation.

Cross-compiling Windows binaries on Linux may have many benefits to it.

    Increased compilation speed. Linux is generally faster than Windows with the mingw toolchain.
    Reduced operating system complexity. On cross-platform projects that are also built on Linux, we can get one less operating system to maintain.
    Access to Unix build tools. Build tools such as make, autoconf, automake and Unix utilities as grep, sed, and cat, to mention a few, become available for use in Windows builds as well. Even though projects such as MSYS port a few of these utilities to Windows, the performance is generally lower, and the versions are older and less supported than the native Unix counterparts. Also, if you already have a build environment set up under Linux, you don’t have to set it up again on Windows, but just use the existing one.
    Lower license costs. As we know, Windows costs in terms of license fees. Building on Linux, developers do not need to have  a Windows installation on their machines, but maybe just a central Windows installation for testing purposes.

How It Works

On a Linux build environment, a gcc that compiles native binaries is usually installed in “/usr/bin”. Native headers and libraries are in turn found in “/usr/include” and “/usr/lib”, respectively. We can see that all these directories are rooted in “/usr”.

Any number of cross-compiler environments can be installed on the same system, as long as they are rooted in different directories. In our example, we will use “/opt/mingw32” and “/opt/mingw64” as root directories for the new build environments. Now, we would perhaps expect to find “/opt/mingw32/bin/gcc” and “/opt/mingw64/bin/gcc”, but we instead see “/opt/mingw32/bin/i686-w64-mingw32-gcc” and “/opt/mingw64/bin/x86_64-w64-mingw32-gcc”. The reason for this is that we  (and configure scripts) should be able to pick the “right” gcc, even if  we have multiple compilers in the PATH environment variable. If they were all named gcc, cross-compiling would easily become messy.
Cross-World Hello

    Go to the MinGW-w64 download page. We need two toolchains – one for targeting win32 and another for targeting win64.Open “Toolchains targetting Win32” , followed by “Automated Builds”, “mingw-builds” and a recent version (e.g. mingw-w64-bin_x86_64-linux_20131228.tar.bz2).

    Now do the same for “Toolchains targetting Win64” (e.g. mingw-w64-bin_x86_64-linux_20131228.tar.bz2).

    There are some notes on the package naming convention below to help you pick the right one. Also note that the direct links above might be to older versions when you read this — so please check the directory structure for updates.
    Unpack the first archive to /opt/mingw32 and the second to /opt/mingw64.
    In a text editor (e.g. gedit or nano), paste in the little greeting-code and save it to /tmp/hello.c
    #include <stdio.h>

    int main()

    {

    printf("Hello World!\n");

    return 0;

    }
    Compile it for both 32- and 64-bit Windows with the following commands.

        /opt/mingw32/bin/i686-w64-mingw32-gcc /tmp/hello.c -o /tmp/hello-w32.exe
        /opt/mingw64/bin/x86_64-w64-mingw32-gcc /tmp/hello.c -o /tmp/hello-w64.exe

    Run “hello-w32.exe” on 32-bit Windows, and “hello-w64.exe” on 64-bit Windows.

 

In order to build useful applications, it is convenient to use existing libraries such as the OpenSSL library on Windows.

Package Naming Conventions

As we saw on the MinGW-w64 download page, there are a lot of available packages with only subtle and perhaps confusing name differences. The automatically built packages have the following generic naming pattern.

    mingw-TARGET-bin_HOST_DATE.PKG

    TARGET states which platform we want the compiled binaries to run, and can be either “w32” (32-bit Windows) or “w64” (64-bit Windows).
    HOST gives the host system, that is, the system on which the compiler binaries themselves are run. Thus, we are cross-compiling if HOST is different from TARGET. If we have a Intel 32-bit Linux distribution, we can pick a HOST value of “i686-linux”, from a 64-bit Linux host we would choose “x86_64-linux”, and from 32-bit Windows we can choose “i686-mingw”.
    DATE is the date, in the form YYYYMMDD, when the automatic build was created.
    PKG is the compressed archive format, such as “zip”, “tar.bz2” and such. Generally, zip archives contain binaries that run on Windows, all other archives contains binaries that run on Linux.

Running the Binaries

Using Wine, we can even test the binaries directly from Linux. However, this only works on 32-bit Windows binaries and is not perfect due to bugs and missing features in Wine itself. After downloading and installing Wine for our distribution, we can test our program above by running “wine hello-w32.exe”.
Note that 64-bit Windows can run 32-bit binaries due to an emulation layer called Windows 32-bit On Windows 64-bit, but native binaries are more efficient.




** 
https://wiki.wxwidgets.org/Cross-Compiling_Under_Linux

Flags

You might need these flags when compiling:

-Wl,--subsystem,windows -mwindows \
-DWINVER=0x0400 -D__WIN95__ -D__GNUWIN32__ \
-DSTRICT -DHAVE_W32API_H -D__WXMSW__ -D__WINDOWS__

And these while linking:

-lregex -lpng -ljpeg -lzlib -ltiff -lstdc++ -lgcc -lodbc32 -lwsock32 -lwinspool -lwinmm -lshell32 \
-lcomctl32 -lctl3d32 -lodbc32 -ladvapi32 -lodbc32 -lwsock32 -lopengl32 -lglu32 -lole32 -loleaut32 \
-luuid

* Build_System_Management

https://wiki.wxwidgets.org/Build_System_Management


** common 编译

*** Link error: undefined reference to `htonl@4' with MinGW 

http://mingw.5.n7.nabble.com/Link-error-undefined-reference-to-htonl-4-with-MinGW-td502.html



Markus Selve wrote:

> selvem@B5561X1D ~/test
> $ gcc -Wall -lws2_32  -o conv conv.o
> conv.o(.text+0x47):conv.c: undefined reference to `htonl@4'

Congratulations, you just made probably the two most common mingw
errors.  If it makes you feel any better the archives of this list are
bulging at the seams with people making these mistakes over and over.

First, the order you specify things on the gcc command line matters.  If
A depends on B then B must come after A.  So put -lws2_32 after conv.o.

Second, you did not call WSAStartup() which is required before using ANY
socket function.  For something like htonl() it probably doesn't matter,
but for any program that actually does anything with sockets it will not
work.  <http://www.mingw.org/MinGWiki/index.php/sockets> 
*** libwsock32.a位置
在centos上是在这里
/usr/i686-w64-mingw32/sys-root/mingw/lib/
在archlinux是在这里

/usr/i686-w64-mingw32/lib/


所以在archlinux 作个软连接, 保持和centos上一致

* python
** python on windows
*** windows　python 测试脚本环境搭建 (mysys2)
  用纯正的windows环境运行python脚本测试ＡＣＵ,　暂时没有调试成功. 现在用MSYS2代替
  - 下载地址
  　http://msys2.github.io/
  　按照这个地址上的网页步骤安装
  - 安装好后再安装python
    pacman -S python
  - 设置acu协议库的路径(放置libproto.so的目录)
    export LD_LIBRARY_PATH=协议库路径:$LD_LIBRARY_PATH
    这个语句也可以写在.bashrc里,　不用每次都敲一次
  - 执行测试
    现在有２个python文件
    1. msg.py -- 对协议python封装
    2. test.py -- 使用msg.py的例子
  - windown下编译.
    1. 安装mingw编译器
       pacman -S　mingw-w64-i686-gcc
    2. ./config_for_win.sh
    3. make
    4. 生成的libproto放在 hosts/win/lib和src/protocol下



*** 安装时
   - 尽量安装目录自己设置,　且不要太长,
     如设成 D:/python
   - 选择设置path
   - 择安装pip
   - 
*** nose
   - pip install nose
   - 
** pyinstaller

http://www.pyinstaller.org/

** python structure
*** 
http://stackoverflow.com/questions/5548387/python-structures

ypedef struct LibraryInfo
    {
        uint32_t    size;                                // Size of the structure
        char        libName[MAX_LIBRARY_NAME+1];                        // Library name
        char        provider[MAX_LIBRARY_PROVIDER_NAME+1];                  // Provider
        uint32_t    version;                                                    // Library version, i.e: 0x01030005 --> v.01.03.0005  
    } LibraryInfo;  

The equivalent Python Code is:

class LibraryInfo(Structure):  
    _fields_=[("size",c_uint),  
              ("libName",c_char * MAX_LIBRARY_NAME ),  
              ("provider",c_char * MAX_LIBRARY_PROVIDER_NAME),  
              ("version",c_uint)]  


libraryInfo = LibraryInfo()
resCode = QueryLibraryInfo(byref(libraryInfo))

*** 
http://stackoverflow.com/questions/18536182/parsing-binary-data-into-ctypes-structure-object-via-readinto
*** 

#pragma pack(1)
typedef union { 
 unsigned char Mpi;  
 unsigned char Ip[4];  
 unsigned char Mac[6]; 
 } CON_ADR_TYPE; 
 
typedef struct { 
 CON_ADR_TYPE Adr;  
 unsigned char AdrType;  
 unsigned char SlotNr;  
 unsigned char RackNr;  
 } CON_TABLE_TYPE; 
#pragma pack(1)  


from ctypes import *
class CON_ADR_TYPE(Union):
 _pack_=1
 _fields_=[("Mpi", c_byte),
  ("Ip", c_byte*4),
  ("Mac", c_byte*6)]

class CON_TABLE_TYPE(Structure): 
 _fields_=[("Adr", CON_ADR_TYPE),
  ("AdrType", c_byte),
  ("SlotNr", c_byte),
  ("RackNr", c_byte)]
 _pack_=1
*** 


 调用C编写的动态链接库
代码示例

from ctypes import * 
dll = CDLL("add.dll")#加载cdecl的dll。另外加载stdcall的dll的方式是WinDLL("dllpath") 
sum=dll.Add(1, 102) 

若参数为指针

p=1 
sum=dll.sub(2, byref(p))#通过库中的byref关键字来实现 

若参数为结构体
C代码如下：

typedef struct 
{ 
    char words[10]; 
}keywords; 
 
typedef struct 
{ 
    keywords *kws; 
    unsigned int len; 
}outStruct; 
 
extern "C"int __declspec(dllexport) test(outStruct *o); 
 
int test(outStruct *o) 
{ 
    unsigned int i = 4; 
    o->kws = (keywords *)malloc(sizeof(unsigned char) * 10 * i); 
    strcpy(o->kws[0].words, "The First Data"); 
    strcpy(o->kws[1].words, "The Second Data"); 
    o->len = i; 
    return 1; 
} 


Python代码如下：

class keywords(Structure): 
    _fields_ = [('words', c_char *10),] 
 
class outStruct(Structure): 
    _fields_ = [('kws', POINTER(keywords)),('len', c_int),] 
 
o = outStruct() 
dll.test(byref(o)) 
 
print (o.kws[0].words) 
print (o.kws[1].words) 
print (o.len) 

 

 

调用Windows API

#导入ctypes模块 
from ctypes import * 
windll.user32.MessageBoxW(0, '内容！', '标题', 0) 
 
#也可以用以下方式为API函数建立个别名后再进行调用 
MessageBox = windll.user32.MessageBoxW 
MessageBox(0, '内容！', '标题', 0) 
*** 

#+BEGIN_SRC c
typedef struct {
    char words[10];
}keywords;

typedef struct {
    keywords *kws;
    unsigned int len;
}outStruct;

extern "C"int __declspec(dllexport) test(outStruct *o);

int test(outStruct *o)
{
    unsigned int i = 4;

    o->kws = (keywords *)malloc(sizeof(unsigned char) * 10 * i);

    strcpy(o->kws[0].words, "The First Data");

    strcpy(o->kws[1].words, "The Second Data");

    o->len = i;

    return 1;
}

#+END_SRC

#+BEGIN_SRC python
class keywords(Structure):

        _fields_ = [('words', c_char *10),]

 

class outStruct(Structure):

        _fields_ = [('kws', POINTER(keywords)),

                    ('len', c_int),]

o = outStruct()

dll.test(byref(o))

 

print o.kws[0].words;

print o.kws[1].words;

print o.len

#+END_SRC

** python os.path
sys.path is only searched for Python modules. For dynamic linked libraries, the paths searched must be in LD_LIBRARY_PATH. Check if your LD_LIBRARY_PATH includes /usr/local/lib, and if it doesn't, add it and try again.

Some more information (source):

    In Linux, the environment variable LD_LIBRARY_PATH is a colon-separated set of directories where libraries should be searched for first, before the standard set of directories; this is useful when debugging a new library or using a nonstandard library for special purposes. The environment variable LD_PRELOAD lists shared libraries with functions that override the standard set, just as /etc/ld.so.preload does. These are implemented by the loader /lib/ld-linux.so. I should note that, while LD_LIBRARY_PATH works on many Unix-like systems, it doesn't work on all; for example, this functionality is available on HP-UX but as the environment variable SHLIB_PATH, and on AIX this functionality is through the variable LIBPATH (with the same syntax, a colon-separated list).


RUN_LD_PATH

** python load dll error (os error winerror 126)

*** Search Path Used by Windows to Locate a DLL
https://msdn.microsoft.com/en-us/library/7d83bc18.aspx

With both implicit and explicit linking, Windows first searches for "known DLLs", such as Kernel32.dll and User32.dll. Windows then searches for the DLLs in the following sequence:

    The directory where the executable module for the current process is located.

    The current directory.

    The Windows system directory. The GetSystemDirectory function retrieves the path of this directory.

    The Windows directory. The GetWindowsDirectory function retrieves the path of this directory.

    The directories listed in the PATH environment variable.
    System_CAPS_noteNote

    The LIBPATH environment variable is not used.

*** 使用cmd模式，python *.py，会弹出缺少的依赖库名。
libgcc_s_dw2-1.dll

http://blog.csdn.net/aha121/article/details/17054487

最近想把一组api做成一个界面，来控制流程。

问题1：使用IDLE中，直接执行程序报错

Traceback (most recent call last):
  File "E:\study\python\client.py", line 143, in <module>
    gtpdll = CDLL("test.dll")
  File "D:\Python33\lib\ctypes\__init__.py", line 353, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] 找不到指定的模块。

后来捣鼓了好久，才知道是缺少依赖的库。

在一次无意中发现，执行的py文件的时候，使用cmd模式，python *.py，会弹出缺少的依赖库名。

 

问题2：链表结构，就是struct里面有类型为struct的类型。在python中，可以按以下方式初始化

class TEST_LIST(Structure):
        pass
TEST_LIST._fields_= [('prev',POINTER(TEST_LIST)),('next',POINTER(TEST_LIST)),('srcfilename',c_char*256),('destfilename',c_char*256)]

 

问题3：指向指针的指针

在实现的过程中，发现若函数参数为指向指针指针，若使用双层pointer表示，就会有问题，后来使用的是

list = TEST_LIST()
list_p = pointer(list)

然后传参时使用pointer(list_p)

 

问题4：数组初始化

destfile = (c_char*256)()
destfile.value = b'result.txt'

需要注意的是，若C代码中涉及strcmp，那么c_char*256，就需要改成len(destfile.value)

* Compiling problems: cannot find crt1.o

http://stackoverflow.com/questions/91576/crti-o-file-missing

gcc -B/usr/lib/x86_64-linux-gnu hello.c

So, you can just add -B/usr/lib/x86_64-linux-gnu to the CFLAGS variable in your Makefile.


** LIBRARY_PATH

Run this to see where these files are located

$ find /usr/ -name crti*
/usr/lib/x86_64-linux-gnu/crti.o

then add this path to LIBRARY_PATH variable

$ export LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LIBRARY_PATH



** gcc -B
gcc -B/usr/lib/x86_64-linux-gnu hello.c

So, you can just add -B/usr/lib/x86_64-linux-gnu to the CFLAGS variable in your Makefile.

** sysroot


Even I got the same compilation error when I was cross compiling i686-cm-linux-gcc.

The below compilation option solved my problem

$ i686-cm-linux-gcc a.c --sysroot=/opt/toolchain/i686-cm-linux-gcc

Note: The sysroot should point to compiler directory where usr/include available

In my case the toolchain is installed at /opt/toolchain/i686-cm-linux-gcc directory and usr/include is also available in the same directory

=========================================

his solved for me (cross compiling pjsip for ARM):

export LDFLAGS='--sysroot=/home/me/<path-to-my-sysroot-parent>/sysroot'


* gcc 
** gcc __STDC_HOSTED__

__STDC_HOSTED__ 如果编译器的目标系统环境中包含完整的标准C库，那么这个宏就定义为1，否则宏的值为0

This macro is defined, with value 1, if the compiler's target is a hosted environment. A hosted environment has the complete facilities of the standard C library available. 

*** C standard library
https://en.wikipedia.org/wiki/C_standard_library


According to the C standard the macro __STDC_HOSTED__ shall be defined to 1 if the implementation is hosted. A hosted implementation has all the headers specified by the C standard. An implementation can also be freestanding which means that these headers will not be present. If an implementation is freestanding, it shall define __STDC_HOSTED__ to 0.

http://gcc.gnu.org/onlinedocs/gcc/Standards.html
GCC aims towards being usable as a conforming freestanding implementation, or as the compiler for a conforming hosted implementation. By default, it will act as the compiler for a hosted implementation, defining __STDC_HOSTED__ as 1 and presuming that when the names of ISO C functions are used, they have the semantics defined in the standard. To make it act as a conforming freestanding implementation for a freestanding environment, use the option -ffreestanding; it will then define __STDC_HOSTED__ to 0 and not make assumptions about the meanings of function names from the standard library, with exceptions noted below. To build an OS kernel, you may well still need to make your own arrangements for linking and startup. See Options Controlling C Dialect. 
*** include_next
首先，我将会说明一下这条指令的功能，然后说明一下为什么要引人这条指令，希望能说个明白。

#include_next和#include指令一样，也是包含一个头文件，它们的不同地方是包含的路径不一样。

#include_next的意思就是“包含指定的这个文件所在的路径的后面路径的那个文件”，听起来是不是很坳口，我自己也觉得是这样，但下面举个例子说明就清楚了。

例如有个搜索路径链，在#include中，它们的搜索顺序依次是A，B，C，D和E。在B目录中有个头文件叫a.h，在D目录中也有个头文件叫a.h，如果在我们的源代码中这样写#include <a.h>，那么我们就会包含的是B目录中的a.h头文件，如果我们这样写#include_next <a.h>那么我们就会包含的是D目录中的a.h头文件。#include_next <a.h>的意思按我们上面的引号包含中的解释来说就是“在B目录中的a.h头文件后面的目录路径（即C，D和E）中搜索a.h头文件并包含进来）。#include_next <a.h>的操作会是这样的，它将在A，B，C，D和E目录中依次搜索a.h头文件，那么首先它会在B目录中搜索到a.h头文件，那它就会以B目录作为分割点，搜索B目录后面的目录（C，D和E），然后在这后面的目录中搜索a.h头文件，并把在这之后搜索到的a.h头文件包含进来。这样说的话大家应该清楚了吧。

 

还有一点是#include_next是不区分<>和""的包含形式的。

 

现在来说说为什么要引人这条指令！

假如，你要创建一个新的头文件，而这个新的头文件和现在已有的头文件有相同的名字，而且你想用你的这个新的头文件，那么你要做的就是把这个新的头文件放在#include指令的搜索路径的前面，即是在旧的头文件的前面新的头文件首先被搜索到，这样你就可以使用你这个新的头文件。但是你在另一个源代码文件中想使用旧的头文件了，那怎么办！有个办法就是使用绝对路径来搜索，那么就不存在这样的问题了。问题出在，如果我们把头文件的位置移动了，移到了其它的目录里了，那我们就得在相应的源码文件中修改这个包含的绝对路径，如果一个源码文件还好，但如果是大型工程的话，修改的地方多了就容易出问题。

又进一步说，如果你这个新的头文件引用了旧的头文件，而这个新的头文件如果没有使用只编译一次的预处理语句包含（即#ifndef，#endif等），那么就会陷入一个无限的递归包含中，这个新的头文件就会无限的包含自己，就会出现一个致命的错误。如果我们使用#include_next就会避免这样的问题。

在标准的C中，这没有一个办法来解决上面的问题的，因此GNU就引人了这个指令#include_next。

 

下面再举一个#include_next的例子。

假设你用-I选项指定了一个编译包含的路径 '-I /usr/local/include'，这个路径下面有个signal.h的头文件，在系统的'/usr/include'下也有个signal.h头文件，我们知道-I选项的路径首先搜索。如果我们这样 #include <signal.h> 包含，就会包含进/usr/local/include下的signal.h头文件；如果是 #include_next <signal.h>，就会包含 '/usr/include'下的signal.h头文件。

GNU建议一般没有其它可取代的办法的情况下才使用#include_next的。

 

又一个例子，如在系统头文件stdio.h中，里面有个函数（应该说是一个宏）getc，它从标准输入中读取一个字符。你想重新定义一个getc，并放到自己新建的stdio.h文件中，那么你可以这样使用你自定义的getc。

#include_next "stdio.h"
#undef getc
#define getc(fp) ((int)'x')

 

更多的说明请参考GNU的官方文档和GCC文档。
http://www.delorie.com/gnu/docs/gcc/cpp_11.html

** gcc 信息查看
./arm-linux-gnueabihf-gcc --help

  -dumpspecs               Display all of the built in spec strings
  -dumpversion             Display the version of the compiler
  -dumpmachine             Display the compiler's target processor
  -print-search-dirs       Display the directories in the compiler's search path
  -print-libgcc-file-name  Display the name of the compiler's companion library
  -print-file-name=<lib>   Display the full path to library <lib>
  -print-prog-name=<prog>  Display the full path to compiler component <prog>
  -print-multiarch         Display the target's normalized GNU triplet, used as
                           a component in the library path
  -print-multi-directory   Display the root directory for versions of libgcc
  -print-multi-lib         Display the mapping between command line options and
                           multiple library search directories
  -print-multi-os-directory Display the relative path to OS libraries
  -print-sysroot           Display the target libraries directory
  -print-sysroot-headers-suffix Display the sysroot suffix used to find headers


./arm-linux-gnueabihf-gcc -v

Using built-in specs.
COLLECT_GCC=./arm-linux-gnueabihf-gcc
COLLECT_LTO_WRAPPER=/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/4.7.3/lto-wrapper
Target: arm-linux-gnueabihf
Configured with: /cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/src/gcc-linaro-4.7-2013.03/configure --build=i686-build_pc-linux-gnu --host=i686-build_pc-linux-gnu --target=arm-linux-gnueabihf --prefix=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install --with-sysroot=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install/arm-linux-gnueabihf/libc --enable-languages=c,c++,fortran --enable-multilib --with-arch=armv7-a --with-tune=cortex-a9 --with-fpu=vfpv3-d16 --with-float=hard --with-pkgversion='crosstool-NG linaro-1.13.1-4.7-2013.03-20130313 - Linaro GCC 2013.03' --with-bugurl=https://bugs.launchpad.net/gcc-linaro --enable-__cxa_atexit --enable-libmudflap --enable-libgomp --enable-libssp --with-gmp=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-mpfr=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-mpc=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-ppl=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-cloog=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-libelf=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static --with-host-libstdcxx='-L/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/.build/arm-linux-gnueabihf/build/static/lib -lpwl' --enable-threads=posix --disable-libstdcxx-pch --enable-linker-build-id --enable-gold --with-local-prefix=/cbuild/slaves/oorts/crosstool-ng/builds/arm-linux-gnueabihf-linux/install/arm-linux-gnueabihf/libc --enable-c99 --enable-long-long --with-mode=thumb
Thread model: posix
gcc version 4.7.3 20130226 (prerelease) (crosstool-NG linaro-1.13.1-4.7-2013.03-20130313 - Linaro GCC 2013.03) 

**  gcc查找头文件的顺序

刚刚翻了翻gcc查找include的头文件的优先级，首先会在当前目录下找，假如没有找到，会在以下几个地方找：
1.编译的时候指定
2.gcc的specs里
3.使用-I参数指定的路径
4.gcc环境变量设置（C_INCLUDE_PATH）

而在这四个当中，-I参数指定的路径优先级最高。在gcc的手册里是这么说的：

       -I dir
           Add the directory dir to the list of directories to be searched for header files.  Directories named
           by -I are searched before the standard system include directories.  If the directory dir is a stan-
           dard system include directory, the option is ignored to ensure that the default search order for sys-
           tem directories and the special treatment of system headers are not defeated .

使用-I参数指定的路径会在标准的系统include路径之前被搜索。

简单写一行shell，就能看到include的搜索的顺序了。
echo 'main(){}' | gcc -E -v  -

#include "..." 搜索从这里开始：
#include <...> 搜索从这里开始：
/usr/local/include
/usr/lib/gcc/i386-redhat-linux/4.1.2/include
/usr/include

加上-I参数 
echo 'main(){}' | gcc -E -v  -I /home/chengcheng/mmsapp/include -,结果则是
/home/chengcheng/mmsapp/include
/usr/local/include
/usr/lib/gcc/i386-redhat-linux/4.1.2/include
/usr/include



=================================


  -print-search-dirs       Display the directories in the compiler's search path
  -print-libgcc-file-name  Display the name of the compiler's companion library
  -print-file-name=<lib>   Display the full path to library <lib>
  -print-prog-name=<prog>  Display the full path to compiler component <prog>
  -print-multiarch         Display the target's normalized GNU triplet, used as
                           a component in the library path
  -print-multi-directory   Display the root directory for versions of libgcc
  -print-multi-lib         Display the mapping between command line options and
                           multiple library search directories
  -print-multi-os-directory Display the relative path to OS libraries
  -print-sysroot           Display the target libraries directory
  -print-sysroot-headers-suffix Display the sysroot suffix used to find headers

** ./arm-linux-gnueabihf-gcc -print-search-dirs

install: /home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/
programs: =/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../libexec/gcc/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/bin/arm-linux-gnueabihf/
libraries: =/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/lib/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf/4.7.3/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/:/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf/

** ./arm-linux-gnueabihf-gcc -print-multi-directory

* cpb编译
** centos 
 yum whatprovides libstdc++.so.6

然后会提示哪个安装包有这个库文件如下：


 yum install libstdc++-4.8.2-16.el7.i686
s
   
** 
　(02:37:00 PM) zcsong: rootfs.img最大多少,　
(02:37:20 PM) zcsong: 我现在是１２ｍ 烧不成功
(02:37:57 PM) 王磊: 我的是11点几M
(02:38:25 PM) 王磊: 总共的空间只有11.5M
(02:38:42 PM) 王磊: 你把你的打包的脚本发过来看下
(02:39:14 PM) zcsong: 好的,　我再删除一些文件吧,
(02:39:33 PM) zcsong: 我放到里边的东西比较多
 (04:00:40 PM) zcsong: app升级的ubi-header.img是怎么生成的??
(04:00:55 PM) 王磊: 现在不用了 
(04:01:01 PM) zcsong: ??
(04:01:14 PM) 王磊: 那个是之前的nand使用的
(04:01:20 PM) zcsong: 那升级怎么是什么文件
(04:01:45 PM) zcsong: 直接tar.bz2
(04:01:46 PM) zcsong: ??
(04:02:18 PM) 王磊: 直接升级到nor里面
(04:02:24 PM) 王磊: 使用rootfs.img
(04:02:41 PM) 王磊: 如果要是用nand，可以使用ubi打包 
(04:03:07 PM) 王磊: ubi的打包工具我也有 
(04:03:55 PM) 王磊: 你现在要用ubi吗
(04:05:59 PM) zcsong: 先给我吧,　
(04:06:39 PM) zcsong: 以后要用这种模式吗?
(04:07:04 PM) 王磊: 不用 
(04:07:16 PM) 王磊: 后面都用nor的了
(04:07:26 PM) 王磊: 后面用两块nor了
(04:12:43 PM) zcsong: 哦,　那就不用了
(04:13:13 PM) 王磊: 恩 

** cpb需要创建的软连接
编译不过,　且表现为头文件或者类型没有定义,　定位为搜索路径不对, 原来的有些软连接都是在另外一台机器上的绝对路径.要根据自己的环境作相应的配置


find . -type l | grep -v bin | grep -v etc |grep -v '\.so.*'

find . -type l | grep -v bin | grep -v etc |grep -v '\.so.*' | xargs ls -l | grep disk230


./linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm -> /disk230/cpb_devkit/kernel/arch/arm/include/asm
./linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux -> /disk230/cpb_devkit/kernel/include/linux/
./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include
./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/lib
./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/include
./linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf -> /disk230/cpb_devkit/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/lib
./linux-devkit/sysroots/i686-arago-linux/usr/include/linux -> /disk230/cpb_devkit/kernel/include/linux/
./usr -> /disk230/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/

#+BEGIN_SRC sh

BASE_DIR=/opt/cpb_devkit

rm -f ${BASE_DIR}/usr
ln -s ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/ ${BASE_DIR}/usr

rm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm
ln -s  ${BASE_DIR}/kernel/arch/arm/include/asm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/asm 

rm ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux
ln -s ${BASE_DIR}/kernel/include/linux/ ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include/linux

rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include
ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/include ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/include

rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf
ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/lib ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/lib/arm-linux-gnueabihf 

rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include
ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/include ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/include

rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf
ln -s ${BASE_DIR}/linux-devkit/sysroots/armv7ahf-vfp-neon-3.2-oe-linux-gnueabi/usr/lib ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/arm-linux-gnueabihf/libc/usr/lib/arm-linux-gnueabihf

rm ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/include/linux
ln -s ${BASE_DIR}/kernel/include/linux/ ${BASE_DIR}/linux-devkit/sysroots/i686-arago-linux/usr/include/linux
#+END_SRC


* ignore directory



#+BEGIN_SRC 

"/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/include"
"/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../lib/gcc/../../lib/gcc/arm-linux-gnueabihf/4.7.3/../../../../arm-linux-gnueabihf/include"
"/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/include/."
"/home/songzc/software/cpb_devkit/linux-devkit/sysroots/i686-arago-linux/usr/bin/../arm-linux-gnueabihf/libc/usr/include/arm-linux-gnueabihf"
#+END_SRC





* 221 安装 
** freeswitch 

   zlib zlib-devel libjpeg-devel sqlite-devel curl-devel pcre-devel speex-devel ldns-devel libedit-devel openssl-devel

** mysql
   mariadb.x86_64 mariadb-server mysql-connector-odbc

   mysqladmin -uroot password root

** fusionpbx
*** php
    php php-common php-pdo php-soap php-xml php-xmlrpc php-mysql php-fpm
*** nginx
    nginx-release-centos-7-0.el7.ngx.noarch.rpm
    yum install -y nginx

mkdir /etc/nginx/sites-available
mkdir /etc/nginx/sites-enabled

cd /etc/nginx
rm nginx.conf
wget http://www.fusionpbx.com/downloads/centos/nginx/nginx.conf
cd /etc/nginx/sites-available
wget http://www.fusionpbx.com/downloads/centos/nginx/fusionpbx.conf
ln /etc/nginx/sites-available/fusionpbx.conf /etc/nginx/sites-enabled/fusionpbx.conf


*** openssl

*** Permissions
    - selinux
    /etc/selinux/config  ==> SELINUX=disabled
    - firewall
      firewalld.service
*** odbc 
    create databases freeswitch;
    /etc/odbc.ini
    
#+BEGIN_SRC 
[freeswitch]
Driver = MySQL
SERVER = localhost
PORT = 3306
DATABASE = freeswitch
OPTION = 67108864
USER = root
PASSWORD = root
#+END_SRC



**** freeswitch odbc 

     find . | xargs grep dsn
     
     添加一个变量 dsn
     value : odbc://freeswitch

*** /etc/nginx/sites-available/fusionpbx.conf
   listen 80 ==> listen 8080

 如果 是80则出现的不是fusionpbx页面,而是nginx的默认页面, 不知道原因 

添加变量local_ip_v4为bind ip

<param name="register-transport" value="tcp"/>

变量domain一定要使能
** quickbuild
   - conf/hibernate.properties
     打开postgresql设置
*** postgresql

    - postgresql-server postgresql
#+BEGIN_SRC 
postgresql-setup initdb
ps aux | grep postgre
#+END_SRC
    看到生成的默认数据库在/var/lib/pgsql/data

    - 更改listen端口
    /var/lib/pgsql/data/postgresql.conf
#+BEGIN_SRC 
    listen_addresses = 'localhost' 
#+END_SRC
    默认是localhost, 根据需要更改
    
    - 更改认证方式
      /var/lib/pgsql/data/pg_hba.conf
      ident ===> trust

    - create user
      su - postgres
      createuser quickbuild -W
      createdb quickbuild

#+BEGIN_SRC 
#psql
键入 \l, 查看所有 database
postgres-# \l

#+END_SRC      
* taiga back
http://taigaio.github.io/taiga-doc/dist/#_installation_guide

** dir
  taiga/back
** packets
   - binutils autoconf flex bison libjpeg-turbo-devel bzip2-devel libzip-devel
   - freetype-devel zlib-devel ncurses-devel gdbm-devel 
   - automake libtool libffi-devel curl git tmux
   - postgresql postgresql-contrib postgresql-doc  postgresql-devel postgresql-server
   - libxml2-devel libxslt-devel
   - pytho3 python-pip (手动安装)


**** Install pip

To install pip, securely download get-pip.py. [2]

Then run the following (which may require administrator access):

python get-pip.py


** cmd

#+BEGIN_SRC 
sudo -u postgres createuser zcsong
sudo -u postgres createdb taiga -O zcsong



#+END_SRC

     #+BEGIN_SRC 
wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - |python3
wget https://raw.github.com/pypa/pip/master/contrib/get-pip.py -O - |python3
     #+END_SRC

#+BEGIN_SRC 

     pip3 install virtualenvwrapper

export VIRTUALENVWRAPPER_PYTHON=python3.4
source /usr/bin/virtualenvwrapper.sh
mkvirtualenv -p /usr/bin/python3.4 taiga

     pip3 install virtualenvwrapper (消除错误 Error while finding spec for 'virtualenvwrapper.hook_loader' (<class 'ImportError'>: No module name)


     pip install -r requirements.txt

     python manage.py migrate --noinput
     python manage.py loaddata initial_user
     python manage.py loaddata initial_project_templates
     python manage.py loaddata initial_role
    python manage.py compilemessages
   python manage.py collectstatic --noinput

python manage.py migrate --noinput
python manage.py loaddata initial_user
python manage.py loaddata initial_project_templates
python manage.py compilemessages
python manage.py collectstatic --noinput
python manage.py sample_data



#+END_SRC
 *This creates a new user admin with password 123123.*


#+BEGIN_EXAMPLE
/usr/bin/virtualenvwrapper.sh

#  1. Create a directory to hold the virtual environments.
#     (mkdir $HOME/.virtualenvs).
#  2. Add a line like "export WORKON_HOME=$HOME/.virtualenvs"
#     to your .bashrc.
#  3. Add a line like "source /path/to/this/file/virtualenvwrapper.sh"
#     to your .bashrc.
#  4. Run: source ~/.bashrc
#  5. Run: workon
#  6. A list of environments, empty, is printed.
#  7. Run: mkvirtualenv temp
#  8. Run: workon
#  9. This time, the "temp" environment is included.
# 10. Run: workon temp
# 11. The virtual environment is activated.

#+END_EXAMPLE



settings/local.py

#+BEGIN_SRC 
from .common import *

MEDIA_URL = "http://example.com/media/"
STATIC_URL = "http://example.com/static/"
ADMIN_MEDIA_PREFIX = "http://example.com/static/admin/"
SITES["front"]["scheme"] = "http"
SITES["front"]["domain"] = "example.com"

SECRET_KEY = "theveryultratopsecretkey"

DEBUG = False
TEMPLATE_DEBUG = False
PUBLIC_REGISTER_ENABLED = True

DEFAULT_FROM_EMAIL = "no-reply@example.com"
SERVER_EMAIL = DEFAULT_FROM_EMAIL

# Uncomment and populate with proper connection parameters
# for enable email sending.
#EMAIL_BACKEND = "django.core.mail.backends.smtp.EmailBackend"
#EMAIL_USE_TLS = False
#EMAIL_HOST = "localhost"
#EMAIL_HOST_USER = ""
#EMAIL_HOST_PASSWORD = ""
#EMAIL_PORT = 25

# Uncomment and populate with proper connection parameters
# for enable github login/singin.
#GITHUB_API_CLIENT_ID = "yourgithubclientid"
#GITHUB_API_CLIENT_SECRET = "yourgithubclientsecret"

#+END_SRC
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.
workon taiga
python3 manage.py runserver

add /etc/nginx/sites-available/taiga

sudo ln -s /etc/nginx/sites-available/taiga /etc/nginx/sites-enabled/taiga


#+BEGIN_SRC 
server {
    listen 8820 default_server;
    server_name _;

    large_client_header_buffers 4 32k;
    client_max_body_size 50M;
    charset utf-8;

    access_log /home/zcsong/taiga/logs/nginx.access.log;
    error_log /home/zcsong/taiga/logs/nginx.error.log;

    # Frontend
    location / {
        root /home/zcsong/taiga/front/dist/;
        try_files $uri $uri/ /index.html;
    }

    # Backend
    location /api {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000/api;
        proxy_redirect off;
    }

    # Django admin access (/admin/)
    location /admin {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000$request_uri;
        proxy_redirect off;
    }

    # Static files
    location /static {
        alias /home/zcsong/taiga/back/static;
    }

    # Media files
    location /media {
        alias /home/zcsong/taiga/back/media;
    }
}


#+END_SRC



Copy and edit initial configuration on ~/taiga-front-dist/dist/js/conf.json
#+BEGIN_SRC 
{
    "api": "http://example.com/api/v1/",
    "eventsUrl": "ws://example.com/events",
    "debug": "true",
    "publicRegisterEnabled": true,
    "feedbackEnabled": true,
    "privacyPolicyUrl": null,
    "termsOfServiceUrl": null,
    "maxUploadFileSize": null,
    "contribPlugins": []
}

#+END_SRC
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.

如nginx运行时的用户和taiga的放置目录不同, 则要加入taiga所属用户的组, 否则会出现文件访问无权限, 参考下面:

*** stat() 13 permission denied nginx 
http://stackoverflow.com/questions/25774999/nginx-stat-failed-13-permission-denied
Nginx operates within the direcotry, so if you can't cd to that directory from the nginx user then it will fail (as does the stat command in your log). Make sure the www-user can cd all the way to the /username/test/static. You can confirm that the stat will fail or succeed by running

sudo -u www-data stat /username/test/static
In your case probably the /username directory is the issue here. Usually www-data does not have permissions to cd to other users home directories.

The best solution in that case would be to add www-data to username group:
#+BEGIN_SRC 
gpasswd -a www-data username
#+END_SRC

and make sure that username group can enter all directories along the path:

#+BEGIN_SRC 
chmod g+x /username && chmod g+x /username/test && chmod g+x /username/test/static
#+END_SRC



***  python的沙盒环境--virtualenv
http://blog.csdn.net/jianhong1990/article/details/7840139

使用 VirtualEnv 的理由：

隔离项目之间的第三方包依赖，如A项目依赖django1.2.5，B项目依赖django1.3。
为部署应用提供方便，把开发环境的虚拟环境打包到生产环境即可,不需要在服务器上再折腾一翻。
使用说明：

安装： sudo easy_install virtualenv

建立新的运行环境：virtualenv <env-name>

进入相应的独立环境：source <env-path>/bin/activate



*** 最后成功

是因为在我自己的机器上开了back, 221的为什么会连接到我的机器
将这里的example.com 改成自己服务器的ip或domain, 不然会设成localhost, 这样浏览器解释成本地, 不会到taiga服务器上取东西.

~/taiga-front-dist/dist/js/conf.json
#+BEGIN_SRC 
{
    "api": "http://example.com/api/v1/",
    "eventsUrl": "ws://example.com/events",
    "debug": "true",
    "publicRegisterEnabled": true,
    "feedbackEnabled": true,
    "privacyPolicyUrl": null,
    "termsOfServiceUrl": null,
    "maxUploadFileSize": null,
    "contribPlugins": []
}
#+END_SRC

这里api设成是和url主地址相同的形式, 如网站是http://192.168.51.221:8820, 这里设成http://192.168.51.221:8820/api/v1

api的请求通过nginx转成内部请求, 通过 http://127.0.0.1:8000/api 去调用接口

#+BEGIN_SRC 
location /api {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000/api;
        proxy_redirect off;
    }

    # Django admin access (/admin/)
    location /admin {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Scheme $scheme;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://127.0.0.1:8000$request_uri;
        proxy_redirect off;
    }
#+END_SRC


** Circus and gunicorn
   pip2 install circus

~/circus.ini
#+BEGIN_SRC 
[circus]
check_delay = 5
endpoint = tcp://127.0.0.1:5555
pubsub_endpoint = tcp://127.0.0.1:5556
statsd = true

[watcher:taiga]
working_dir = /home/zcsong/taiga/back
cmd = gunicorn
args = -w 3 -t 60 --pythonpath=. -b 127.0.0.1:8001 taiga.wsgi
uid = zcsong
numprocesses = 1
autostart = true
send_hup = true
stdout_stream.class = FileStream
stdout_stream.filename = /home/zcsong/taiga/logs/gunicorn.stdout.log
stdout_stream.max_bytes = 10485760
stdout_stream.backup_count = 4
stderr_stream.class = FileStream
stderr_stream.filename = /home/zcsong/taiga/logs/gunicorn.stderr.log
stderr_stream.max_bytes = 10485760
stderr_stream.backup_count = 4

[env:taiga]
PATH = /home/zcsong/.virtualenvs/taiga/bin:$PATH
TERM=xterm
SHELL=/bin/bash
USER=zcsong
LANG=en_US.UTF-8
HOME=/home/zcsong
PYTHONPATH=/home/zcsong/.virtualenvs/taiga/lib/python3.4/site-packages

#+END_SRC
** nohup
   nohup python manage.py runserver &
   nohup command > myout.file 2>&1 


** systemd
#+BEGIN_SRC 
[Unit]
Description=taiga back
After=network.target

[Service]
WorkingDirectory=/home/zcsong/taiga/back
User=zcsong
Group=www-data
Environment=PATH=/home/zcsong/.virtualenvs/taiga/bin:$PATH 
Environment=PYTHONPATH=/home/zcsong/.virtualenvs/taiga/lib/python3.4/site-packages

#Restart=on-failure

#ExecStartPre=/usr/sbin/nginx -t -c /etc/nginx/nginx.conf
ExecStart=/home/zcsong/.virtualenvs/taiga/bin/python3.4 manage.py runserver
#ExecReload=/bin/kill -s HUP $MAINPID
#ExecStop=/bin/kill -s QUIT $MAINPID
#PrivateTmp=true
 
[Install]
WantedBy=multi-user.target

#+END_SRC


* scrum 
** issue vs user story vs task on boards
https://github.com/taigaio/taiga-front/issues/797


We share the standard scrum point of view about what user story and task means. A user story, is a way to define a software feature from an end-user perspective. For example, a user story may look like "As a user, I want to be able to update my profile with age, present occupation and social interests, so that people visiting my profile page get an idea of my interests". A task, on the other hand, is typically something like code this, design that, create test data for such-and-such, automate that, and so on. These tend to be things done by one person. When creating a backlog or doing estimations you should be thinking at user story level, not in tasks, that's the reason why backlog work with user stories.

The usual scrum workflow divides the user stories in tasks when you create a sprint and you move them there, that's the reason why the visualization of tasks only makes sense when you have selected a sprint.

Backlog -> works with user stories
Sprint -> work with tasks from the user stories associated to that sprint
Issues -> we use to cover any other items that can have different workflows, the most common should be bugs/defects

Conceptually it makes sense not to mix these three different items under the same areas. Another thing to consider is that in taiga user stories, tasks and issues have completely independant and customizable workflows with different status, so they can't be grouped by it. What Taiga supports is the "promote to user story" feature for issues, we found that commonly there are issues that really are user stories but the team needs to keep full traceability about them.

We have an article in our blog speaking about user stories, it could be usefull -> https://blog.taiga.io/user-stories-demystified.html .
There are also a lot of articles about the differences between user stories, tasks, epics, features...(for example this one https://www.mountaingoatsoftware.com/blog/the-difference-between-a-story-and-a-task).
I would also recommend the reading of https://en.wikipedia.org/wiki/Scrum_(software_development)#Artifacts (the different artifacts used on scrum), it also includes a clear explanation about how backlog and sprint at user story and task level.

Regards! ;)


* boogie board
** python 
https://github.com/jbedo/boogiesync-tablet

pip install pyusb

** c

https://github.com/chonan/boogie
  
* xorg evdev vs. libinput
* gocd
** systemd service file
#+BEGIN_SRC 

GO_SERVER_PORT=8810
export GO_SERVER_PORT
GO_SERVER_SSL_PORT=8811
export GO_SERVER_SSL_PORT
SERVER_WORK_DIR=/home/gocd/go-server
export SERVER_WORK_DIR
DAEMON=Y

#+END_SRC


#+BEGIN_SRC 
[Unit]
Description=go cd
After=network.target

[Service]
Type=forking
PIDFile=/home/gocd/go-server/go-server.pid
Environment=JAVA_HOME=/usr/lib/jvm/jre-1.7.0-openjdk/
ExecStart=/home/gocd/go-server/server.sh
ExecStop=/home/gocd/go-server/stop-server.sh
WorkingDirectory=/home/gocd/go-server
User=go
Group=gitit
#Restart=on-failure

[Install]
WantedBy=multi-user.target

#+END_SRC

* 在网页浏览器中原生显示PDF文件

<html>
    <body>
         <embed width="100%" height="100%" name="plugin" src="xx.pdf" type="application/pdf" />
    </body>
</html>


<object data="1.pdf" type="application/pdf"
           width="800"
           height="1050">
      <a href='http://get.adobe.com/cn/reader'>Adobe Reader.pdf </a> 
</object> 



<html>
  <body>
    <centre>
         <object data="BB_00001.PDF" type="application/pdf"
           width="500"
           >
      <a href='http://get.adobe.com/cn/reader'>Adobe Reader.pdf </a> 
         </object>
         <centre>
    </body>
</html>
<object data="BB_00054.PDF" type="application/pdf" width="500"/>
<embed width="100%" height="100%" name="plugin" src="BB_00054.pdf" type="application/pdf" />
* c++ ORM
** 
 http://www.codesynthesis.com/products/odb/
** 
 http://quince-lib.com/index.html
* coredump file 
    当系统中的一些程序在遇到一些错误以及crash时，系统会自动产生core file记录crash时刻系统信息包括内存和寄存器信息，用以程序员日后debug时可以使用。这些错误包括断错误，非法指令，总线错误和用户自己生成的退出信号等等。一般的，core file会在当前文件夹中存放。
         core file有时可能在你发生错误时，并没有出现在你的当前文件夹中，发生这种情况的原因有两个，一个是当前终端被设置为不能弹出core file；另一种则是core file被制定了路径。
         对于前者，我们可以使用ulimit这条命令对core file文件的大小进行设定。一般默认情况下，core file的大小被设置为了0，这样系统就不dump出core file了。这时用如下命令进行设置：
ulimit -c unlimited
这样便把core file的大小设置为了无限大，同时也可以使用数字来替代unlimited，对core file的上限值做更精确的设定。
         除了可以设置core file的大小之外，还可以对core file的名称进行一些规定。这种设置是对/proc/sys/kernel/core_pattern和/proc/sys/kernel/core_uses_pid这两个文件进行修改。改动这两个文件的方法是：
echo <pattern> > /proc/sys/kernel/core_pattern
echo <"0"/"1"> > /proc/sys/kernel/core_uses_pid
并且注意，只有超级用户可以修改这两个文件。
         core_pattern接受的是core file名称的pattern，它包含任何字符串，并且用％作为转移符号生成一些标识符，为core file名称加入特殊含义。已定义的标识符有如下这些：
%%: 相当于%
%p: 相当于<pid>
%u: 相当于<uid>
%g: 相当于<gid>
%s: 相当于导致dump的信号的数字
%t: 相当于dump的时间
%h: 相当于hostname
%e: 相当于执行文件的名称
除了这些标识符之外，还规定：
1.末尾的单个％可以直接去除。
2.%加上除上述之外的任何字符，％和该字符都被去除。
3.所有其他字符都作为一般字符加入名称中。
4.core file的名称最大值为64字节（包括\0）。
5.core_pattern中的默认pattern为core。
6.为了保持兼容性，通过设置core_uses_pid，可以在core file名称的末尾加上％p。
7.pattern中可以包含路径信息。 
* npm国内镜像设置
** 使用淘宝镜像
https://npm.taobao.org/


你可以使用我们定制的 cnpm (gzip 压缩支持) 命令行工具代替默认的 npm:

$ npm install -g cnpm --registry=https://registry.npm.taobao.org


安装模块

从 registry.npm.taobao.org 安装所有模块. 当安装的时候发现安装的模块还没有同步过来, 淘宝 NPM 会自动在后台进行同步, 并且会让你从官方 NPM registry.npmjs.org 进行安装. 下次你再安装这个模块的时候, 就会直接从 淘宝 NPM 安装了.

$ cnpm install [name]

同步模块

直接通过 sync 命令马上同步一个模块, 只有 cnpm 命令行才有此功能:

$ cnpm sync connect

当然, 你可以直接通过 web 方式来同步: /sync/connect

$ open https://npm.taobao.org/sync/connect

其它命令

支持 npm 除了 publish 之外的所有命令, 如:

$ cnpm info connect



** 
方法一

国内镜像源，http://cnpmjs.org

1、通过config命令，

npm config set registry http://registry.cnpmjs.org
npm info underscore （如果上面配置正确这个命令会有字符串response）

或者

npm install -g cnpm --registry=http://r.cnpmjs.org

registry 参数的作用就是指向需要 download 的仓库。 cnpm 跟国外的 npm 是同步的，只要 npm 有更新，cnpm 就会跟着一起更新。

2、也可以安装 cnpm，安装好了之后使用 cnpm 来下载文件，原理跟上面是一样的，命令如下：

cnpm install -g package_name

3、编辑 ~/.npmrc 加入下面内容

registry = https://registry.npm.taobao.org

方法二

使用代理


# 设置代理地址和端口
npm config set proxy=http://127.0.0.1:8086

# 设置 https 的代理
npm config set https_proxy=http://127.0.0.1:8086

# 修改registry为npm默认镜像
npm config set registry=http://registry.npmjs.org

开启本地代理，npm 走你~

方法三

直接下载到本地。

直接把文件 download 下来，然后放到 node_module 之中就行了。如果是全局模块，找到全局 node_module 的位置，然后解压放进去就行了。

** 


npm

npm --registry=https://registry.npm.taobao.org install

electron

## Electron Mirror of China
ELECTRON_MIRROR="https://npm.taobao.org/mirrors/electron/"

phantomjs

PHANTOMJS_CDNURL=http://cnpmjs.org/downloads npm install phantomjs

chromedirver

CHROMEDRIVER_CDNURL=http://npm.taobao.org/mirrors/chromedriver

http://npm.taobao.org/mirrors

    3月26日发布 

* npm tftp 
https://github.com/gagle/node-tftp

npm install tftp -g




* Linking problems due to symbols with abi::cxx11?
http://stackoverflow.com/questions/36159238/linking-problems-due-to-symbols-with-abicxx11




Disclaimer, the following is not tested in production, use at your own risk.

You can yourself release your library under dual ABI. This is more or less analogous to OSX "fat binary", but built entirely with C++.

The easiest way to do so would be to compile the library twice: with -D_GLIBCXX_USE_CXX11_ABI=0 and with -D_GLIBCXX_USE_CXX11_ABI=1. Place the entire library under two different namespaces depending on the value of the macro:

#if _GLIBCXX_USE_CXX11_ABI
#  define DUAL_ABI cxx11 __attribute__((abi_tag("cxx11")))
#else
#  define DUAL_ABI cxx03
#endif

namespace CryptoPP {
  inline namespace DUAL_ABI {
    // library goes here
  }
}

Now your users can use CryptoPP::whatever as usual, this maps to either CryptoPP::cxx11::whatever or CryptoPP::cxx03::whatever depending on the ABI selected.

Note, the GCC manual says that this method will change mangled names of everything defined in the tagged inline namespace. In my experience this doesn't happen.

The other method would be tagging every class, function, and variable with __attribute__((abi_tag("cxx11"))) if _GLIBCXX_USE_CXX11_ABI is nonzero. This attribute nicely adds [cxx11] to the output of the demangler. I think that using a namespace works just as well though, and requires less modification to the existing code.

In theory you don't need to duplicate the entire library, only functions and classes that use std::string and std::list, and functions and classes that use these functions and classes, and so on recursively. But in practice it's probably not worth the effort, especially if the library is not very big.


#ifdef ABI_CHANGE 
inline namespace abi2 __attribute ((abi_tag)) { 
  class MyType { ... }; 
  MyType fn(); 
} 
#endif

* #CFLAGS+= -D_GLIBCXX_USE_CXX11_ABI=0

 使用 这个定义后 acum_uitls的库和acum一起联调时会死机, 
 查了一下, 发现在库里log出的acumconfig和在acum main里打出来的结构大小不一样.


2016-08-02 12:07:25.339307:[DEBUG]:acum_config.cc(1066): acumConfig [88][16]
2016-08-02 12:07:25.339380:[DEBUG]:acum_config.h(52): name[0], id[0], pre_alarm_enable[0], alarm_enable[0], watchdog_enable[0], buzz_enable[0], acu_query_interval[0]
2016-08-02 12:07:25.339393:[DEBUG]:acum_config.h(56): ip[]
2016-08-02 12:07:25.339401:[DEBUG]:acum_config.h(57): netmask[]
2016-08-02 12:07:25.339409:[DEBUG]:acum_config.h(58): gateway[]
2016-08-02 12:07:25.339418:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.339426:[DEBUG]:acum_config.h(60): tftp[]
2016-08-02 12:07:25.339433:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.339441:[DEBUG]:acum_config.h(63): sql[0]
2016-08-02 12:07:25.339449:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.340555:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.340576:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.340584:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.340590:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.340598:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.340606:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.340613:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.340619:[DEBUG]:acum_config.h(63): sql[1d13940]
2016-08-02 12:07:25.340625:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.340876:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.340908:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.340930:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.340969:[INFO]:acum_config.cc(424): loadAcuConfig end
2016-08-02 12:07:25.340981:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.340990:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.340996:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341004:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341013:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341020:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341026:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341032:[DEBUG]:acum_config.h(63): sql[1d13940]
2016-08-02 12:07:25.341038:[DEBUG]:acum_config.h(66): acu size [3]
2016-08-02 12:07:25.341046:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.341053:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.341061:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.341071:[DEBUG]:main.cc(53): tttttttttttttttttttttttttttttttttt
2016-08-02 12:07:25.341083:[DEBUG]:main.cc(34): acum config size[240][24]
2016-08-02 12:07:25.341612:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.341629:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.341637:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341643:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341649:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341656:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341664:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341671:[DEBUG]:acum_config.h(63): sql[1d28a90]
2016-08-02 12:07:25.341677:[DEBUG]:acum_config.h(66): acu size [0]
2016-08-02 12:07:25.341853:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.341881:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.341902:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.341935:[INFO]:acum_config.cc(424): loadAcuConfig end
2016-08-02 12:07:25.341946:[DEBUG]:acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.341954:[DEBUG]:acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.341961:[DEBUG]:acum_config.h(57): netmask[255.255.255.0]
2016-08-02 12:07:25.341969:[DEBUG]:acum_config.h(58): gateway[192.168.7.1]
2016-08-02 12:07:25.341977:[DEBUG]:acum_config.h(59): dns[]
2016-08-02 12:07:25.341984:[DEBUG]:acum_config.h(60): tftp[192.168.7.20]
2016-08-02 12:07:25.341991:[DEBUG]:acum_config.h(61): syslog[]
2016-08-02 12:07:25.341999:[DEBUG]:acum_config.h(63): sql[1d28a90]
2016-08-02 12:07:25.342006:[DEBUG]:acum_config.h(66): acu size [3]
2016-08-02 12:07:25.342013:[DEBUG]:acum_config.h(27): id[1], enable[1], ip[192.168.62.179]
2016-08-02 12:07:25.342021:[DEBUG]:acum_config.h(27): id[2], enable[0], ip[192.168.6.111]
2016-08-02 12:07:25.342029:[DEBUG]:acum_config.h(27): id[3], enable[0], ip[192.168.6.127]
2016-08-02 12:07:25.342036:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(52): name[1], id[1], pre_alarm_enable[1], alarm_enable[1], watchdog_enable[0], buzz_enable[1], acu_query_interval[10]
2016-08-02 12:07:25.342045:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(56): ip[192.168.7.130]
2016-08-02 12:07:25.342053:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(57): netmask[192.168.7.20]
2016-08-02 12:07:25.342061:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(58): gateway[Ò]
2016-08-02 12:07:25.342069:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(59): dns[(null)]
2016-08-02 12:07:25.342077:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(60): tftp[(null)]
2016-08-02 12:07:25.342085:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(61): syslog[(null)]
2016-08-02 12:07:25.342093:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(63): sql[0]
2016-08-02 12:07:25.342100:[DEBUG]:/home/songzc/codbase/new_330/acum/hosts/i386/include/acum_config.h(66): acu size [0]
Segmentation fault (core dumped)
* code style 

** java 
http://geosoft.no/development/javastyle.html

*** 
void setTopic(Topic topic) // NOT: void setTopic(Topic value) // NOT: void setTopic(Topic aTopic) // NOT: void setTopic(Topic t) void connect(Database database) // NOT: void connect(Database db) // NOT: void connect(Database oracleDB)

*** 
 12. The name of the object is implicit, and should be avoided in a method name.
line.getLength();   // NOT: line.getLineLength();
*** 
Note that the casing of the original words is almost entirely disregarded. Examples:
Prose form 	Correct 	Incorrect
"XML HTTP request" 	XmlHttpRequest 	XMLHTTPRequest
"new customer ID" 	newCustomerId 	newCustomerID
"inner stopwatch" 	innerStopwatch 	innerStopWatch
"supports IPv6 on iOS?" 	supportsIpv6OnIos 	supportsIPv6OnIOS
"YouTube importer" 	YouTubeImporter
YoutubeImporter* 	


* flatcc 在centos 7 上编译
  用 ninja 方式不行, 改用make 方式
  - 进入scripts目录
    cp build.cfg.make build.cfg
    然后再上层目录执行 ./scritps/build.sh

* archlinux 双显卡设置
 /etc/X11/xorg.conf.d


#+BEGIN_EXAMPLE

Section "InputClass"
    Identifier "evdev keyboard catchall"
    MatchIsKeyboard "on"
    MatchDevicePath "/dev/input/event*"
    Option "XkbLayout" "us"
    Option "XkbVariant" "dvp"
    Driver "evdev"
EndSection

Section "Device"
    Identifier             "Screen1"
    Driver                 "nouveau"
    BusID                  "PCI:1:0:0"
EndSection

#+END_EXAMPLE



** 
More than one graphics card

You must define the correct driver to use and put the bus ID of your graphic cards.

Section "Device"
    Identifier             "Screen0"
    Driver                 "nouveau"
    BusID                  "PCI:0:12:0"
EndSection

Section "Device"
    Identifier             "Screen1"
    Driver                 "radeon"
    BusID                  "PCI:1:0:0"
EndSection

To get your bus ID:

$ lspci | grep VGA

01:00.0 VGA compatible controller: nVidia Corporation G96 [GeForce 9600M GT] (rev a1)

The bus ID here is 1:0:0. 

* bash 文件大小
#+BEGIN_EXAMPLE


ls -l filename | awk '{print $5}'

du -b filename | awk '{print $1}'

wc -c filename | awk '{print $1}'

wc -c < filename

stat -c "%s" filename

#+END_EXAMPLE


* build tool
 - cmake
 - ninja build
 - scons
 - gmake

* 代码检查,分析,

vera++

** kitware


Software Testing for Everyone

CDash is an open source, web-based software testing server. CDash aggregates, analyzes and displays the results of software testing processes submitted from clients located around the world. Developers depend on CDash to convey the state of a software system, and to continually improve its quality. CDash is a part of a larger software process that integrates Kitware’s CMake, CTest, and CPack tools, as well as other external packages used to design, manage and maintain large-scale software systems.
Kitware’s Practical Software Process

Kitware’s Practical Software Process has grown organically over more than a decade of developing large-scale, scientific software systems such as VTK and ITK. It is a low-overhead, streamlined process that results in high-quality software systems. Rooted in the concepts of extreme programming and test-driven development, the PSP has benefited from the contributions of many dozens of individuals scattered around the globe. Besides the GE, Kitware, VTK and ITK communities and sponsors like NA-MIC and Sandia National Labs, many individuals like Alexander Neundorf (assisted with the CMake/KDE port) and Andrew Maclean (systems evaluator and tester) at the University of Sydney have and continue to contribute to the development of the PSP.

In recent years the PSP has grown to consist of several tools developed by Kitware and its partners, as well as external tools that have been adopted into the software process. These tools include:

    CMake, for cross-platform development
    CDash, the testing dashboard server
    CTest, the testing client
    CPack, the cross-platform packaging tool
    KWStyle, the configurable coding style evaluation tool
    Doxygen, for documentation generation
    Mantis, a bug tracking system
    dynamic memory analysis tools including ValGrind and Purify
    and code coverage tools such as gcov and BullsEye.



** 静态检查,分析
*** infer
*** cppcheck
  - compile

  make install SRCDIR=build CFGDIR=/usr/share/cppcheck/ HAVE_RULES=yes CXXFLAGS="-O2 -DNDEBUG -Wall -Wno-sign-compare -Wno-unused-function"


*** cpplint.py
    目前只能在python2下执行

     python2 ./cpplint.py  --verbose=0 --filter=-whitespace/parens,-whitespace/braces src/basic_lib/*
** code beautifier
   - astyle http://astyle.sourceforge.net/
   - clang-format
   - Uncrustify
* 自动备份

#+BEGIN_EXAMPLE

#!/bin/sh
BACKUPDIR=/home/BACKUP
LOGFILE=/home/data/backup.log
DIRFILE=/home/data/dir_need_backup
SUBDIR=`cat$DIRFILE`
MAILTO="zzz@163.com bbb@163.com"
BACKUP=$BACKUPDIR/backup
BACKUP_TMP=$BACKUPDIR/backup_tmp
#checkdirs
if [! -d $BACKUP ]; then
  mkdir-p $BACKUP
fi
if [! -d $BACKUP_TMP ]; then
  mkdir-p $BACKUP_TMP
fi
echo"Start backup!!!" > $LOGFILE 2>&1
fori in $SUBDIR
do
  echo$i >> $LOGFILE 2>&1
  if[ -f $i/HEAD ]; then
    #cleanup the git repository
    cd $i
    sudo su git -c "git gc" >> $LOGFILE 2>&1
    cd -
    #backup start
    mkdir$BACKUP_TMP/`basename $i`
    gitclone --bare $i $BACKUP_TMP/`basename $i` >> $LOGFILE 2>&1
    if[ $? = 0 ]; then
      echo$i backup success!!! >> $LOGFILE 2>&1
      rm-rf $BACKUP/`basename $i`
      mv$BACKUP_TMP/`basename $i` $BACKUP
    else
      echo$i backup fail!!! >> $LOGFILE 2>&1
      echo"Error occured during backup $i, detail in $LOGFILE" |mutt -s "BACKUP MESSAGE" $MAILTO
    fi
  else
    echo$i is not a git dir!!! >> $LOGFILE 2>&1
  fi
done
echo"Backup complete!!!" >> $LOGFILE 2>&1

#+END_EXAMPLE
* cgi/fcgi
  - cgicc
  - fastcgi++
  - kcgi
  - qdecoder
* json
*** json MIME
   - Why you shouldn't use text/html for JSON
   http://jibbering.com/blog/?p=514

#+BEGIN_EXAMPLE

Don't serve JSON as text/html
Another day, another XSS flaw, this one in Google again, but this is a little more interesting than the normal ones, what this one shows is how JSON results add an extra vector to attack that might be missed by your QA team. The problem here was that the JSON was returned with a mime-type of text/html, a browser will render that as if it was an HTML page, even if it's really just a javascript snippet. The easiest way to protect against these is to ensure that all javascript recieved by the XMLHTTPRequest object is returned with a suitable mime-type - application/json That will mean even when you make a mistake and write un-encoded untrusted data to the document, it won't allow people to attack your site. The google exploit was reported here, it's at the time of writing unpatched, unfortunately that was down to the discoverer not giving google any time to fix, whilst they have had their problems before, recently they have patched quickly, so this was not very fair, or wise. Google also appear to be taking testing their own services for security flaws more seriously, they recently had a presentation to the QA team that you can watch on Google Video. As I've said before, the everything on a single domain causes problems, it means any exploit anywhere on the domain, allows you to exploit any service provided for the domain. This exploit is also present in https:// google, so to re-enforce the problem XSS can present to a user, and why XSS is not simply about cookie stealing. Here's a simple demonstration of using the exploit to steal username and password from google adsense. The exploit is simply used to create an IFRAME that fills the document and points it to a google adsense login, when the user logs in, the username and password are alerted - also after logging in, then the "today's earnings" are alerted. Of course a real attacker would not alert these fields, but would sent them off to a site to be collected later. Are google adsense passwords useful? Would you notice if the address or account to get the cash changed until you'd not got the cheque? The script code is simple, you don't need to be clever, and phishers generally aren't stupid, it takes brains to launder money.

    document.body.innerHTML="<div><iframe src='https://www.google.com/adsense/report/overview'"+
    " onload='go()' style='position:absolute;top:0;left:0;height:100%;width:100%;'></div>";

    function go() {
      try {
      var win=window.frames[0];
      win.document.body.style.overflow="hidden";
      win.document.body.style.border="0px solid white";
      var doc=win.frames[0].document.forms[0];
      doc.onsubmit=function() {
       alert("Your adsense username and password are:\n"+
       doc["Email"].value+'\nand\n'+doc["Passwd"].value);
       x=window.open(location.href);
      }
     } catch (e) {
      try {
       var win=window.frames[0];
       var doc=win.document.body;
       var x="Today's Earnings:"+doc.getElementsByTagName('h1')[0];
       alert(x.getElementsByTagName('span')[0].innerHTML.replace(" ",""));
      } catch (e) {}
     }
    }

#+END_EXAMPLE

*** JSON 的 MIME 类型 
   http://blog.csdn.net/lilin_emcc/article/details/26670099

RFC 4627 "The application/json Media Type for JavaScript Object Notation (JSON)" 于2006年7月将 JSON 注册为一个 MIME 类型：application/json 。

参见：

    ECMA-404 The JSON Data Interchange Format
    RFC 4627 - The application/json Media Type for JavaScript Object Notation (JSON)
    JSON: http://json.org/json-zh.html
    MIME: http://www.iana.org/assignments/media-types/media-types.xhtml
    application/json: http://www.iana.org/assignments/media-types/application/json
    在线 JSON 校验格式化工具

* linux 硬盘分区
** /usr 独立分区
   archlinux mkinitcpio

   /usr放到单独分区
   arch-chroot
   


#+BEGIN_EXAMPLE
[songzc@localhost ~]$ df -h 
Filesystem      Size  Used Avail Use% Mounted on
dev             1.9G     0  1.9G   0% /dev
run             1.9G  908K  1.9G   1% /run
/dev/sda6        15G   13G  922M  94% /
tmpfs           1.9G     0  1.9G   0% /dev/shm
tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup
tmpfs           1.9G   36K  1.9G   1% /tmp
/dev/sda5       123G   15G  108G  13% /media/d
/dev/sda7       9.8G  9.6G     0 100% /var
/dev/sda8       216G  146G   60G  72% /home
tmpfs           384M  4.0K  384M   1% /run/user/1000

#+END_EXAMPLE

#+BEGIN_EXAMPLE

Disk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xb753b753

Device     Boot     Start       End   Sectors   Size Id Type
/dev/sda1  *           63 209728574 209728512   100G  7 HPFS/NTFS/exFAT
/dev/sda2       209728575 976768064 767039490 365.8G  f W95 Ext'd (LBA)
/dev/sda5       209728638 465611894 255883257   122G  7 HPFS/NTFS/exFAT
/dev/sda6       465614848 497072127  31457280    15G 83 Linux
/dev/sda7       497074176 518045695  20971520    10G 83 Linux
/dev/sda8       518047744 976768064 458720321 218.8G 83 Linux


#+END_EXAMPLE

#+BEGIN_EXAMPLE
# 
# /etc/fstab: static file system information
#
# <file system>	<dir>	<type>	<options>	<dump>	<pass>
UUID=1b91cdb6-ea27-4574-8321-1fefc80af74d           	/         	ext4      	rw,relatime,data=ordered	0 1

UUID=02ae4950-f8f8-44ca-98cd-32752bc746c7           	/var      	ext4      	rw,relatime,data=ordered	0 2

UUID=a43ba9f8-4dda-4330-b7ae-72f70d4c6852    	/home     	ext4      	rw,relatime,data=ordered	0 2
UUID=00092D2F0005B649                           /media/d    ntfs-3g      defaults,umask=000 0 0

#+END_EXAMPLE

#+BEGIN_EXAMPLE
total 0
lrwxrwxrwx 1 root root 10 Nov 17 08:13 00092D2F0005B649 -> ../../sda5
lrwxrwxrwx 1 root root 10 Nov 17 08:13 000A975F0008338E -> ../../sda1
lrwxrwxrwx 1 root root 10 Nov 17 08:13 02ae4950-f8f8-44ca-98cd-32752bc746c7 -> ../../sda7
lrwxrwxrwx 1 root root 10 Nov 17 08:13 1b91cdb6-ea27-4574-8321-1fefc80af74d -> ../../sda6
lrwxrwxrwx 1 root root 10 Nov 17 08:13 a43ba9f8-4dda-4330-b7ae-72f70d4c6852 -> ../../sda8

#+END_EXAMPLE

#+BEGIN_EXAMPLE
[songzc@localhost ~]$ sudo du -d 1 -h /
[sudo] password for songzc: 
0	/sys
333M	/root
13G	/usr
15G	/media
0	/dev
9.6G	/var
340M	/opt
4.0K	/mnt
74M	/srv
16K	/lost+found
du: cannot access '/proc/852/task/852/fd/4': No such file or directory
du: cannot access '/proc/852/task/852/fdinfo/4': No such file or directory
du: cannot access '/proc/852/fd/3': No such file or directory
du: cannot access '/proc/852/fdinfo/3': No such file or directory
0	/proc
du: cannot access '/run/user/1000/gvfs': Permission denied
912K	/run
8.0K	/.docker
41M	/boot
19M	/etc
36K	/tmp
146G	/home
183G	/
[songzc@localhost ~]$ 


#+END_EXAMPLE


* su: Authentication failure
I fixed the UID with Konqueror <Properties>. For my information, how do you add UID with "chown"?

You mean chmod?  chmod u+s <file>


>>> The module r8168 conflicts with r8169. You can blacklist it with:
>>>  `echo "blacklist r8169" > /etc/modprobe.d/r8169_blacklist.conf`
ldconfig: /usr/lib/libjlinkarm.so.4 is not a symbolic link

* kernal module load
** virtualbox
/etc/modules-load.d/virtualbox.conf

#+BEGIN_EXAMPLE

vboxdrv
vboxguest
vboxsf
vboxvideo
vboxnetadp
vboxnetflt


#+END_EXAMPLE


** 
#+BEGIN_EXAMPLE
man modules-load.d
#+END_EXAMPLE

  modules-load.d

 systemd-modules-load.service(8) reads files from the above
       directories which contain kernel modules to load during boot in a
       static list. Each configuration file is named in the style of
       /etc/modules-load.d/program.conf. Note that it is usually a better
       idea to rely on the automatic module loading by PCI IDs, USB IDs, DMI
       IDs or similar triggers encoded in the kernel modules themselves
       instead of static configuration like this. In fact, most modern
       kernel modules are prepared for automatic loading already.
https://wiki.archlinux.org/index.php/Systemd

Troubleshooting

#+BEGIN_EXAMPLE
$ systemctl --failed
$ systemctl status systemd-modules-load
$ journalctl _PID=15630
#+END_EXAMPLE

* virtual machine
https://en.wikipedia.org/wiki/Virtual_machine


在详细介绍之前，有几个概念要说明：
1. VM（Virtual Machine）——虚拟机，指由Vmware模拟出来的一台虚拟的计算机，也即逻辑上的一台计算机。
2.HOST——指物理存在的计算机，Host′s OS指HOST上运行的操作系统。
3. Guest OS——指运行在VM上的操作系统。例如在一台安装了Windows NT的计算机上安装了Vmware，那么，HOST指的是安装Windows NT的这台计算机，其Host′s OS为Windows NT。VM上运行的是Linux，那么Linux即为Guest OS。

** ArchLinux 下 VirtualBox 增强设置
http://www.cnblogs.com/itxdm/p/5544269.html

关于guest-dkms和host-dkms你知道有什么区别嘛？这俩个并不是让你随便选择的，我在一篇文章上看到这俩者的区别。

    virtualbox-host-dkms是对archlinux来说的。arch是vbox主人，所以arch应该安装host-dkms。
    virtualbox-guest-dkms是对virtualbox来说的。arch是vbox客人，所以virtualbox安装arch的！

对我来说，archlinux是我的主系统，virtualbox是我的虚拟机。（关于DKMS点这里）

    我需要真机安装virtualbox-host-dkms，然后安装virtualbox-guest-dkms是虚拟机增强工具。

virtualbox-guest-utils是属于virtualbox中的ArchLinux安装增强工具来使用的！！！

* packet crafting
** software
   - ostinato
   - scapy
   - tcpreplay
   - yersinia
   - Mausezahn (dead) ==> netsniff-ng toolkit






https://en.wikipedia.org/wiki/Packet_crafting

Packet crafting is a technique that allows network administrators to probe firewall rule-sets and find entry points into a targeted system or network. This is done by manually generating packets to test network devices and behaviour, instead of using existing network traffic.[1] Testing may target the firewall, IDS, TCP/IP stack, router or any other component of the network.[1][2] Packets are usually created by using a packet generator or packet analyzer which allows for specific options and flags to be set on the created packets. The act of packet crafting can be broken into four stages: Packet Assembly, Packet Editing, Packet Play and Packet Decoding.[1][2] Tools exist for each of the stages - some tools are focussed only on one stage while others such as Ostinato try to encompass all stages.

Contents

    1 Packet Assembly
    2 Packet Editing
    3 Packet Play
    4 Packet Decoding
    5 See also
    6 References
    7 External links

Packet Assembly

Packet Assembly is the creation of the packets to be sent. Some popular programs used for packet assembly are Hping, Nemesis, Ostinato, Cat Karat packet builder, Libcrafter, libtins, Scapy, Wirefloss and Yersinia.[1][2][3] Packets may be of any protocol and are designed to test specific rules or situations. For example, a TCP packet may be created with a set of erroneous flags to ensure that the target machine sends a RESET command or that the firewall blocks any response.[1][2]
Packet Editing

Packet Editing is the modification of created or captured packets. This involves modifying packets in manners which are difficult or impossible to do in the Packet Assembly stage, such as modifying the payload of a packet.[2] Programs such as Ostinato, Netdude allow a user to modify recorded packets' fields, checksums and payloads quite easily.[1] These modified packets can be saved in packet streams which may be stored in pcap files to be replayed later.
Packet Play

Packet Play or Packet Replay is the act of sending a pre-generated or captured series of packets. Packets may come from Packet Assembly and Editing or from captured network attacks. This allows for testing of a given usage or attack scenario for the targeted network. Tcpreplay is the most common program for this task since it is capable of taking a stored packet stream in the pcap format and sending those packets at the original rate or a user-defined rate. Ostinato added support for pcap files in version 0.4.[4] Some packet analyzers are also capable of packet replay.
Packet Decoding

Packet Decoding is the capture and analysis of the network traffic generated during Packet Play. In order to determine the targeted network's response to the scenario created by Packet Play, the response must be captured by a packet analyzer and decoded according to the appropriate specifications. Depending on the packets sent, a desired response may be no packets were returned or that a connection was successfully established, among others.
See also

    Comparison of packet analyzers
    Comparison of packet crafter libraries
    Packetsquare
    Replay attack
    Packet Sender

References
Zereneh, William. "Packet Crafting" (PDF). Retrieved 2010-08-01.
Poor, Mike. "Packet Craft for Defense-in-Depth" (PDF). InGuardians. Retrieved 2010-08-01.
"Top 4 Packet Crafting Tools". SecTools.org. Retrieved 2010-08-01.

    "Ostinato ChangeLog". Retrieved 2011-04-30.

External links

    Packet Crafting for Firewall & IDS Audits (Part 1 of 2) by Don Parker [1]
    Wikiformat article detailing Packet crafting
    Bit-Twist - Libpcap-based Ethernet packet generator [2]
    Packet Sender - open source packet generator focused on ease-of-use


#+BEGIN_EXAMPLE
'''Packet crafting''' is a technique that allows [[network administrator]]s to probe [[Firewall (computing)|firewall]] rule-sets and find entry points into a targeted system or network.  This is done by manually generating [[Packet (information technology)|packets]] to test network devices and behaviour, instead of using existing network traffic.<ref name=zereneh>{{cite web | last = Zereneh | first= William | url = http://www.scs.ryerson.ca/~zereneh/linux/PacketCrafting.pdf | title= Packet Crafting | accessdate = 2010-08-01 }}</ref> Testing may target the firewall, [[Intrusion detection system|IDS]], [[TCP/IP stack]], [[Router (computing)|router]] or any other component of the network.<ref name=zereneh /><ref name=poor>{{cite web | last = Poor | first = Mike | url = http://www.inguardians.com/research/docs/packetfoo.pdf | title = Packet Craft for Defense-in-Depth | publisher = InGuardians | accessdate = 2010-08-01 }}</ref> Packets are usually created by using a [[packet generator]] or [[packet analyzer]] which allows for specific options and [[Flag (computing)|flags]] to be set on the created packets. The act of packet crafting can be broken into four stages: Packet Assembly, Packet Editing, Packet Play and Packet Decoding.<ref name=zereneh /><ref name=poor /> Tools exist for each of the stages - some tools are focussed only on one stage while others such as [http://ostinato.org/ Ostinato] try to encompass all stages.

==Packet Assembly==
Packet Assembly is the creation of the packets to be sent. Some popular programs used for packet assembly are [[Hping]], [[Nemesis (software)|Nemesis]], [http://ostinato.org/ Ostinato], [http://packetbuilder.net/ Cat Karat packet builder], [http://code.google.com/p/libcrafter/ Libcrafter], [http://libtins.sourceforge.net libtins], [[Scapy]], [http://wirefloss.com Wirefloss] and [[Yersinia (computing)|Yersinia]].<ref name=zereneh /><ref name=poor /><ref name=sectools>{{cite web | url = http://sectools.org/packet-crafters.html | title= Top 4 Packet Crafting Tools | publisher =  SecTools.org | accessdate = 2010-08-01}}</ref> Packets may be of any [[Communications protocol|protocol]] and are designed to test specific rules or situations. For example, a [[TCP packet]] may be created with a set of erroneous flags to ensure that the target machine sends a RESET command or that the firewall blocks any response.<ref name=zereneh /><ref name=poor />

==Packet Editing==
Packet Editing is the modification of created or captured packets. This involves modifying packets in manners which are difficult or impossible to do in the Packet Assembly stage, such as modifying the payload of a packet.<ref name=poor /> Programs such as [http://ostinato.org/ Ostinato], [[Netdude]] allow a user to modify recorded packets' fields, checksums and payloads quite easily.<ref name=zereneh /> These modified packets can be saved in packet streams which may be stored in [[pcap]] files to be replayed later.

==Packet Play==
Packet Play or Packet Replay is the act of sending a pre-generated or captured series of packets. Packets may come from Packet Assembly and Editing or from captured network attacks. This allows for testing of a given usage or attack scenario for the targeted network. [[Tcpreplay]] is the most common program for this task since it is capable of taking a stored packet stream in the [[pcap]] format and sending those packets at the original rate or a user-defined rate. [http://ostinato.org/ Ostinato] added support for [[pcap]] files in version 0.4.<ref name=ostcl>{{cite web | url = http://ostinato.org/wiki/ChangeLog | title= Ostinato ChangeLog | accessdate = 2011-04-30 }}</ref> Some packet analyzers are also capable of packet replay.

==Packet Decoding==
Packet Decoding is the [[Packet capture|capture]] and analysis of the network traffic generated during Packet Play. In order to determine the targeted network's response to the scenario created by Packet Play, the response must be captured by a [[packet analyzer]] and decoded according to the appropriate specifications. Depending on the packets sent, a desired response may be no packets were returned or that a connection was successfully established, among others.

== See also ==
*[[Comparison of packet analyzers]]
*[[Comparison of packet crafter libraries]]
*[[Packetsquare]]
*[[Replay attack]]
*[[Packet Sender]]

==References==
<references />

==External links==
*Packet Crafting for Firewall & IDS Audits (Part 1 of 2) by Don Parker [http://www.securityfocus.com/infocus/1787]
*Wikiformat article detailing [http://www.wikistc.org/wiki/Packet_crafting Packet crafting]
*Bit-Twist - Libpcap-based Ethernet packet generator [http://bittwist.sourceforge.net/]
*[https://packetsender.com/ Packet Sender] - open source packet generator focused on ease-of-use

{{DEFAULTSORT:Packet Crafting}}
[[Category:Network analyzers]]

[[ru:Конструктор пакетов]]
#+END_EXAMPLE


* packet injection

https://en.wikipedia.org/wiki/Packet_injection

* Linux distributions
  - kali linux
    Our Most Advanced Penetration Testing Distribution, Ever.
* rrdtool
   Data logging and graphing application
* 消息队列
  kafka、rabbitmq、zeromq
* gdb cross compile
https://sourceware.org/gdb/wiki/BuildingCrossGDBandGDBserver

#+BEGIN_EXAMPLE

 When using autoconf, there are three system definitions (or machine definitions)
 that are used to identify the “actors” in the build process; (...) These three definitions are:

host

    The system that is going to run the software once it is built. Once the software
    has been built, it will execute on this particular system.

build

    The system where the build process is being executed. For most uses this
    would be the same as the host system, but in case of cross-compilation
    the two obviously differ.

target

    The system against which the software being built will run on. This only exists, or rather
    has a meaning, when the software being built may interact specifically with a
    system that differs from the one it's being executed on (our host). This is the case
    for compilers, debuggers, profilers and analyzers and other tools in general.

#+END_EXAMPLE

** gdb 在ppc上编译　
config_ppc.mk
#+BEGIN_SRC 
GDB_VERSION=7.12
CLEAN_DIRS+=gdb

gdb:$(PACKET_DIR)/gdb-$(GDB_VERSION).tar.gz 
	$(EXTRACT_GZ)
	cd $@ &&  ./configure --target=ppc-linux

.gdb: gdb
	cd $< && make

#+END_SRC

** gdb 
#+BEGIN_SRC 
/path/to/gdb-src/configure --target=arm-linux-gnueabi

#+END_SRC
** gdb server

#+BEGIN_SRC 

/path/to/gdb-src/gdb/gdbserver/configure --host=arm-linux-gnueabi
#+END_SRC

* stl 删除操作
  由于stl(map, set)内部的存储结构不是线性的,　所以删除时不能用++it这种操作
  
     for(it = enter_faults_.begin(); it != enter_faults_.end(); ++it){

             enter_faults_.erase(it);
                
        }

这种操作因为删除后的内部结构重整,　++it已经不是原来的了,　会crash
 
 list的操作erase会返回下一个单元的iterator,　但是在for要按照下面的写法
      for(it = enter_faults_.begin(); it != enter_faults_.end();){
            if(it->id.ToInt() == id.ToInt() && it->sub_type == sub_type){
                it = enter_faults_.erase(it);
            }else{
               ++it; 
            }
        }
        
把++操作移出来,　因为如果删除的是最后一个单元时,　再执行++就会crash

  还是最好写成while 形式

#+BEGIN_SRC cpp
    it = enter_faults_.begin();
    while(it != enter_faults_.end()){
            if(it->id.ToInt() == id.ToInt() && it->sub_type == sub_type){
                enter_faults_.erase(it);
               it = enter_faults_.begin();               
            }
   }
  
#+END_SRC  
* ssh 不输入密码

在自己的机器
#+BEGIN_EXAMPLE
ssh-keygen -t rsa
#+END_EXAMPLE

接受默认值就行

其中公共密钥保存在 ~/.ssh/id_rsa.pub
私有密钥保存在 ~/.ssh/id_rsa

scp id_rsa.pub到服务器的用户目录下

#+BEGIN_EXAMPLE
 scp ~/.ssh/id_rsa.pub rh1:/home/user1/.ssh/authorized_keys
#+END_EXAMPLE

** http://bbs.chinaunix.net/thread-343905-1-1.html

有些时候，我们在复制/移动文件到另一台机器时会用到scp，因为它比较安全。但如果每次

都要输入密码，就比较烦了，尤其是在script里。不过，ssh有另一种用密钥对来验证的方

式。下面写出我生成密匙对的过程，供大家参考。

第一步：生成密匙对，我用的是rsa的密钥。使用命令 "ssh-keygen -t rsa"


       [user1@rh user1]$ ssh-keygen -t rsa
       Generating public/private rsa key pair.
       Enter file in which to save the key (/home/user1/.ssh/id_rsa):
       Created directory '/home/user1/.ssh'.
       Enter passphrase (empty for no passphrase):
       Enter same passphrase again:
       Your identification has been saved in /home/user1/.ssh/id_rsa.
       Your public key has been saved in /home/user1/.ssh/id_rsa.pub.
       The key fingerprint is:
       e0:f0:3b:d3:0a:3d:da:42:01:6a:61:2f:6c:a0:c6:e7 user1@rh.test.com
       [user1@rh user1]$

复制代码

生成的过程中提示输入密钥对保存位置，直接回车，接受默认值就行了。接着会提示输入一

个不同于你的password的密码，直接回车，让它空着。当然，也可以输入一个。(我比较懒

，不想每次都要输入密码。) 这样，密钥对就生成完了。

其中公共密钥保存在 ~/.ssh/id_rsa.pub
私有密钥保存在 ~/.ssh/id_rsa

然后改一下 .ssh 目录的权限，使用命令 "chmod 755 ~/.ssh"


       [user1@rh user1]$ chmod 755 ~/.ssh
       [user1@rh user1]$

复制代码


之后把这个密钥对中的公共密钥复制到你要访问的机器上去，并保存为

~/.ssh/authorized_keys.


       [user1@rh user1]$ scp ~/.ssh/id_rsa.pub rh1:/home/user1/.ssh/authorized_keys
       user1@rh1's password:
       id_rsa.pub                                    100%  228     3.2MB/s   00:00
       [user1@rh user1]$

复制代码


之这样就大功告成了。之后你再用ssh scp sftp 之类的访问那台机器时，就不用输入密码

了，用在script上更是方便。

** 
 http://jingyan.baidu.com/article/f3ad7d0fe7bd0d09c2345b75.html
 - 首先登入一台linux服务器，此台做为母机（即登入其他linux系统用这台做为入口）；执行一行命令生成key文件：

#+BEGIN_EXAMPLE
ssh-keygen -t rsa
#+END_EXAMPLE
  - 在母机上，进入/roo/.ssh目录，找到id_rsa.pub该文件，这个文件就是刚才执行ssh-keygen所生成的公钥key文件。

  - 用scp命令，将母机产生的key拷一份到远程的linux服务器上，并命名成authorized_keys；scp ~/.ssh/id_rsa.pub  root@192.168.1.113:/root/.ssh/authorized_keys。这一步的操作需要手动输入密码。
如何通过linux ssh远程linux不用输入密码登入
  - 现在为止，你已完成了所有的操作；可在母机通过ssh root@192.168.1.113 你会发现不在用输放密码了。相同的scp命令也是一样的情况，无需手动输入密码

  注意: 要正确命名公钥的名称

* libreoffice 转换文件格式(doc, pdf...) 代替(pandoc的相同格式转换)



#+BEGIN_SRC 
libreoffice --invisible --convert-to docx test.odt

#+END_SRC

* lua vs js

** should-i-use-lua
http://stackoverflow.com/questions/18507447/should-i-use-lua

https://debian-administration.org/article/264/Embedding_a_scripting_language_inside_your_C/C_code
http://lua-users.org/wiki/BindingCodeToLua

https://developers.google.com/v8/embed
https://github.com/v8/v8/wiki/Embedder's%20Guide

http://webserver2.tecgraf.puc-rio.br/~celes/tolua/


https://www.lua.org/wshop16/

http://blog.carlesmateo.com/2014/10/13/performance-of-several-languages/


#+BEGIN_EXAMPLE

I'm assuming you're writing a standalone application in C or C++, and you're looking for a language to enable people to extend that application by embedding another language. That's pretty much Lua's territory.

Lua's strong point is that it's very easy to embed in your (C) application. It compiles quickly, it's tiny, licensing is liberal and using C functions from Lua is relatively easy. Standard Lua has enough performance for most things you'd use a language like this for; if you need more raw speed you could look into LuaJIT, the JIT-compiler for Lua.

As for your questions:

    LuaJIT will probably be pretty much as fast as you can get for a dynamic language. Lua is used in games (Sim City, Far Cry, World of Warcraft), where performance is very important.
    If by 'resources' you mean documentation: sure. Lua is a very simple language, much simpler than JavaScript; the manual should help you get started with the language itself, the wiki is tasty for tips about the embedding process. This article has an example you can copy/paste.
    Lua pretty much runs everywhere a C program will run. It doesn't even need an operating system, and it doesn't depend on anything at runtime if you bundle it correctly.

Embedding a full JavaScript environment in your application, and interfacing your application to the JS environment, can be a lot of work (even though Google's V8 engine has some functionality to help you; see here).

#+END_EXAMPLE

* test tool && lib
** quickcheck(haskell) , rapidcheck(c++)
** fff (Fake Function Framework)

fff is a micro-framework for creating fake C functions for tests. Because life is too short to spend time hand-writing fake functions for testing.
https://github.com/meekrosoft/fff

**  C/C++单元测试框架的选择

http://blog.csdn.net/neilxp/article/details/4397275



Quote from Bas's mail:


CppUTest is not "my framework" Its an framework that is a 4th generation from CppUnit. Michael Feathers started CppUnit long ago. But then, the framework became too complex as it added too much features and used things like templates, which makes code quickly complex. So he abandoned and wrote CppUnitLite. That got improved in ObjectMentor to a tool called CppTest Tools (which included FIT for C++). The unit test part were extracted by me and James Grenning and  renamed to CppUTest. Thus, its a rather old framework and not  "developed by me"

Similar frameworks are googletest, which came a lot later than CppUTest, which is pretty good but it has some serious sucky features (like the braindead << way of working). CxxTest is quite popular, but it requires a perl script to generate your test runner, which kinda sucks. CppUnit is sucky and is even abandoned by the original author (as mentioned) and CUnit sucks even more as it requires you to put manual calls.

* wine qq 

#+BEGIN_SRC 

  pacman -S winetricks
  WINEARCH=win32 winetricks qq 

#+END_SRC
** wine in Xmonad is not tiling
http://unix.stackexchange.com/questions/71318/wine-in-xmonad-is-not-tiling


Q: I'm using Wine application in Xmonad. But it is not tiling like other applications. Can this be configured, that Wine will tile like other applications?

A: You're not giving me a whole lot to work with (no details about your wine setup) but maybe the following will help: Run winecfg, switch to the graphics tab and tell it to emulate a virtual desktop (choose an appropriate size). Afterwards, everything should tile nicely.
A: Wine programs default to floating, but using super-t (or whatever key binding you use for turning on tiling) will make them tile just fine. Is that your issue?

* postgresql


** PostgreSQL数据库远程连接功能的开启
 
需要修改连个配置文件，默认位于 安装目录的data子文件夹下。
 
1.postgresql.conf
修改成监听所有ip地址的连接请求，如下：
listen_addresses = '*'  
 
2.pg_hda.conf
在末尾的地方添加一行，如下：
host    all         all         0.0.0.0/0      md5



  
** How to change PostgreSQL user password?
https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password



Then type:

    sudo -u postgres psql

Then:

    \password postgres

Then to quit:

    \q

If that does not work, reconfigure authentication.

Edit /etc/postgresql/9.1/main/pg_hba.conf (path will differ) and change:

    local   all             all                                     peer

to:

    local   all             all                                     md5

Then restart the server:

sudo service postgresql restart

** PostgreSQL Requirements

As of GitLab 9.3, PostgreSQL 9.2 or newer is required, and earlier versions are not supported. We highly recommend users to use at least PostgreSQL 9.6 as this is the PostgreSQL version used for development and testing.

Users using PostgreSQL must ensure the pg_trgm extension is loaded into every GitLab database. This extension can be enabled (using a PostgreSQL super user) by running the following query for every database:

CREATE EXTENSION pg_trgm;

On some systems you may need to install an additional package (e.g. postgresql-contrib) for this extension to become available.
* gitlab
** centos 7
https://about.gitlab.com/downloads/#centos7

#+BEGIN_EXAMPLE
sudo yum install curl policycoreutils openssh-server openssh-clients
sudo systemctl enable sshd
sudo systemctl start sshd
sudo yum install postfix
sudo systemctl enable postfix
sudo systemctl start postfix
sudo firewall-cmd --permanent --add-service=http
sudo systemctl reload firewalld

curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
sudo yum install gitlab-ce

sudo gitlab-ctl reconfigure

#+END_EXAMPLE

gitlab-ctl start
gitlab-ctl stop

** nginx 配置

gitlab 使用自带的nginx，默认配置在

#+BEGIN_EXAMPLE
/var/opt/gitlab/nginx
#+END_EXAMPLE

默认使用80端口，和系统的nginx冲突，改为8850
** subgit 

#+BEGIN_EXAMPLE
/opt/subgit-3.2.4/bin/subgit configure svn://192.168.51.239/iplc/esc /var/opt/gitlab/git-data/repositories/iplc/esc.git

#+END_EXAMPLE

Make sure you see the line

#+BEGIN_EXAMPLE
Git repository is served by GitLab, hooks will be installed into 'custom_hooks' directory.
#+END_EXAMPLE



#+BEGIN_EXAMPLE

Using subgit to migrate non-standard layout subversion repository with no branches, tags and trunk

Tag: git,svn,subgit

I have been using subgit to convert my subversion repository to git. Unfortunately, I have a sub-project in one of my branches which is not in standard layout. The non-standard sub-project is not included in the conversion.

The content of the sub-project is

/my-subproject
   file1
   dir1
     subdir1-file1
   file2

Is there a way to specify it in the mapping in subgit.conf? Below is the mapping in subgit.conf.

Eg.

[git "my-subproject"]
translationRoot = my-subprojcet
repository = /var/git/my-subproject.git
pathEncoding=UTF-8

trunk = trunk:refs/heads/master
branches = branches/*:refs/heads/*
shelves = shelves/*:refs/shelves/*
tags = tags/*:refs/tags/*

Best How To :

As found in subgit project mapping the configuration should be

[git "my-subproject"]
repository = /var/git/my-subproject.git

translationRoot = /

trunk = /my-subproject:refs/heads/master
branches = branches/project/*:refs/heads/*
shelves = shelves/project/*:refs/shelves/*
tags = tags/project/*:refs/tags/*

I hope this would be useful.

#+END_EXAMPLE



** sync2git
https://github.com/dpocock/sync2git

* lldp,

chown -R ldap:ldap /var/lib/ldap

** phpldapadmin

nginx的配置文件，/etc/nginx/sites-available/

在 /etc/nginx/sites-enable/建立一个软联接

php-fpm 设置在/etc/php-fpm.conf ,/etc/php-fpm.d/www.conf

#+BEGIN_SRC 
server {
    #server_name ldap01.linoxide.com;
    listen 8840;
    
    # application: phpldapadmin
    location /phpldapadmin {
        alias /usr/share/phpldapadmin/htdocs;
        index index.php index.html index.htm;
    }
    
    location ~ ^/phpldapadmin/.*\.php$ {
        root /usr/share;
        if ($request_filename !~* htdocs) {
           rewrite ^/phpldapadmin(/.*)?$ /phpldapadmin/htdocs$1;
        }
        
        #fastcgi_pass unix:/run/php/php7.0-fpm.sock;
		fastcgi_pass 127.0.0.1:9000;        
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME $request_filename;
        include /etc/nginx/fastcgi_params;
    }
    
    # logging
    error_log /var/log/nginx/phpldapadmin.error.log;
    access_log /var/log/nginx/phpldapadmin.access.log;
}

#+END_SRC

#+BEGIN_SRC 

chow

#+END_SRC

* sway 

https://github.com/SirCmpwn/sway/wiki

#+BEGIN_SRC 

mkdir -p ~/.config/sway
cp /etc/sway/config ~/.config/sway/
$EDITOR ~/.config/sway/config


#+END_SRC
* 一行命令解决linux显示txt乱码
一行命令解决linux显示windows中文txt乱码的问题
前提是你的linux可以正常显示中文，只是打开windows txt文件会有乱码。
原因是linux下用的编码一般是utf-8； windows 一般是gb18030或gb2312；那么只需要简单的转换下就可以正常显示了。
用以下命令：(需要首先安装convert转换器：#yum install convert)
# iconv -f gb18030 -t utf-8 test.txt > test utf8.txt
（-f 就是源编码from，-t 转换目标编码to，test.txt是源文件，>是重定向，testutf8.txt是生成目标编码的文件）。
注：上一行所用命令中到gb18030也可以用gb2312代替，GB18030编码向下兼容GBK和GB2312，兼容的含义是不仅字符兼容，而且相同字符的编码也相同。推荐优先使用gb18030.
* 'rxvt-unicode-256color': unknown terminal type.
    /etc/profile 加入
#+BEGIN_EXAMPLE
  export TERM="xterm"
#+END_EXAMPLE




* 解决sudo: sorry, you must have a tty to run sudo



1. 编辑 /etc/sudoers
 
  1）Defaults    requiretty，修改为 #Defaults    requiretty，表示不需要控制终端。
 
  2）Defaults    requiretty，修改为 Defaults:nobody !requiretty，表示仅 nobody 用户不需要控制终端。

  如果修改为 Defaults:%nobody !requiretty，表示仅 nobody 组不需要控制终端。
 
其实只要注释掉）Defaults    requiretty 那个就可以了。表示在执行的时候不打开终端。但是，有的shell必须要有终端才可以执行。这样显然是不行的。后来，又找到一片文章才搞定。下面为抄录的，仅为记录以后使用。
 

* cmake cross compile

http://www.vtk.org/Wiki/CMake_Cross_Compiling


#+BEGIN_EXAMPLE

Setting up the system and toolchain

When cross compiling, CMake cannot guess the target system, so you have to preset some CMake variables, e.g. using a toolchain file. The following variables have to be preset:

CMAKE_SYSTEM_NAME 
    this one is mandatory, it is the name of the target system, i.e. the same as CMAKE_SYSTEM_NAME would have if CMake would run on the target system. Typical examples are "Linux" and "Windows". This variable is used for constructing the file names of the platform files like Linux.cmake or Windows-gcc.cmake. If your target is an embedded system without OS set CMAKE_SYSTEM_NAME to "Generic". If CMAKE_SYSTEM_NAME is preset, the CMake variable CMAKE_CROSSCOMPILING is automatically set to TRUE, so this can be used for testing in the CMake files. 
CMAKE_SYSTEM_VERSION 
    optional, version of your target system, not used very much. 
CMAKE_SYSTEM_PROCESSOR 
    optional, processor (or hardware) of the target system. This variable is not used very much except for one purpose, it is used to load a CMAKE_SYSTEM_NAME-compiler-CMAKE_SYSTEM_PROCESSOR.cmake file, which can be used to modify settings like compiler flags etc. for the target. You probably only have to set this one if you are using a cross compiler where every target hardware needs special build settings. 

Since CMake cannot guess the target system, it also cannot guess which compiler it should use, so you have to preset this too:

CMAKE_C_COMPILER 
    the C compiler executable, may be the full path or just the filename. If it is specified with full path, then this path will be prefered when searching the C++ compiler and the other tools (binutils, linker, etc.). If this compiler is a gcc-cross compiler with a prefixed name (e.g. "arm-elf-gcc") CMake will detect this and automatically find the corresponding C++ compiler (i.e. "arm-elf-c++"). The compiler can also be preset via the CC environment variables. 
CMAKE_CXX_COMPILER 
    the C++ compiler executable, may be the full path or just the filename. It is handled the same way as CMAKE_C_COMPILER. If the toolchain is a GNU toolchain, you only need to set one of both. 

Once the system and the compiler are determined by CMake, it loads the corresponding files in the following order:

    Platform/${CMAKE_SYSTEM_NAME}.cmake (optional, but issues a stern warning)
    Platform/${CMAKE_SYSTEM_NAME}-<compiler>.cmake (optional)
    Platform/${CMAKE_SYSTEM_NAME}-<compiler>-${CMAKE_SYSTEM_PROCESSOR}.cmake (optional)

<compiler> is either the basename of the compiler executable, e.g. "gcc" (this is also used if gcc has a different name) or "cl", or by a compiler id, which is detected by compiling a test source file.

For testing the host system, there is a corresponding set of variables, which is set automatically by CMake:

    CMAKE_HOST_SYSTEM_NAME
    CMAKE_HOST_SYSTEM_VERSION
    CMAKE_HOST_SYSTEM_PROCESSOR
    CMAKE_HOST_SYSTEM

Without cross compiling the variables for the host system and the target system are identical. In most cases you will want to test for the target system, then the same way as without cross compiling use the CMAKE_SYSTEM_xxx variables, this will work both for cross compiling and for native building.

With these variables correctly set, CMake will now use the cross compiling toolchain for building and in the CMakeLists.txt you can still use the CMAKE_SYSTEM_XXX variables for testing for which system you are building. This is already enough to use CMake for cross compiling simple (buildsystem-wise) projects.
Searching and finding external software

Most non-trivial projects will depend on external libraries or tools. CMake offers the FIND_PROGRAM(), FIND_LIBRARY(), FIND_FILE(), FIND_PATH() and FIND_PACKAGE() commands for this purpose. They search the file system in common places for files and return the results. FIND_PACKAGE() is a bit different in that it actually doesn't search itself, but "only" executes FindXXX.cmake modules, which usually call the FIND_PROGRAM(), FIND_LIBRARY(), FIND_FILE() and FIND_PATH() commands then.

When cross compiling e.g. for a target with an ARM processor getting /usr/lib/libjpeg.so as the result of a FIND_PACKAGE(JPEG) wouldn't be much of a help, since this would be the JPEG library for the host system, e.g. an x86 Linux box. So you need to tell CMake to search in other locations. This can be done by setting the following variables:

CMAKE_FIND_ROOT_PATH 
    this is a list of directories, each of the directories listed there will be prepended to each of the search directories of every FIND_XXX() command. So e.g. if your target environment is installed under /opt/eldk/ppc_74xx, set CMAKE_FIND_ROOT_PATH to this directory. Then e.g. FIND_LIBRARY(BZ2_LIB bz2) will search in /opt/eldk/ppc_74xx/lib, /opt/eldk/ppc_74xx/usr/lib, /lib, /usr/lib and so give /opt/eldk/ppc_74xx/usr/lib/libbz2.so as result. By default CMAKE_FIND_ROOT_PATH is empty. If set, at first the directories prefixed with the directories given in CMAKE_FIND_ROOT_PATH will be searched and after that the unprefixed versions of the search directories will be searched. This behaviour can be modified individually for every FIND_XXX() call with the NO_CMAKE_FIND_ROOT_PATH, ONLY_CMAKE_FIND_ROOT_PATH and CMAKE_FIND_ROOT_PATH_BOTH options or the default for all FIND_XXX() commands can be adjusted with the CMAKE_FIND_ROOT_PATH_MODE_PROGRAM, CMAKE_FIND_ROOT_PATH_MODE_LIBRARY and CMAKE_FIND_ROOT_PATH_MODE_INCLUDE variables. If you don't want to use only libraries that come with the toolchain but also build and use additional libraries for your target platform, you should create an install directory for these packages, e.g. $HOME/eldk-ppc_74xx-inst/ and add this to CMAKE_FIND_ROOT_PATH, so the FIND_XXX() commands will search there too. If you then build packages for your target platform, they should be installed into this directory. 
CMAKE_FIND_ROOT_PATH_MODE_PROGRAM
    This sets the default behaviour for the FIND_PROGRAM() command. It can be set to NEVER, ONLY or BOTH (default). If set to NEVER, CMAKE_FIND_ROOT_PATH will not be used for FIND_PROGRAM() calls (except where it is enabled explicitely). If set to ONLY, only the search directories with the prefixes coming from CMAKE_FIND_ROOT_PATH will be used in FIND_PROGRAM(). The default is BOTH, which means that at first the prefixed directories and after that the unprefixed directories will be searched. In most cases FIND_PROGRAM() is used to search for an executable which will then be executed e.g. using EXECUTE_PROCESS() or ADD_CUSTOM_COMMAND(). So in most cases an executable from the build host is required, so usually set CMAKE_FIND_ROOT_PATH_MODE_PROGRAM to NEVER. 
CMAKE_FIND_ROOT_PATH_MODE_LIBRARY
    This is the same as above, but for the FIND_LIBRARY() command. In most cases this is used to find a library which will then be used for linking, so a library for the target is required. So in the common case, set it to ONLY. 
CMAKE_FIND_ROOT_PATH_MODE_INCLUDE
    This is the same as above and used for both FIND_PATH() and FIND_FILE(). In many cases this is used for finding include directories, so the target environment should be searched. So in the common case, set it to ONLY. You may have to adjust this behaviour for some of the FIND_PATH() or FIND_FILE() calls using the NO_CMAKE_FIND_ROOT_PATH, ONLY_CMAKE_FIND_ROOT_PATH and CMAKE_FIND_ROOT_PATH_BOTH options. 

The toolchain file

Defining all the variables mentioned above using -DCMAKE_SYSTEM_NAME etc. would be quite tedious and error prone. To make things easier, there is another cmake variable you can set:

CMAKE_TOOLCHAIN_FILE 
    absolute or relative path to a cmake script which sets up all the toolchain related variables mentioned above 

For instance for crosscompiling from Linux to Embedded Linux on PowerPC this file could look like this:

# this one is important
SET(CMAKE_SYSTEM_NAME Linux)
#this one not so much
SET(CMAKE_SYSTEM_VERSION 1)

# specify the cross compiler
SET(CMAKE_C_COMPILER   /opt/eldk-2007-01-19/usr/bin/ppc_74xx-gcc)
SET(CMAKE_CXX_COMPILER /opt/eldk-2007-01-19/usr/bin/ppc_74xx-g++)

# where is the target environment 
SET(CMAKE_FIND_ROOT_PATH  /opt/eldk-2007-01-19/ppc_74xx /home/alex/eldk-ppc74xx-inst)

# search for programs in the build host directories
SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
# for libraries and headers in the target directories
SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

If this file is named Toolchain-eldk-ppc74xx.cmake and is located in your home directory and you are building in the subdirectory build then you can do:

~/src$ cd build
~/src/build$ cmake -DCMAKE_TOOLCHAIN_FILE=~/Toolchain-eldk-ppc74xx.cmake ..
...

You don't have to write a toolchain file for every piece of software you want to build, the toolchain files are per target platform, i.e. if you are building several software packages all for the same target platform, you have to write only one toolchain file and you can use this for all packages.

If your compiler is not able to build a simple program by default without special flags or files (e.g. linker scripts or memory layout files), the toolchain file as shown above doesn't work. Then you have to force the compiler:

INCLUDE(CMakeForceCompiler)

# this one is important
SET(CMAKE_SYSTEM_NAME eCos)

# specify the cross compiler
CMAKE_FORCE_C_COMPILER(arm-elf-gcc GNU)
CMAKE_FORCE_CXX_COMPILER(arm-elf-g++ GNU)

# where is the target environment 
SET(CMAKE_FIND_ROOT_PATH  /home/alex/src/ecos/install )

# search for programs in the build host directories
SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
# for libraries and headers in the target directories
SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

This is done using the CMAKE_FORCE_XXX_COMPILER() macros. The second argument is the compiler id, which is used by CMake to recognize the compiler.


A toolchain for crosscompiling for Win32 using mingw32 might look like this:

# the name of the target operating system
SET(CMAKE_SYSTEM_NAME Windows)

# which compilers to use for C and C++
SET(CMAKE_C_COMPILER i486-mingw32-gcc)
SET(CMAKE_CXX_COMPILER i486-mingw32-g++)
SET(CMAKE_RC_COMPILER i486-mingw32-windres)

# here is the target environment located
SET(CMAKE_FIND_ROOT_PATH /usr/i486-mingw32)

# adjust the default behaviour of the FIND_XXX() commands:
# search headers and libraries in the target environment, search 
# programs in the host environment
set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

System introspection

Many non-trivial software projects have a set of system introspection tests for finding out properties of the (target) system. In CMake there are macros provided for this purpose like e.g. CHECK_INCLUDE_FILES() or CHECK_C_SOURCE_RUNS(). Most of these tests will internally use either the TRY_COMPILE() or the TRY_RUN() CMake commands. The TRY_COMPILE() commands work as expected also when cross compiling, they will try to compile the piece of code with the cross compiling toolchain, which will give the expected result. All tests using TRY_RUN() internally cannot work, since the created executables cannot run on the build host system. At first TRY_RUN() tries to compile the software, which will work the same way when cross compiling. If this succeeded, it will check the variable CMAKE_CROSSCOMPILING whether the resulting executable is runnable or not. If not, it will create two cache variables, which then have to be set by the user or via the CMake cache. Let's say the command looks like this:

TRY_RUN(SHARED_LIBRARY_PATH_TYPE SHARED_LIBRARY_PATH_INFO_COMPILED
        ${PROJECT_BINARY_DIR}/CMakeTmp
        ${PROJECT_SOURCE_DIR}/CMake/SharedLibraryPathInfo.cxx
        OUTPUT_VARIABLE OUTPUT
        ARGS "LDPATH")

The variable SHARED_LIBRARY_PATH_INFO_COMPILED will be set to the result of the build (i.e. TRUE or FALSE). CMake will create a cache variable SHARED_LIBRARY_PATH_TYPE and preset it to PLEASE_FILL_OUT-FAILED_TO_RUN. This one has to be set to the exit code of the executable if it would have been executed on the target. It will also create a cache variable SHARED_LIBRARY_PATH_TYPE__TRYRUN_OUTPUT and preset it to PLEASE_FILL_OUT-NOTFOUND. This one has to be set to the output the executable prints to stdout and stderr if it is executed on the target. This variable is only created if the TRY_RUN() command was used with the RUN_OUTPUT_VARIABLE or the OUTPUT_VARIABLE argument. You have to fill in appropriate values for these variables. To help you with this CMake tries its best to give you useful information.

To do so CMake creates a file ${CMAKE_BINARY_DIR}/TryRunResults.cmake. There you will find all variables which CMake could not determine, from which CMake file they were called, the source file, the arguments for the executable and the path to the executable. CMake will also copy the executables in the build directory, they have the names cmTryCompileExec-<name of the variable>, e.g. cmTryCompileExec-SHARED_LIBRARY_PATH_TYPE. You can then try to run this executable manually on the actual target platform and check the results.

Once you have these results, they have to get in the CMake cache. You can either use ccmake/CMakeSetup/"make edit_cache" and edit the variables directly in the cache. Then you won't be able to reuse your changes in another build directory or if you remove CMakeCache.txt. The second option is to use the TryRunResults.cmake file. Copy it to a safe location (i.e. where it is not deleted if you delete the build dir) and give it a useful name, e.g. MyProjectTryRunResults-eldk-ppc.cmake. Then edit it so that the SET() commands set the required values. You can the use this file to preload the CMake cache by using the -C option of cmake:

src/build/ $ cmake -C ~/MyProjectTryRunResults-eldk-ppc.cmake .

You don't have to use the other CMake options again, they are now already in the cache. This way you can use MyProjectTryRunResults-eldk-ppc.cmake in multiple build trees and it could also be distributed with your project so it gets easier for other users who want to compile it.

This script may be helpful to automatically populate the TRY_RUN results with those placed in a CMakeCache.txt that were created on the target.
Using executables in the build created during the build

In some cases during a build executables are created which are then used in ADD_CUSTOM_COMMAND() or ADD_CUSTOM_TARGET() during the same build process.

When cross compiling this won't work without modifications because the executables cannot run on the build host. Starting with CMake 2.6 it is possible to "import" executable targets into a CMake project. When cross compiling this has to be used to import executables built in a native build into the cross-build. This can be done like this:

# when crosscompiling import the executable targets from a file
IF(CMAKE_CROSSCOMPILING)
  SET(IMPORT_EXECUTABLES "IMPORTFILE-NOTFOUND" CACHE FILEPATH "Point it to the export file from a native build")
  INCLUDE(${IMPORT_EXECUTABLES})
ENDIF(CMAKE_CROSSCOMPILING)

...


# only build the generator if not crosscompiling
IF(NOT CMAKE_CROSSCOMPILING)
   ADD_EXECUTABLE(mygenerator mygen.cpp)
   TARGET_LINK_LIBRARIES(mygenerator ${SOME_LIBS})
ENDIF(NOT CMAKE_CROSSCOMPILING)

# then use the target name as COMMAND, CMake >= 2.6 knows how to handle this
ADD_CUSTOM_COMMAND(OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   COMMAND mygenerator foo.dat -o ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   DEPENDS foo.dat )


...
# export the generator target to a file, so it can be imported (see above) by another build
# the IF() is not necessary, but makes the intention clearer
IF(NOT CMAKE_CROSSCOMPILING) 
  EXPORT(TARGETS mygenerator FILE ${CMAKE_BINARY_DIR}/ImportExecutables.cmake )
ENDIF(NOT CMAKE_CROSSCOMPILING) 

So during the native build the target "mygenerator" will be built and used in ADD_CUSTOM_COMMAND(). As command only the target name is used. CMake >= 2.6.0 recognizes this and creates the dependencies and will use the path to the created executable when executing the command. At the end the EXPORT() function (since CMake 2.6.0) is called, which "exports" the listed targets to the file ${CMAKE_BINARY_DIR}/ImportExecutables.cmake, which will look like this:

ADD_EXECUTABLE(mygenerator IMPORT)
SET_TARGET_PROPERTIES(mygenerator PROPERTIES 
                      LOCATION /home/alex/build-native/bin/mygenerator )

This file is then included when cross compiling, it either has to be specified using -D or via the cmake GUI. Then later on the command for actually building mygenerator is excluded. In ADD_CUSTOM_COMMAND() mygenerator will be recognized as an imported target and it will be used when executing the command.


If the executable mygenerator also has to be built when cross compiling, then some more logic needs to be added, e.g. like this:

# when crosscompiling import the executable targets from a file
IF(CMAKE_CROSSCOMPILING)
  SET(IMPORT_EXECUTABLES "IMPORTFILE-NOTFOUND" CACHE FILEPATH "Point it to the export file from a native build")
  INCLUDE(${IMPORT_EXECUTABLES})
ENDIF(CMAKE_CROSSCOMPILING)

...

# always build the executable
ADD_EXECUTABLE(mygenerator mygen.cpp)
TARGET_LINK_LIBRARIES(mygenerator ${SOME_LIBS})

# but use different names for the command
IF(CMAKE_CROSSCOMPILING)
   SET(mygenerator_EXE native-mygenerator)
ELSE(CMAKE_CROSSCOMPILING)
   SET(mygenerator_EXE mygenerator)
ENDIF(CMAKE_CROSSCOMPILING)

# then use the target name as COMMAND, CMake >= 2.6 knows how to handle this
ADD_CUSTOM_COMMAND(OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   COMMAND ${mygenerator_EXE} foo.dat -o ${CMAKE_CURRENT_BINARY_DIR}/generated.c
                   DEPENDS foo.dat )


...
# export the generator target to a file, so it can be imported (see above) by another build
# the IF() is not necessary, but makes the intention clearer
# use the NAMESPACE option of EXPORT() to get a different target name for mygenerator when exporting
IF(NOT CMAKE_CROSSCOMPILING) 
  EXPORT(TARGETS mygenerator FILE ${CMAKE_BINARY_DIR}/ImportExecutables.cmake NAMESPACE native- )
ENDIF(NOT CMAKE_CROSSCOMPILING) 

Cross compilation for Windows CE

Building for Windows CE requires Visual Studio 2005 or 2008 (No Express Edition!) with at least one installed SDK. If you don't have a specific installation file for your target device it is possible to use the Windows CE 5.0 Standard SDK from http://www.microsoft.com/downloads/details.aspx?familyid=fa1a3d66-3f61-4ddc-9510-ae450e2318c3.

CMake supports Windows CE out of the box since version 2.8.10 when used with the NMake Makefiles generator. To use it you first need the corresponding environment variables set, for which the CMake command env_vs8_wince has been added in the following version. Using 2.8.10 is possible too, if the environment is set manually. To get there start a command prompt and type the following commands:

"%VS80COMNTOOLS%vsvars32.bat"
cmake -E env_vs8_wince "STANDARDSDK_500 (ARMV4I)" > env.bat
env.bat

Then the Makefiles can be generated and built with the following commands:

cmake -G "NMake Makefiles" -DCMAKE_SYSTEM_NAME=WindowsCE -DCMAKE_SYSTEM_PROCESSOR=ARMV4I \path\to\source
cmake --build .

Starting with CMake 2.8.11 it is also possible to create Visual Studio solution for Windows CE targets. Depending on the installed SDKs CMake will accept additional generators. The following command will create Visual Studio 2005 files for the WinCE standard SDK:

cmake -G "Visual Studio 8 2005 STANDARDSDK_500 (ARMV4I)" \path\to\source

To use VS2008 instead of VS2005 simple replace "VS80COMNTOOLS" with "VS90COMNTOOLS", "vs8" with "vs9" and "8 2005" with "9 2008".
Information how to set up various cross compiling toolchains

    [1], detailed instructions for using CMake for the iPhone (external, thirdparty website).
    eldk, embedded Linux cross compiling toolchain from Denx
    mingw - gcc for cross compiling from Linux to Windows
    SDCC - the small devices C compiler
    eCos - the embedded Configurable operating system
    ADSP - the Analog Devices toolchain for their DSPs
    IBM BlueGene/L
    Cray XT3 / Catamount
    Crosstool NG - may be used to easily build various cross compiler toolchain. The produced toolchain seems to work well with CMake cross-compiling.
    MXE - Builds compiler and libraries for cross compiling from Linux to Windows. Comes with CMake toolchain file!

How to cross compile specific projects

    ITK
    ParaView3
    Python

FAQ/Potential Problems

    On mixed 32/64 bit Linux installations cross compilation cannot be used to build for 32/64 bit only.

    FindXXX.cmake modules, which rely on executing a binary tool like pkg-config may have problems, since the pkg-config of the target platform cannot be executed on the host. Tools like pkg-config should be used only optional in FindXXX.cmake files.

    What about Scratchbox ? CMake should work without problems in scratchbox, then it will just work in native mode.

    Can it build software for PS3 ? If you build software for PS3, you build software for two architectures, for the PowerPC and for the cells. This is done using two different toolchains. Currently CMake doesn't support using multiple toolchains in one buildtree or building for multiple target architectures in one build tree. So building for PS3 doesn't work out-of-the-box. It should work using ADD_CUSTOM_COMMAND() or by using two buildtrees.


#+END_EXAMPLE


https://cmake.org/cmake/help/v3.0/manual/cmake-toolchains.7.html

Cross Compiling

If cmake(1) is invoked with the command line parameter -DCMAKE_TOOLCHAIN_FILE=path/to/file, the file will be loaded early to set values for the compilers. A typical cross-compiling toolchain has content such as:

set(CMAKE_SYSTEM_NAME Linux)

set(CMAKE_SYSROOT /home/devel/rasp-pi-rootfs)
set(CMAKE_STAGING_PREFIX /home/devel/stage)

set(CMAKE_C_COMPILER /home/devel/gcc-4.7-linaro-rpi-gnueabihf/bin/arm-linux-gnueabihf-gcc)
set(CMAKE_CXX_COMPILER /home/devel/gcc-4.7-linaro-rpi-gnueabihf/bin/arm-linux-gnueabihf-g++)

set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)

The CMAKE_SYSTEM_NAME is the CMake-identifier of the target platform to build for.

The CMAKE_SYSROOT is optional, and may be specified if a sysroot is available.

The CMAKE_STAGING_PREFIX is also optional. It may be used to specify a path on the host to install to. The CMAKE_INSTALL_PREFIX is always the runtime installation location, even when cross-compiling.

The CMAKE_<LANG>_COMPILER variables may be set to full paths, or to names of compilers to search for in standard locations. In cases where CMake does not have enough information to extract information from the compiler, the CMakeForceCompiler module can be used to bypass some of the checks.

CMake find_* commands will look in the sysroot, and the CMAKE_FIND_ROOT_PATH entries by default in all cases, as well as looking in the host system root prefix. Although this can be controlled on a case-by-case basis, when cross-compiling, it can be useful to exclude looking in either the host or the target for particular artifacts. Generally, includes, libraries and packages should be found in the target system prefixes, whereas executables which must be run as part of the build should be found only on the host and not on the target. This is the purpose of the CMAKE_FIND_ROOT_PATH_MODE_* variables.

Some compilers are inherently cross compilers, such as Clang and the QNX QCC compiler. The CMAKE_<LANG>_COMPILER_TARGET can be set to pass a value to those supported compilers when compiling:

set(CMAKE_SYSTEM_NAME Linux)

set(triple arm-linux-gnueabihf)

set(CMAKE_C_COMPILER clang)
set(CMAKE_C_COMPILER_TARGET ${triple})
set(CMAKE_CXX_COMPILER clang++)
set(CMAKE_CXX_COMPILER_TARGET ${triple})

Or, for QCC:

set(CMAKE_SYSTEM_NAME QNX)

set(arch gcc_ntoarmv7le)

set(CMAKE_C_COMPILER qcc)
set(CMAKE_C_COMPILER_TARGET ${arch})
set(CMAKE_CXX_COMPILER QCC)
set(CMAKE_CXX_COMPILER_TARGET ${arch})

Similarly, some compilers do not ship their own supplementary utilities such as linkers, but provide a way to specify the location of the external toolchain which will be used by the compiler driver. The CMAKE_<LANG>_COMPILER_EXTERNAL_TOOLCHAIN variable can be set in a toolchain file to pass the path to the compiler driver.

The CMAKE_CROSSCOMPILING variable is set to true when CMake is cross-compiling.

* use case


  1) 使用 use case 十大误区
  1) 系统的boundary 没有定义或经常改变；
  1) 从系统观点而不是actor观点来定义Use Case；
  1) Actor的名称不一致；
  1) Use Case 定义过多；
  1) Use Case 和actor之间的关系象蜘蛛网一样错综复杂；
  1) Use Case的说明太长；
  1) Use Case的说明不清楚；
  1) Use Case没有正确的描述功能需求；
  1) 用户无法理解Use Case；
  1) Use Case 无法正常结束


* socat

#+BEGIN_SRC 


  Open a terminal (let's call it Terminal 0) and execute it:

socat -d -d pty,raw,echo=0 pty,raw,echo=0

The code above returns:

2013/11/01 13:47:27 socat[2506] N PTY is /dev/pts/2
2013/11/01 13:47:27 socat[2506] N PTY is /dev/pts/3
2013/11/01 13:47:27 socat[2506] N starting data transfer loop with FDs [3,3] and [5,5]

Open another terminal and write (Terminal 1):

cat < /dev/pts/2

this command's port name can be changed according to the pc. it's depends on the previous output.

2013/11/01 13:47:27 socat[2506] N PTY is /dev/pts/**2**
2013/11/01 13:47:27 socat[2506] N PTY is /dev/pts/**3**
2013/11/01 13:47:27 socat[2506] N starting data transfer loop with FDs 

you should use the number available on highlighted area.

Open another terminal and write (Terminal 2):

echo "Test" > /dev/pts/3

Now back to Terminal 1 and you'll see the string "Test".
#+END_SRC
* lighttpd设置(调试本地fastcgi)

** 编辑/etc/lighttpd/lighttpd.conf
修改

#+BEGIN_SRC 
server.username         = "songzc"
server.groupname        = "songzc"
server.document-root = "/home/songzc/codbase/exam/Vue_Full_Project/www"

#+END_SRC
  

** 添加 /etc/lighttpd/conf.d/fastcgi.conf

#+BEGIN_SRC 

server.modules += ( "mod_fastcgi" )
fastcgi.debug = 1
#server.indexfiles += ( "dispatch.fcgi" ) #this is deprecated
#index-file.names += ( "dispatch.fcgi" ) #dispatch.fcgi if rails specified
#server.error-handler-404   = "/dispatch.fcgi" #too
fastcgi.server = (
    ".cgi" => (
    "localhost" => ( 
       "socket" => "/run/lighttpd/rails-fastcgi.sock",
        "bin-path" => "/home/songzc/codbase/330/acum/dist/i386/agioe_fcgi.cgi",
 #   "bin-path" => "/tmp/test.fastcgi",
        "max-procs" => 1,
"check-local" => "disable"
                )
    )
    
)

include "conf.d/fastcgi.conf"

#+END_SRC


** 手动执行ligttpd
#+BEGIN_SRC 
chmod 777  /run/lighttpd/

export LD_LIBRARY_PATH=/home/songzc/codbase/rail_robot/dist/i386:$LD_LIBRARY_PATH 
lighttpd -D -f /etc/lighttpd/lighttpd.conf
#+END_SRC

* postgresal使用错误解决

https://blog.csdn.net/wangyezi19930928/article/details/20358369


错误：psql: FATAL: Peer authentication failed for user "postgres"

解决办法如下:

1）. 运行下面的命令编辑pg_hba.conf文件 sudo vim /etc/postgresql/9.1/main/pg_hba.conf

2）. 将

 # Database administrative login by Unix domain socket

local     all      postgres        peer

改为

# Database administrative login by Unix domain socket

local     all     postgres         trust

3）. 保存后执行下面的命令重新加载配置文件: sudo /etc/init.d/postgresql reload
* gitlab
** gitlab 安装以及管理
https://www.jianshu.com/p/37abb00e4b67

Gitlab 默认安装文件位置

1 项目文件位置gitlab代码放在/var/opt/gitlab/git-data/下）

2 配置文件目录  /etc/gitlab/gitlab.rb修改完配置后执行gitlab-ctl reconfigure生效

3安装文件目录   /var/opt/gitlab/

4备份文件目录   /var/opt/gitlab/backups

5 日志位置：/var/log/gitlab

6 部分配置文件地址/opt/gitlab/embedded/service/gitlab-rails/config

vim gitlab.yml 修改服务器的ip地址 ，邮件发送名称

7 文件上传位置/var/opt/gitlab/gitlab-rails/uploads/

gitlab安装以后有两个目录：

           一个在/opt/gitlab，这里都是程序文件，不包含数据。

            另一个在/var/opt/gitlab，这里都输数据文件。

安装后的配置文件目录：

主文件：/etc/gitlab/  

主配置文件：/var/opt/gitlab/

 /opt/gitlab/

日志目录：/var/log/gitlab/


gitlab-ctl

tail #查看所有日志gitlab-ctl tail nginx/gitlab_access.log #查看nginx访问日志cd op

迁移需要/etc/gitlab/gitlab.rb

6 查看版本的信息

cat /opt/gitlab/embedded/service/gitlab-rails/VERSION


* test case managment
